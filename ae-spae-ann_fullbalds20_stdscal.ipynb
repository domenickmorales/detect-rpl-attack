{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Donâ€™t pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "from keras import optimizers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Activation, Dense, Dropout, Input\n",
    "from keras import backend as k\n",
    "\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "#k.tensorflow_backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------Import modules------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(23)\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from datetime import datetime \n",
    "import os.path\n",
    "\n",
    "dsnum=20\n",
    "verbose_level=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/01Code/00Datasets_final/00BalancedDS/FullCloneID20bal_stdscal.csv\n"
     ]
    }
   ],
   "source": [
    "pathds = os.path.abspath('/home/user/01Code/00Datasets_final/00BalancedDS')\n",
    "file_name = \"FullCloneID\"+str(dsnum)+\"bal_stdscal.csv\"\n",
    "full_path = os.path.join(pathds,file_name)\n",
    "print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2686202, 67)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "neurons=df.shape[1]-1\n",
    "batch_size=df.shape[1]-1\n",
    "print(neurons)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Explaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1343101\n",
      "Class 1: 1343101\n",
      "Proportion: 1.0 : 1\n"
     ]
    }
   ],
   "source": [
    "#if you don't have an intuitive sense of how imbalanced these two classes are, let's go visual\n",
    "count_classes = pd.value_counts(df['class'], sort = True)\n",
    "print('Class 0:', count_classes[0])\n",
    "print('Class 1:', count_classes[1])\n",
    "print('Proportion:', round(count_classes[0] / count_classes[1], 3), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.xticks(range(2), ['Normal [0]','Malicious [1]'])\n",
    "plt.title(\"Frequency by observation number\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Observations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed and percentage of test data\n",
    "RANDOM_SEED = 23 #used to help randomly select the data points\n",
    "TEST_PCT = 0.20 # 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_df = train_test_split(df, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ df -> original dataset \n",
    "+ train -> subset of 80% from original dataset \n",
    "+ test_df -> subset of 20% from original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(train, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train -> subset of 80% from original dataset \n",
    "+ train_df -> subset of 80% from train\n",
    "+ dev_df -> subset of 20% from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000465341374433\n",
      "0.49896578120164825\n",
      "0.5006784664610482\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of mal samples in train and test set\n",
    "print(train_df.iloc[:, batch_size].sum()/train_df.shape[0]) \n",
    "print(dev_df.iloc[:, batch_size].sum()/dev_df.shape[0]) \n",
    "print(test_df.iloc[:, batch_size].sum()/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.iloc[:, :batch_size] \n",
    "dev_x = dev_df.iloc[:, :batch_size] \n",
    "test_x = test_df.iloc[:, :batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_x -> features of train_df **Training subset for AE**\n",
    "+ dev_x -> features of dev_df **Validation subset for AE**\n",
    "+ test_x -> features of test_df **Testing subset for ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final train and test sets\n",
    "train_y = train_df.iloc[:,batch_size]\n",
    "dev_y = dev_df.iloc[:,batch_size]\n",
    "test_y = test_df.iloc[:,batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_y -> **Labels for supervised training of ANN**\n",
    "+ dev_y -> labels of dev_df  *not used for AE neither ANN*\n",
    "+ test_y -> labels of test_df  **Ground Truth for predictions of supervised ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "train_x =np.array(train_x)\n",
    "dev_x =np.array(dev_x)\n",
    "test_x = np.array(test_x)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "dev_y = np.array(dev_y)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "print(train_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(factor_enc_dim, enc_activation, dec_activation, \n",
    "                optimizer, loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer #RELU\n",
    "    encoded = Dense(encoding_dim, activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer #SIMOID\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spae(factor_enc_dim,dec_activation,enc_activation,\n",
    "         optimizer,loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer\n",
    "    encoded = Dense(encoding_dim, activity_regularizer=regularizers.l1(1e-4), activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pca(thr):\n",
    "    #train_x_pca,test_x_pca = to_pca(0.95)\n",
    "    pca = PCA(n_components = thr, svd_solver = 'full')\n",
    "    train_x_ = np.array(train_x)\n",
    "    print(type(train_x_))\n",
    "\n",
    "    test_x_ = np.array(test_x)\n",
    "    print(type(test_x_))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(time.ctime(start_time))\n",
    "\n",
    "    train_x_pca = pca.fit_transform(train_x_)\n",
    "    print(train_x_pca.shape)\n",
    "\n",
    "    test_x_pca = pca.fit_transform(test_x_)\n",
    "    print(test_x_pca.shape)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "\n",
    "    print(\"--- PCA spent %s seconds ---\" %elapsed_time )\n",
    "    \n",
    "    return  train_x_pca,test_x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ae(checkpoint_file, autoencoder,\n",
    "           epochs, batch_size, shuffle):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    hist_auto = autoencoder.fit(train_x, train_x,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    verbose=verbose_level,\n",
    "                    callbacks=[early_stopping, cp, tb],\n",
    "                    validation_data=(dev_x, dev_x))\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "    \n",
    "    return hist_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_auto(hist_auto, fig_file):\n",
    "    best_loss_value = hist_auto.history['loss'][-1]\n",
    "    print('Best loss value:', best_loss_value)\n",
    "\n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist_auto.history['loss'])\n",
    "    plt.plot(hist_auto.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.savefig(fig_file)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h(neurons,encoded_train_x,init_mode,activation_input,\n",
    "               weight_constraint,dropout_rate,activation_output,\n",
    "               loss,optimizer):\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=encoded_train_x.shape[1],\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h_():\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=input_dim,\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(checkpoint_file,ann,enc_train_x,train_y,epochs,shuffle,batch_size):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    history = ann.fit(enc_train_x,\n",
    "                      train_y,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[early_stopping, cp, tb],\n",
    "                      epochs=epochs,\n",
    "                      shuffle=shuffle,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=verbose_level)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict(ann,enc_test_x):\n",
    "    pred_ann_prob = ann.predict(enc_test_x)\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "    \n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob, pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict_():\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))  \n",
    "\n",
    "    modelk = KerasClassifier(build_fn=ann_2h_,\n",
    "                             epochs=epochs, \n",
    "                             batch_size=batch_size, \n",
    "                             verbose=verbose_level\n",
    "                            )\n",
    "\n",
    "    pred_ann_prob = cross_val_predict(modelk,\n",
    "                                      enc_test_x,\n",
    "                                      test_y,\n",
    "                                      cv=KFold(n_splits=5, random_state=23),\n",
    "                                      verbose=1)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "\n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob,pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cm(pred_ann_prob, pred_ann_01, roc_file, cm_file):\n",
    "    false_positive_rate, recall, thresholds = roc_curve(test_y, pred_ann_prob)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.savefig(roc_file)\n",
    "    plt.show()\n",
    "    \n",
    "    cm = confusion_matrix(test_y, pred_ann_01)\n",
    "    labels = ['Normal', 'Malicious']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=\"RdYlGn\", vmin = 0.2);\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.savefig(cm_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- PCA Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_pca,test_x_pca = to_pca(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- AE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 66)                0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 44)                2948      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 66)                2970      \n",
      "=================================================================\n",
      "Total params: 5,918\n",
      "Trainable params: 5,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_sigmoid_adam_mse,enc_train_x_asam,enc_test_x_asam = ae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ae_sigmoid_adam_mse = load_model('ae_sigmoid_adam_mse_redds10bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  8 16:16:04 2019\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1719168 samples, validate on 429793 samples\n",
      "Epoch 1/200\n",
      "1719168/1719168 [==============================] - 48s 28us/step - loss: 0.1590 - acc: 0.1868 - val_loss: 0.1575 - val_acc: 0.2005\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15752, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 2/200\n",
      "1719168/1719168 [==============================] - 46s 27us/step - loss: 0.1568 - acc: 0.2160 - val_loss: 0.1575 - val_acc: 0.2100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15752 to 0.15752, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 3/200\n",
      "1719168/1719168 [==============================] - 46s 27us/step - loss: 0.1568 - acc: 0.1984 - val_loss: 0.1575 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15752\n",
      "Epoch 4/200\n",
      "1719168/1719168 [==============================] - 45s 26us/step - loss: 0.1568 - acc: 0.1524 - val_loss: 0.1575 - val_acc: 0.1356\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15752 to 0.15752, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 5/200\n",
      "1719168/1719168 [==============================] - 45s 26us/step - loss: 0.1568 - acc: 0.1332 - val_loss: 0.1575 - val_acc: 0.1277\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15752 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 6/200\n",
      "1719168/1719168 [==============================] - 44s 26us/step - loss: 0.1568 - acc: 0.1366 - val_loss: 0.1575 - val_acc: 0.1458\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 7/200\n",
      "1719168/1719168 [==============================] - 44s 26us/step - loss: 0.1568 - acc: 0.1491 - val_loss: 0.1575 - val_acc: 0.1192\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.15751\n",
      "Epoch 8/200\n",
      "1719168/1719168 [==============================] - 43s 25us/step - loss: 0.1568 - acc: 0.1492 - val_loss: 0.1575 - val_acc: 0.1558\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 9/200\n",
      "1719168/1719168 [==============================] - 43s 25us/step - loss: 0.1568 - acc: 0.1599 - val_loss: 0.1575 - val_acc: 0.1294\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 10/200\n",
      "1719168/1719168 [==============================] - 43s 25us/step - loss: 0.1567 - acc: 0.1610 - val_loss: 0.1575 - val_acc: 0.1857\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 11/200\n",
      "1719168/1719168 [==============================] - 42s 25us/step - loss: 0.1567 - acc: 0.1659 - val_loss: 0.1575 - val_acc: 0.1683\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 12/200\n",
      "1719168/1719168 [==============================] - 42s 24us/step - loss: 0.1567 - acc: 0.1626 - val_loss: 0.1574 - val_acc: 0.1329\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.15751 to 0.15743, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 13/200\n",
      "1719168/1719168 [==============================] - 42s 24us/step - loss: 0.1567 - acc: 0.1593 - val_loss: 0.1574 - val_acc: 0.1657\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15743 to 0.15743, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 14/200\n",
      "1719168/1719168 [==============================] - 41s 24us/step - loss: 0.1567 - acc: 0.1565 - val_loss: 0.1574 - val_acc: 0.1688\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.15743 to 0.15743, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 15/200\n",
      "1719168/1719168 [==============================] - 41s 24us/step - loss: 0.1567 - acc: 0.1726 - val_loss: 0.1574 - val_acc: 0.1714\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.15743\n",
      "Epoch 16/200\n",
      "1719168/1719168 [==============================] - 41s 24us/step - loss: 0.1567 - acc: 0.1571 - val_loss: 0.1574 - val_acc: 0.1505\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.15743 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 17/200\n",
      "1719168/1719168 [==============================] - 41s 24us/step - loss: 0.1567 - acc: 0.1522 - val_loss: 0.1574 - val_acc: 0.1645\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 18/200\n",
      "1719168/1719168 [==============================] - 41s 24us/step - loss: 0.1566 - acc: 0.1502 - val_loss: 0.1574 - val_acc: 0.1469\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 19/200\n",
      "1719168/1719168 [==============================] - 41s 24us/step - loss: 0.1566 - acc: 0.1449 - val_loss: 0.1574 - val_acc: 0.1535\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.15741\n",
      "Epoch 20/200\n",
      "1719168/1719168 [==============================] - 41s 24us/step - loss: 0.1566 - acc: 0.1607 - val_loss: 0.1574 - val_acc: 0.1900\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.15741\n",
      "Epoch 21/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1656 - val_loss: 0.1574 - val_acc: 0.1606\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 22/200\n",
      "1719168/1719168 [==============================] - 41s 24us/step - loss: 0.1566 - acc: 0.1505 - val_loss: 0.1574 - val_acc: 0.1504\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15741\n",
      "Epoch 23/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1442 - val_loss: 0.1574 - val_acc: 0.1448\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.15741\n",
      "Epoch 24/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1538 - val_loss: 0.1574 - val_acc: 0.1516\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 25/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1486 - val_loss: 0.1574 - val_acc: 0.1323\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 26/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1254 - val_loss: 0.1574 - val_acc: 0.1434\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 27/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1398 - val_loss: 0.1574 - val_acc: 0.1427\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 28/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1491 - val_loss: 0.1574 - val_acc: 0.1546\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15741\n",
      "Epoch 29/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1533 - val_loss: 0.1574 - val_acc: 0.1587\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 30/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1524 - val_loss: 0.1574 - val_acc: 0.1559\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15741\n",
      "Epoch 31/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1559 - val_loss: 0.1574 - val_acc: 0.1541\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 32/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1491 - val_loss: 0.1574 - val_acc: 0.1622\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 33/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1584 - val_loss: 0.1574 - val_acc: 0.1656\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.15741\n",
      "Epoch 34/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1636 - val_loss: 0.1574 - val_acc: 0.1786\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 35/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1629 - val_loss: 0.1574 - val_acc: 0.1497\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 36/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1455 - val_loss: 0.1574 - val_acc: 0.1411\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 37/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1417 - val_loss: 0.1574 - val_acc: 0.1480\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.15741\n",
      "Epoch 38/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1314 - val_loss: 0.1574 - val_acc: 0.1302\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.15741\n",
      "Epoch 39/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1318 - val_loss: 0.1574 - val_acc: 0.1335\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 40/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1375 - val_loss: 0.1574 - val_acc: 0.1380\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.15741\n",
      "Epoch 41/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1439 - val_loss: 0.1574 - val_acc: 0.1273\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15741\n",
      "Epoch 42/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1544 - val_loss: 0.1574 - val_acc: 0.1615\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.15741 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 43/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1695 - val_loss: 0.1574 - val_acc: 0.1797\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 44/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1749 - val_loss: 0.1574 - val_acc: 0.1716\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 45/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1590 - val_loss: 0.1574 - val_acc: 0.1516\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.15740\n",
      "Epoch 46/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1625 - val_loss: 0.1574 - val_acc: 0.1856\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.15740\n",
      "Epoch 47/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1598 - val_loss: 0.1574 - val_acc: 0.1388\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 48/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1455 - val_loss: 0.1574 - val_acc: 0.1551\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15740\n",
      "Epoch 49/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1579 - val_loss: 0.1574 - val_acc: 0.1657\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 50/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1649 - val_loss: 0.1574 - val_acc: 0.1722\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.15740\n",
      "Epoch 51/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1696 - val_loss: 0.1574 - val_acc: 0.1584\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.15740\n",
      "Epoch 52/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1568 - val_loss: 0.1574 - val_acc: 0.1516\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 53/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1581 - val_loss: 0.1574 - val_acc: 0.1419\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.15740\n",
      "Epoch 54/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1490 - val_loss: 0.1574 - val_acc: 0.1421\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.15740\n",
      "Epoch 55/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1437 - val_loss: 0.1574 - val_acc: 0.1370\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 56/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1526 - val_loss: 0.1574 - val_acc: 0.1570\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.15740\n",
      "Epoch 57/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1579 - val_loss: 0.1574 - val_acc: 0.1651\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 58/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1557 - val_loss: 0.1574 - val_acc: 0.1688\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.15740\n",
      "Epoch 59/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1624 - val_loss: 0.1574 - val_acc: 0.1442\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.15740\n",
      "Epoch 60/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1571 - val_loss: 0.1574 - val_acc: 0.1525\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 61/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1591 - val_loss: 0.1574 - val_acc: 0.1548\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 62/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1700 - val_loss: 0.1574 - val_acc: 0.1953\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 63/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1710 - val_loss: 0.1574 - val_acc: 0.1102\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 64/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1664 - val_loss: 0.1574 - val_acc: 0.1788\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 65/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1713 - val_loss: 0.1574 - val_acc: 0.1941\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.15740 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 66/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1640 - val_loss: 0.1574 - val_acc: 0.1510\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 67/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1573 - val_loss: 0.1574 - val_acc: 0.1594\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 68/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1629 - val_loss: 0.1574 - val_acc: 0.1644\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 69/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1710 - val_loss: 0.1574 - val_acc: 0.1543\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.15739\n",
      "Epoch 70/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1907 - val_loss: 0.1574 - val_acc: 0.1618\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 71/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1883 - val_loss: 0.1574 - val_acc: 0.1862\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 72/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1836 - val_loss: 0.1574 - val_acc: 0.1784\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 73/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1830 - val_loss: 0.1574 - val_acc: 0.1889\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 74/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.1939 - val_loss: 0.1574 - val_acc: 0.2348\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.15739\n",
      "Epoch 75/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2060 - val_loss: 0.1574 - val_acc: 0.1935\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.15739\n",
      "Epoch 76/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2161 - val_loss: 0.1574 - val_acc: 0.2159\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 77/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2122 - val_loss: 0.1574 - val_acc: 0.2107\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 78/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2207 - val_loss: 0.1574 - val_acc: 0.2279\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.15739\n",
      "Epoch 79/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2142 - val_loss: 0.1574 - val_acc: 0.2098\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.15739\n",
      "Epoch 80/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2058 - val_loss: 0.1574 - val_acc: 0.2136\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.15739\n",
      "Epoch 81/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2272 - val_loss: 0.1574 - val_acc: 0.2360\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 82/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2279 - val_loss: 0.1574 - val_acc: 0.2221\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.15739\n",
      "Epoch 83/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2163 - val_loss: 0.1574 - val_acc: 0.2250\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 84/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2190 - val_loss: 0.1574 - val_acc: 0.2252\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.15739\n",
      "Epoch 85/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2363 - val_loss: 0.1574 - val_acc: 0.2684\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.15739\n",
      "Epoch 86/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2307 - val_loss: 0.1574 - val_acc: 0.2322\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.15739 to 0.15739, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 87/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2400 - val_loss: 0.1574 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.15739\n",
      "Epoch 88/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2308 - val_loss: 0.1574 - val_acc: 0.2347\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.15739\n",
      "Epoch 89/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2259 - val_loss: 0.1574 - val_acc: 0.2059\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.15739\n",
      "Epoch 90/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2357 - val_loss: 0.1574 - val_acc: 0.2552\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.15739\n",
      "Epoch 91/200\n",
      "1719168/1719168 [==============================] - 40s 23us/step - loss: 0.1566 - acc: 0.2397 - val_loss: 0.1574 - val_acc: 0.2596\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.15739\n",
      "Time elapsed (hh:mm:ss.ms) 1:01:43.486101\n"
     ]
    }
   ],
   "source": [
    "hist_ae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/ae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = ae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.15662941263750635\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_ae_sigmoid_adam_mse  = plot_hist_auto(hist_ae_sigmoid_adam_mse, './Figures/hist_ae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- SPAE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 66)                0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 44)                2948      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 66)                2970      \n",
      "=================================================================\n",
      "Total params: 5,918\n",
      "Trainable params: 5,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "spae_sigmoid_adam_mse,enc_train_x_spsam,enc_test_x_spsam = spae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spae_sigmoid_adam_mse = load_model('spae_sigmoid_adam_mse_redds20bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  8 17:18:07 2019\n",
      "Train on 1719168 samples, validate on 429793 samples\n",
      "Epoch 1/200\n",
      "1719168/1719168 [==============================] - 42s 24us/step - loss: 0.1953 - acc: 0.1346 - val_loss: 0.1778 - val_acc: 0.1432\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17776, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 2/200\n",
      "1719168/1719168 [==============================] - 42s 24us/step - loss: 0.1732 - acc: 0.1677 - val_loss: 0.1715 - val_acc: 0.1862\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17776 to 0.17155, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 3/200\n",
      "1719168/1719168 [==============================] - 42s 24us/step - loss: 0.1690 - acc: 0.2000 - val_loss: 0.1684 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17155 to 0.16840, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 4/200\n",
      "1719168/1719168 [==============================] - 42s 24us/step - loss: 0.1666 - acc: 0.2282 - val_loss: 0.1664 - val_acc: 0.2334\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16840 to 0.16642, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 5/200\n",
      "1719168/1719168 [==============================] - 42s 24us/step - loss: 0.1651 - acc: 0.2443 - val_loss: 0.1655 - val_acc: 0.2343\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16642 to 0.16546, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 6/200\n",
      "1719168/1719168 [==============================] - 42s 24us/step - loss: 0.1642 - acc: 0.2495 - val_loss: 0.1647 - val_acc: 0.2441\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.16546 to 0.16469, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 7/200\n",
      "1718244/1719168 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.2597"
     ]
    }
   ],
   "source": [
    "hist_spae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/spae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = spae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_value_spae_sigmoid_adam_mse  = plot_hist_auto(hist_spae_sigmoid_adam_mse, './Figures/hist_spae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valueDict = {\n",
    "    'loss_value_ae_sigmoid_adam_mse': best_loss_value_ae_sigmoid_adam_mse,\n",
    "    'loss_value_spae_sigmoid_adam_mse': best_loss_value_spae_sigmoid_adam_mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc_train_x_asam.shape)\n",
    "print(enc_test_x_asam.shape)\n",
    "\n",
    "print(enc_train_x_spsam.shape)\n",
    "print(enc_test_x_spsam.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_asam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ae_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ae_ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = ae_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_asam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_ae_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_ae_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_ae_ann_2h_unisoftsigbinlosadam, './Figures/ae_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_ae_ann_2h_prob_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict(ae_ann_2h_unisoftsigbinlosadam,enc_test_x_asam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_asam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ae_ann_2h_prob_unisoftsigbinlosadam,pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_spsam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_sp_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = sp_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_spsam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_sp_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_sp_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_sp_ann_2h_unisoftsigbinlosadam, './Figures/sp_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict(sp_ann_2h_unisoftsigbinlosadam,enc_test_x_spsam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_spsam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sp_ann_2h_prob_unisoftsigbinlosadam,pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with no encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodr_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=train_x,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_nodr_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = nodr_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = train_x,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_nodr_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_nodr_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_nodr_ann_2h_unisoftsigbinlosadam, './Figures/nodr_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict(nodr_ann_2h_unisoftsigbinlosadam,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=train_x\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=test_x\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_ = PCA(n_components = 0.95, svd_solver = 'full').fit(train_x)\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# n_coml = [pca_.n_components_]\n",
    "\n",
    "# plt.plot(np.cumsum(pca_.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components', fontsize=14)\n",
    "# plt.ylabel('Variance (%)', fontsize=14) #for each component\n",
    "# plt.title('Pulsar Dataset Explained Variance '+str(dsnum)+' node DS', fontsize=14)\n",
    "\n",
    "# n_coml = [*n_coml]\n",
    "\n",
    "# for i, v in enumerate(n_coml):\n",
    "#     plt.text(v-0.8, i+0.94, '{:.0f}'.format(v), color='navy', fontsize=14)\n",
    "\n",
    "# plt.savefig('./Figures/PCA_components_ds'+str(dsnum)+'bal.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators=300, \n",
    "#                              criterion='gini', \n",
    "#                              max_depth=16, \n",
    "#                              #min_samples_split=2, \n",
    "#                              #min_samples_leaf=1, \n",
    "#                              max_features=0.3, \n",
    "#                              #bootstrap=True,\n",
    "#                              oob_score=True,\n",
    "#                              random_state=23)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(enc_train_x_asam, train_y)\n",
    "\n",
    "# pred_y_ae_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(enc_test_x_asam),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_ae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_ae_RF, pred_y_ae_RF, './Figures/ROC_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(enc_train_x_spsam, train_y)\n",
    "\n",
    "# pred_y_spae_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(enc_test_x_spsam),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_spae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_spae_RF, pred_y_spae_RF, './Figures/ROC_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with pca DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(train_x_pca, train_y)\n",
    "\n",
    "# pred_y_pca_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(test_x_pca),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_pca_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_pca_RF, pred_y_pca_RF, './Figures/ROC_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_ae_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_ae_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "# print(pred_y_ae_RF.shape)\n",
    "# print(pred_y_spae_RF.shape)\n",
    "# print(pred_y_pca_RF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate_ae_ann, recall_ae_ann, thresholds_ae_ann = roc_curve(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_ae_ann = auc(false_positive_rate_ae_ann, recall_ae_ann)\n",
    "false_positive_rate_sp_ann, recall_sp_ann, thresholds_sp_ann = roc_curve(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_sp_ann = auc(false_positive_rate_sp_ann, recall_sp_ann)\n",
    "false_positive_rate_nodr_ann, recall_nodr_ann, thresholds_nodr_ann = roc_curve(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_nodr_ann = auc(false_positive_rate_nodr_ann, recall_nodr_ann)\n",
    "\n",
    "# false_positive_rate_ae_RF, recall_ae_RF, thresholds_ae_RF = roc_curve(test_y, pred_y_ae_RF)\n",
    "# roc_auc_ae_RF = auc(false_positive_rate_ae_RF, recall_ae_RF)\n",
    "# false_positive_rate_spae_RF, recall_spae_RF, thresholds_spae_RF = roc_curve(test_y, pred_y_spae_RF)\n",
    "# roc_auc_spae_RF = auc(false_positive_rate_spae_RF, recall_spae_RF)\n",
    "# false_positive_rate_pca_RF, recall_pca_RF, thresholds_pca_RF = roc_curve(test_y, pred_y_pca_RF)\n",
    "# roc_auc_pca_RF = auc(false_positive_rate_pca_RF, recall_pca_RF)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC)', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "# plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "# plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "# plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "# plt.ylim([0.97,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC) Zoom in', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "# plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "# plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "# plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "# plt.ylim([0.0,1.0])\n",
    "plt.ylim([0.955,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal_zoom.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ae_ann = \"AE+DNN\"\n",
    "acc_ae_ann = (sm.accuracy_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_ae_ann = (sm.precision_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_ae_ann = (sm.recall_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_ae_ann = (sm.f1_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_sp_ann = \"SAE+DNN\"\n",
    "acc_sp_ann = (sm.accuracy_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_sp_ann = (sm.precision_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_sp_ann = (sm.recall_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_sp_ann = (sm.f1_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_nodr_ann = \"DNN\"\n",
    "acc_nodr_ann = (sm.accuracy_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_nodr_ann = (sm.precision_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_nodr_ann = (sm.recall_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_nodr_ann = (sm.f1_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "# classi_ae_RF = \"AE+RF\"\n",
    "# acc_ae_RF = (sm.accuracy_score(test_y, pred_y_ae_RF)*100) \n",
    "# pre_ae_RF = (sm.precision_score(test_y, pred_y_ae_RF)*100) \n",
    "# recall_ae_RF = (sm.recall_score(test_y, pred_y_ae_RF)*100) \n",
    "# f1score_ae_RF = (sm.f1_score(test_y, pred_y_ae_RF)*100)\n",
    "\n",
    "# classi_spae_RF = \"SAE+RF\"\n",
    "# acc_spae_RF = (sm.accuracy_score(test_y, pred_y_spae_RF)*100) \n",
    "# pre_spae_RF = (sm.precision_score(test_y, pred_y_spae_RF)*100) \n",
    "# recall_spae_RF = (sm.recall_score(test_y, pred_y_spae_RF)*100) \n",
    "# f1score_spae_RF = (sm.f1_score(test_y, pred_y_spae_RF)*100)\n",
    "\n",
    "# classi_pca_RF = \"PCA+RF\"\n",
    "# acc_pca_RF = (sm.accuracy_score(test_y, pred_y_pca_RF)*100) \n",
    "# pre_pca_RF = (sm.precision_score(test_y, pred_y_pca_RF)*100) \n",
    "# recall_pca_RF = (sm.recall_score(test_y, pred_y_pca_RF)*100) \n",
    "# f1score_pca_RF = (sm.f1_score(test_y, pred_y_pca_RF)*100)\n",
    "\n",
    "\n",
    "print('Classifier\\tAcc\\tPreci\\tRecall\\tF1Score')\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_ann, acc_ae_ann, pre_ae_ann, recall_ae_ann, f1score_ae_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_sp_ann, acc_sp_ann, pre_sp_ann, recall_sp_ann, f1score_sp_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_nodr_ann, acc_nodr_ann, pre_nodr_ann, recall_nodr_ann, f1score_nodr_ann))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_RF, acc_ae_RF, pre_ae_RF, recall_ae_RF, f1score_ae_RF))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_spae_RF, acc_spae_RF, pre_spae_RF, recall_spae_RF, f1score_spae_RF))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_pca_RF, acc_pca_RF, pre_pca_RF, recall_pca_RF, f1score_pca_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1list = [[\"AE+DNN\",f1score_ae_ann],[\"SAE+DNN\",f1score_sp_ann],[\"DNN\",f1score_nodr_ann]]#,\n",
    "#           [\"AE+RF\",f1score_ae_RF],[\"SAE+RF\",f1score_spae_RF],[\"PCA+RF\",f1score_pca_RF]]\n",
    "\n",
    "xs, ys = [*zip(*f1list)]\n",
    "\n",
    "'{:.2f}'.format(f1score_ae_ann)\n",
    "\n",
    "plt.figure(figsize=(8,6), )\n",
    "plt.barh(xs, ys, color = \"purple\")\n",
    "plt.title(\"F1 score vs Classifier\", fontsize=16)\n",
    "plt.xlabel(\"Classifier\", fontsize=14)\n",
    "plt.ylabel(\"F1 score\", fontsize=14)\n",
    "plt.xticks(np.arange(0, 101, 10), fontsize=12)\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "for i, v in enumerate(ys):\n",
    "    plt.text(v+1, i+0.1, '{:.2f}'.format(v), color='purple', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/F1scoreplot_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
