{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Donâ€™t pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "from keras import optimizers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Activation, Dense, Dropout, Input\n",
    "from keras import backend as k\n",
    "\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "#k.tensorflow_backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------Import modules------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(23)\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from datetime import datetime \n",
    "\n",
    "import multiprocessing\n",
    "# njobscpu = multiprocessing.cpu_count() - 2\n",
    "njobscpu = 7\n",
    "\n",
    "\n",
    "dsnum=50\n",
    "verbose_level=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathds = '/home/user/01Code/00Datasets_final/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_chunk = pd.read_csv(pathds+'SubsetAllSamples/ThirdCloneID10bal_minmax.csv', chunksize=1000)\n",
    "#df = df_chunk.get_chunk(300000)\n",
    "df = pd.read_csv(pathds+\"00BalancedDS/FullCloneID\"+str(dsnum)+\"bal_minmax.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2078832, 211)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "neurons=df.shape[1]-1\n",
    "batch_size=df.shape[1]-1\n",
    "print(neurons)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Explaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1039416\n",
      "Class 1: 1039416\n",
      "Proportion: 1.0 : 1\n"
     ]
    }
   ],
   "source": [
    "#if you don't have an intuitive sense of how imbalanced these two classes are, let's go visual\n",
    "count_classes = pd.value_counts(df['class'], sort = True)\n",
    "print('Class 0:', count_classes[0])\n",
    "print('Class 1:', count_classes[1])\n",
    "print('Proportion:', round(count_classes[0] / count_classes[1], 3), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.xticks(range(2), ['Normal [0]','Malicious [1]'])\n",
    "plt.title(\"Frequency by observation number\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Observations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed and percentage of test data\n",
    "RANDOM_SEED = 23 #used to help randomly select the data points\n",
    "TEST_PCT = 0.20 # 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_df = train_test_split(df, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ df -> original dataset \n",
    "+ train -> subset of 80% from original dataset \n",
    "+ test_df -> subset of 20% from original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(train, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train -> subset of 80% from original dataset \n",
    "+ train_df -> subset of 80% from train\n",
    "+ dev_df -> subset of 20% from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49990228884619664\n",
      "0.500260061993969\n",
      "0.5001046259082611\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of mal samples in train and test set/\n",
    "print(train_df.iloc[:, batch_size].sum()/train_df.shape[0]) \n",
    "print(dev_df.iloc[:, batch_size].sum()/dev_df.shape[0]) \n",
    "print(test_df.iloc[:, batch_size].sum()/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.iloc[:, :batch_size] \n",
    "dev_x = dev_df.iloc[:, :batch_size] \n",
    "test_x = test_df.iloc[:, :batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_x -> features of train_df **Training subset for AE**\n",
    "+ dev_x -> features of dev_df **Validation subset for AE**\n",
    "+ test_x -> features of test_df **Testing subset for ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final train and test sets\n",
    "train_y = train_df.iloc[:,batch_size]\n",
    "dev_y = dev_df.iloc[:,batch_size]\n",
    "test_y = test_df.iloc[:,batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_y -> **Labels for supervised training of ANN**\n",
    "+ dev_y -> labels of dev_df  *not used for AE neither ANN*\n",
    "+ test_y -> labels of test_df  **Ground Truth for predictions of supervised ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "train_x =np.array(train_x)\n",
    "dev_x =np.array(dev_x)\n",
    "test_x = np.array(test_x)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "dev_y = np.array(dev_y)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "print(train_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(factor_enc_dim, enc_activation, dec_activation, \n",
    "                optimizer, loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer #RELU\n",
    "    encoded = Dense(encoding_dim, activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer #SIMOID\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spae(factor_enc_dim,dec_activation,enc_activation,\n",
    "         optimizer,loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer\n",
    "    encoded = Dense(encoding_dim, activity_regularizer=regularizers.l1(1e-4), activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pca(thr):\n",
    "    #train_x_pca,test_x_pca = to_pca(0.95)\n",
    "    pca = PCA(n_components = thr, svd_solver = 'full')\n",
    "    train_x_ = np.array(train_x)\n",
    "    print(type(train_x_))\n",
    "\n",
    "    test_x_ = np.array(test_x)\n",
    "    print(type(test_x_))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(time.ctime(start_time))\n",
    "\n",
    "    train_x_pca = pca.fit_transform(train_x_)\n",
    "    print(train_x_pca.shape)\n",
    "\n",
    "    test_x_pca = pca.fit_transform(test_x_)\n",
    "    print(test_x_pca.shape)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "\n",
    "    print(\"--- PCA spent %s seconds ---\" %elapsed_time )\n",
    "    \n",
    "    return  train_x_pca,test_x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ae(checkpoint_file, autoencoder,\n",
    "           epochs, batch_size, shuffle):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    hist_auto = autoencoder.fit(train_x, train_x,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    verbose=verbose_level,\n",
    "                    callbacks=[early_stopping, cp, tb],\n",
    "                    validation_data=(dev_x, dev_x))\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "    \n",
    "    return hist_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_auto(hist_auto, fig_file):\n",
    "    best_loss_value = hist_auto.history['loss'][-1]\n",
    "    print('Best loss value:', best_loss_value)\n",
    "\n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist_auto.history['loss'])\n",
    "    plt.plot(hist_auto.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.savefig(fig_file)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h(neurons,encoded_train_x,init_mode,activation_input,\n",
    "               weight_constraint,dropout_rate,activation_output,\n",
    "               loss,optimizer):\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=encoded_train_x.shape[1],\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h_():\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=input_dim,\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(checkpoint_file,ann,enc_train_x,train_y,epochs,shuffle,batch_size):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    history = ann.fit(enc_train_x,\n",
    "                      train_y,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[early_stopping, cp, tb],\n",
    "                      epochs=epochs,\n",
    "                      shuffle=shuffle,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=verbose_level)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict(ann,enc_test_x):\n",
    "    pred_ann_prob = ann.predict(enc_test_x)\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "    \n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob, pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict_():\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))  \n",
    "\n",
    "    modelk = KerasClassifier(build_fn=ann_2h_,\n",
    "                             epochs=epochs, \n",
    "                             batch_size=batch_size, \n",
    "                             verbose=verbose_level\n",
    "                            )\n",
    "\n",
    "    pred_ann_prob = cross_val_predict(modelk,\n",
    "                                      enc_test_x,\n",
    "                                      test_y,\n",
    "                                      cv=KFold(n_splits=5, random_state=23),\n",
    "                                      verbose=1,\n",
    "                                      n_jobs=njobscpu)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "\n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob,pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cm(pred_ann_prob, pred_ann_01, roc_file, cm_file):\n",
    "    false_positive_rate, recall, thresholds = roc_curve(test_y, pred_ann_prob)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.savefig(roc_file)\n",
    "    plt.show()\n",
    "    \n",
    "    cm = confusion_matrix(test_y, pred_ann_01)\n",
    "    labels = ['Normal', 'Malicious']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=\"RdYlGn\", vmin = 0.2);\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.savefig(cm_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- PCA Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Sat Aug 24 19:19:36 2019\n",
      "(1330452, 132)\n",
      "(415767, 132)\n",
      "--- PCA spent 466.674170255661 seconds ---\n"
     ]
    }
   ],
   "source": [
    "train_x_pca,test_x_pca = to_pca(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- AE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 19:27:30.079811 140578914449216 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0824 19:27:32.356395 140578914449216 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0824 19:28:00.472543 140578914449216 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0824 19:28:19.413132 140578914449216 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0824 19:28:23.971008 140578914449216 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 140)               29540     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 210)               29610     \n",
      "=================================================================\n",
      "Total params: 59,150\n",
      "Trainable params: 59,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 19:28:25.433808 140578914449216 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ae_sigmoid_adam_mse,enc_train_x_asam,enc_test_x_asam = ae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ae_sigmoid_adam_mse = load_model('ae_sigmoid_adam_mse_ds10bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 24 19:30:14 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 19:30:15.344754 140578914449216 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0824 19:30:15.355398 140578914449216 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1330452 samples, validate on 332613 samples\n",
      "Epoch 1/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0056 - acc: 0.0421 - val_loss: 2.9932e-04 - val_acc: 0.0114\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00030, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 2/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.1660e-04 - acc: 0.0056 - val_loss: 3.4390e-05 - val_acc: 0.0031\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00030 to 0.00003, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 3/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 2.2005e-05 - acc: 0.0039 - val_loss: 1.4267e-05 - val_acc: 0.0033\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00003 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 4/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.3646e-05 - acc: 0.0032 - val_loss: 1.3469e-05 - val_acc: 0.0024\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 5/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.3178e-05 - acc: 0.0021 - val_loss: 1.3181e-05 - val_acc: 0.0014\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 6/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2988e-05 - acc: 0.0015 - val_loss: 1.3008e-05 - val_acc: 0.0013\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 7/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2866e-05 - acc: 0.0013 - val_loss: 1.2879e-05 - val_acc: 0.0011\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 8/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.2780e-05 - acc: 0.0011 - val_loss: 1.2786e-05 - val_acc: 5.9228e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 9/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2717e-05 - acc: 6.9751e-04 - val_loss: 1.2709e-05 - val_acc: 5.8627e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 10/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.2664e-05 - acc: 6.9225e-04 - val_loss: 1.2643e-05 - val_acc: 6.0130e-04\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 11/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2617e-05 - acc: 7.0878e-04 - val_loss: 1.2582e-05 - val_acc: 6.2836e-04\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 12/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2576e-05 - acc: 7.0352e-04 - val_loss: 1.2544e-05 - val_acc: 6.1333e-04\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 13/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2539e-05 - acc: 6.9300e-04 - val_loss: 1.2504e-05 - val_acc: 6.1333e-04\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 14/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2503e-05 - acc: 6.8849e-04 - val_loss: 1.2466e-05 - val_acc: 6.1333e-04\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 15/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2466e-05 - acc: 6.7947e-04 - val_loss: 1.2429e-05 - val_acc: 5.8627e-04\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 16/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 1.2433e-05 - acc: 6.5016e-04 - val_loss: 1.2389e-05 - val_acc: 5.5620e-04\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 17/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2397e-05 - acc: 6.2310e-04 - val_loss: 1.2354e-05 - val_acc: 5.2614e-04\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 18/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2364e-05 - acc: 6.0731e-04 - val_loss: 1.2319e-05 - val_acc: 5.2614e-04\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 19/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.2332e-05 - acc: 5.8852e-04 - val_loss: 1.2287e-05 - val_acc: 4.8705e-04\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 20/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.2299e-05 - acc: 5.3290e-04 - val_loss: 1.2276e-05 - val_acc: 4.2392e-04\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 21/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.2263e-05 - acc: 4.9607e-04 - val_loss: 1.2228e-05 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 22/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2233e-05 - acc: 4.7427e-04 - val_loss: 1.2198e-05 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 23/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2201e-05 - acc: 4.4421e-04 - val_loss: 1.2156e-05 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 24/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2167e-05 - acc: 4.3594e-04 - val_loss: 1.2132e-05 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 25/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2132e-05 - acc: 4.3594e-04 - val_loss: 1.2106e-05 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 26/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.2097e-05 - acc: 4.3594e-04 - val_loss: 1.2083e-05 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 27/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.2057e-05 - acc: 4.3669e-04 - val_loss: 1.2060e-05 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 28/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.2018e-05 - acc: 4.3970e-04 - val_loss: 1.2044e-05 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 29/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1978e-05 - acc: 4.4196e-04 - val_loss: 1.2028e-05 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 30/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1937e-05 - acc: 4.4947e-04 - val_loss: 1.1987e-05 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 31/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1899e-05 - acc: 4.7202e-04 - val_loss: 1.1946e-05 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 32/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1852e-05 - acc: 4.7653e-04 - val_loss: 1.1938e-05 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 33/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1805e-05 - acc: 4.7803e-04 - val_loss: 1.1930e-05 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 34/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1760e-05 - acc: 4.7878e-04 - val_loss: 1.1933e-05 - val_acc: 4.1790e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00001\n",
      "Epoch 35/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1715e-05 - acc: 8.4633e-04 - val_loss: 1.1912e-05 - val_acc: 7.9372e-04\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 36/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1668e-05 - acc: 9.0270e-04 - val_loss: 1.1878e-05 - val_acc: 7.9372e-04\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 37/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1614e-05 - acc: 9.0420e-04 - val_loss: 1.1853e-05 - val_acc: 8.1175e-04\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 38/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1551e-05 - acc: 9.0496e-04 - val_loss: 1.1805e-05 - val_acc: 8.1476e-04\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 39/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1479e-05 - acc: 9.2375e-04 - val_loss: 1.1756e-05 - val_acc: 8.3581e-04\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 40/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1397e-05 - acc: 9.3878e-04 - val_loss: 1.1657e-05 - val_acc: 8.5084e-04\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 41/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 1.1278e-05 - acc: 9.5080e-04 - val_loss: 1.1454e-05 - val_acc: 8.7790e-04\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 42/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 1.0642e-05 - acc: 8.2077e-04 - val_loss: 1.0176e-05 - val_acc: 4.6601e-04\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 43/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 9.8463e-06 - acc: 5.1486e-04 - val_loss: 9.9629e-06 - val_acc: 4.2091e-04\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 44/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 9.7221e-06 - acc: 4.7052e-04 - val_loss: 9.9006e-06 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 45/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 9.6416e-06 - acc: 4.3745e-04 - val_loss: 9.8378e-06 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 46/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 9.5625e-06 - acc: 4.3669e-04 - val_loss: 9.7481e-06 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 47/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 9.4538e-06 - acc: 4.3669e-04 - val_loss: 9.6103e-06 - val_acc: 3.8483e-04\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 48/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 9.2729e-06 - acc: 4.5097e-04 - val_loss: 9.3923e-06 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 49/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 9.0564e-06 - acc: 4.7277e-04 - val_loss: 9.1033e-06 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 50/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 8.7842e-06 - acc: 4.7427e-04 - val_loss: 8.7588e-06 - val_acc: 4.1189e-04\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 51/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 8.5888e-06 - acc: 4.8179e-04 - val_loss: 8.6085e-06 - val_acc: 4.5398e-04\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 52/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 8.4750e-06 - acc: 4.9457e-04 - val_loss: 8.4840e-06 - val_acc: 4.5398e-04\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 53/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 8.3837e-06 - acc: 5.1637e-04 - val_loss: 8.3203e-06 - val_acc: 4.7202e-04\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 54/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 8.3029e-06 - acc: 5.3290e-04 - val_loss: 8.1783e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 55/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 8.2333e-06 - acc: 5.5019e-04 - val_loss: 8.0759e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 56/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 8.1691e-06 - acc: 5.5019e-04 - val_loss: 7.9828e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 57/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 8.0962e-06 - acc: 5.5019e-04 - val_loss: 7.8978e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 58/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 8.0220e-06 - acc: 5.5019e-04 - val_loss: 7.8253e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 59/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.9525e-06 - acc: 5.5019e-04 - val_loss: 7.7647e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 60/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.8928e-06 - acc: 5.5019e-04 - val_loss: 7.7108e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 61/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.8365e-06 - acc: 5.5019e-04 - val_loss: 7.6544e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 62/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.7805e-06 - acc: 5.5019e-04 - val_loss: 7.5976e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 63/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.7112e-06 - acc: 5.5019e-04 - val_loss: 7.5334e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 64/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 7.6559e-06 - acc: 5.5019e-04 - val_loss: 7.4913e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 65/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 7.6087e-06 - acc: 5.5019e-04 - val_loss: 7.4430e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 66/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 7.5565e-06 - acc: 5.5019e-04 - val_loss: 7.3833e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 67/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 7.4970e-06 - acc: 5.5019e-04 - val_loss: 7.3144e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 68/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.4205e-06 - acc: 5.5019e-04 - val_loss: 7.2095e-06 - val_acc: 4.9607e-04\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 69/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.2975e-06 - acc: 6.5316e-04 - val_loss: 7.1123e-06 - val_acc: 8.2979e-04\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 70/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.1658e-06 - acc: 9.1848e-04 - val_loss: 6.9944e-06 - val_acc: 8.7790e-04\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 71/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 7.0319e-06 - acc: 9.7335e-04 - val_loss: 6.8785e-06 - val_acc: 8.7790e-04\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 72/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 6.9281e-06 - acc: 9.7411e-04 - val_loss: 6.7785e-06 - val_acc: 8.9293e-04\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 73/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 6.8391e-06 - acc: 0.0010 - val_loss: 6.6726e-06 - val_acc: 9.4404e-04\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 74/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 6.7498e-06 - acc: 0.0010 - val_loss: 6.5459e-06 - val_acc: 0.0010\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 75/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 6.6512e-06 - acc: 0.0011 - val_loss: 6.4411e-06 - val_acc: 0.0011\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 76/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 6.5747e-06 - acc: 0.0012 - val_loss: 6.3491e-06 - val_acc: 0.0012\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 77/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 6.4952e-06 - acc: 0.0012 - val_loss: 6.2621e-06 - val_acc: 0.0012\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 78/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 6.4397e-06 - acc: 0.0013 - val_loss: 6.2065e-06 - val_acc: 0.0012\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 79/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 6.3849e-06 - acc: 0.0013 - val_loss: 6.1510e-06 - val_acc: 0.0013\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 80/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 6.3341e-06 - acc: 0.0014 - val_loss: 6.1091e-06 - val_acc: 0.0014\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 81/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 6.2468e-06 - acc: 0.0015 - val_loss: 6.0299e-06 - val_acc: 0.0015\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 82/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 6.1244e-06 - acc: 0.0015 - val_loss: 5.9527e-06 - val_acc: 0.0016\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 83/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 6.0103e-06 - acc: 0.0017 - val_loss: 5.8634e-06 - val_acc: 0.0016\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 84/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.9537e-06 - acc: 0.0017 - val_loss: 5.7906e-06 - val_acc: 0.0016\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 85/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.8965e-06 - acc: 0.0017 - val_loss: 5.7598e-06 - val_acc: 0.0017\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 86/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.8547e-06 - acc: 0.0018 - val_loss: 5.7213e-06 - val_acc: 0.0016\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 87/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.8141e-06 - acc: 0.0016 - val_loss: 5.6338e-06 - val_acc: 0.0015\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 88/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 5.7780e-06 - acc: 0.0016 - val_loss: 5.6381e-06 - val_acc: 0.0016\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00001\n",
      "Epoch 89/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 5.7471e-06 - acc: 0.0016 - val_loss: 5.6222e-06 - val_acc: 0.0016\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 90/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.7212e-06 - acc: 0.0018 - val_loss: 5.6247e-06 - val_acc: 0.0020\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00001\n",
      "Epoch 91/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.6957e-06 - acc: 0.0020 - val_loss: 5.5915e-06 - val_acc: 0.0021\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 92/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.6562e-06 - acc: 0.0021 - val_loss: 5.5649e-06 - val_acc: 0.0021\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 93/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.6281e-06 - acc: 0.0021 - val_loss: 5.5191e-06 - val_acc: 0.0023\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 94/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.6034e-06 - acc: 0.0022 - val_loss: 5.4299e-06 - val_acc: 0.0023\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 95/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.5790e-06 - acc: 0.0023 - val_loss: 5.4181e-06 - val_acc: 0.0025\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 96/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 5.5241e-06 - acc: 0.0024 - val_loss: 5.3437e-06 - val_acc: 0.0025\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 97/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.4876e-06 - acc: 0.0025 - val_loss: 5.3025e-06 - val_acc: 0.0026\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 98/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.4668e-06 - acc: 0.0026 - val_loss: 5.2948e-06 - val_acc: 0.0027\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 99/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.4495e-06 - acc: 0.0027 - val_loss: 5.2747e-06 - val_acc: 0.0027\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 100/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.4268e-06 - acc: 0.0028 - val_loss: 5.2561e-06 - val_acc: 0.0028\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 101/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.3956e-06 - acc: 0.0029 - val_loss: 5.2303e-06 - val_acc: 0.0029\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 102/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 5.3760e-06 - acc: 0.0030 - val_loss: 5.2079e-06 - val_acc: 0.0030\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 103/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 5.3571e-06 - acc: 0.0030 - val_loss: 5.1855e-06 - val_acc: 0.0030\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00001 to 0.00001, saving model to ./H5files/ae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 104/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.3381e-06 - acc: 0.0030 - val_loss: 5.2044e-06 - val_acc: 0.0030\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00001\n",
      "Epoch 105/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.3181e-06 - acc: 0.0030 - val_loss: 5.2680e-06 - val_acc: 0.0030\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00001\n",
      "Epoch 106/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.2986e-06 - acc: 0.0030 - val_loss: 5.2410e-06 - val_acc: 0.0030\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00001\n",
      "Epoch 107/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.2802e-06 - acc: 0.0030 - val_loss: 5.1950e-06 - val_acc: 0.0030\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00001\n",
      "Epoch 108/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 5.2634e-06 - acc: 0.0031 - val_loss: 5.1973e-06 - val_acc: 0.0030\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00001\n",
      "Time elapsed (hh:mm:ss.ms) 0:26:09.171331\n"
     ]
    }
   ],
   "source": [
    "hist_ae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/ae_sigmoid_adam_mse_ds\"+str(dsnum)+\"bal_minmax.h5\",\n",
    "                                  autoencoder = ae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size*2,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 5.2634154415754054e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3df5xVdb3v8df+AcNgCIJgyaBOiZ87WKlR6snslB59oIek7oHAjohF1k256LVzO3rPuR7zEaV50LgnqRRLMJO4ZDfykmhZdv2FOEmdcPc5IoIMoigov2dkZvb9Y609s2fP/jnMmh+b9/Px4MHea32/a3+/s2G/57u++7tWLJ1OIyIiUq54fzdAREQGFwWHiIhURMEhIiIVUXCIiEhFFBwiIlIRBYeIiFQk2d8NEKlGZnYS8DIwxN1bS5S9Aviiu3/scI4j0lcUHHLEM7PNwPHA8e7+Ztb29cBpQL27b+6XxokMQDpVJRJ4Gbg088TMPgDU9l9zRAYujThEAvcBlwP/Fj6fAywDvpEpYGYjw/0XAQeAu4Fvunu7mSWAW4ErgD3AwuyDh3VvBy4G2oEfAf/i7m2VNNLMjge+D3wM2AXc6u53h/vOBBYDpwAHgfvd/TozGwYsCdudAF4Eprr765W8tkiGRhwigWeAo82sIQyBmcCPc8r8GzASeC/w1wRB8/lw35XAVOAM4MPA9Jy6S4FW4OSwzIXAF3vQzgeAJoJTa9OBb5rZ+eG+RcAidz8aeB+wItw+J2z3BGAM8F8IgkWkRzTiEOmUGXU8DvwF2JbZkRUmZ7j7XmCvmS0EZgP3AJ8FvuPuW8Py3wI+ET4+juC3/VHufhDYb2Z3AF8CflBu48xsAsFIY6q7NwPrzWxJ2IbfAIeAk83s2HCu5pmw6iGCwDjZ3f8ENFb6gxHJpuAQ6XQf8HugnuA0VbZjgaHAlqxtW4Dx4ePjga05+zJOBIYA280ssy2eU74cxwO7wuDKfp0Ph4/nAjcDfzGzl4Gvu/tDYb8mAMvNbBTBSOqf3P1Qha8vAig4RDq4+5bwA/digg/hbG8S/OZ+IvBCuO0EOkcl2wk+nMnal7EVaAGOPcyv1L4KjDazEVnh0dEGd38RuNTM4sB/Blaa2Rh33w98Hfh6+PXe1YATjJREKqY5DpGu5gLnhR+2HcJJ7BXAAjMbYWYnAtfROQ+yAphvZnVmdgxwfVbd7cAjwEIzO9rM4mb2PjP760oaFp4Gewr4lpkNM7MPhu29H8DMLjOzse7eDrwdVmszs0+a2QfC0217CAKwokl5kWwKDpEs7v6Suz9XYPd/BfYDm4AngJ8APwz33Q2sAf4I/AF4MKfu5QSnul4A3gJWAu/pQRMvBU4iGH38nOCbWY+G+6YAG8xsH8FE+axwLuTd4evtAVIEczi5E/8iZYvpRk4iIlIJjThERKQiCg4REamIgkNERCqi4BARkYocEes41q9fn66pqelR3ZaWFnpad7BQHwe/au8fqI997cCBA29Onjx5bL59R0Rw1NTU0NDQ0KO6qVSqx3UHC/Vx8Kv2/oH62NcaGxu3FNqnU1UiIlIRBYeIiFREwSEiIhU5IuY4REQqdejQIZqammhubu7T10ylUn32egDDhg2jrq6OIUOGlF1HwSEikkdTUxMjRozgpJNOIhaL9clrHjx4kNravrtjcTqdZufOnTQ1NVFfX192PZ2qEhHJo7m5mTFjxvRZaPSHWCzGmDFjKh5VKThERAqo5tDI6EkfFRxF/NZ3sGPf4dx3R0Sk+ig4ipj/wPM85Lv7uxkicgTas2cP999/f8X1rrzySvbs2RNBizopOIpJw6E23a9ERPrenj17eOCBB7ptb2srfvPGu+++m6OPPjqqZgH6VlVRiUQM5YaI9IeFCxfyyiuvMG3aNJLJJMOHD2fcuHGkUilWr17NVVddxWuvvUZLSwuXX345M2fOBOC8885j5cqVHDhwgCuvvJLJkyfz/PPPc9xxx7F48WKGDRt22G1TcBSRjMdoa1dyiBzpftbYxIrntvbqMT/74Qn83eS6gvu/+tWv8uKLL/KLX/yCtWvX8uUvf5lf/vKXTJgwAYBvfvObjBo1iubmZqZPn86FF17IMccc0+UYW7Zs4fbbb+cb3/gG11xzDWvWrGHatGmH3XYFRxHxWAzlhogMBB/4wAc6QgPgvvvu49FHg9vNb9++nS1btnQLjrq6uo6LJp566qls27atV9qi4CgiGY/Rpnuyixzx/m5yXdHRQV8YPnx4x+O1a9fy1FNP8dOf/pTa2lpmz55NS0tLtzpDhw7teJxIJPKW6QlNjheRSMRob+/vVojIkeioo45i//79efft3buXkSNHUltby0svvcT69ev7tG0acRSRjMc14hCRfnHMMcfwoQ99iKlTp1JTU8Oxxx7bse/jH/84y5cv51Of+hT19fWcfvrpfdo2BUcR8Ri0acQhIv1k4cKFebcPHTqUJUuW5N332GOPATB69Ggeeuihju1z587ttXbpVFURyXicdo04RES6UHAUkYhrHYeISC4FRxHJhNZxiIjkUnAUEY9pxCEikkvBUUQyrgWAIiK5FBxFJHTJERGRbhQcRSQTWjkuIv2jp5dVB7j33ns5ePBgL7eok4KjiHgspnUcItIvCl1WvRzLli2LNDi0ALCIYI5DIw4R6XvZl1X/6Ec/ypgxY/jVr37FO++8wwUXXMD8+fM5cOAA1157La+99hrt7e1cddVVvPnmm+zYsYM5c+YwatQo7rvvvl5vW6TBYWZTgEVAAlji7rfk7K8BlgGTgZ3ATHffHO67AZgLtAHz3X1NuH0zsDfc3uruH46q/Yl4XN+qEhFY/wA8/+PePeYZl8HplxbcnX1Z9SeeeII1a9awcuVK0uk0X/nKV1i3bh27du1i3Lhx3HXXXUBwDasRI0Zw7733snTpUkaPHt27bQ5FdqrKzBLAncBFwCTgUjOblFNsLvCWu58M3AHcGtadBMwCTgWmAIvD42V80t1PjzI0IBxxaHJcRPrZk08+yZNPPsmnP/1pPvOZz7Bp0yY2b97MKaecwlNPPcVtt93Gc889x4gRI/qkPVGOOM4ENrr7JgAzWw5MA17IKjMNuCl8vBL4rpnFwu3L3b0FeNnMNobHezrC9najleMiAgQjgyKjg6il02m+9KUvMWvWrG77HnzwQR5//HEWLlzIOeecw7x58yJvT5TBMR7IvmVWE3BWoTLu3mpmu4Ex4fZncuqODx+ngUfMLA38wN3vKtWQlpYWUqlUxR3Yv28PrW3tPao7mDQ3N6uPg1y19w/6vo+HDh2KdII5n3Q63fGaiUSCffv2cfDgQT7ykY+wePFiLrjgAoYPH87rr7/OkCFDaG1tZeTIkVx44YUkk0lWrVrFwYMHqa2tZdeuXdTW1pb1uocOHaroZxtlcMTybMv9/b1QmWJ1z3H3V81sHPComf3F3X9frCE1NTUdd8GqxOg/tZB+4/Ue1R1MUqmU+jjIVXv/oO/7mEqlyv7g7S2ZD32A2tpaJk+ezIwZMzj33HO55JJLuOKKK4Dgpk633XYbr7zyCt/+9reJx+Mkk0luuukmamtrmTVrFvPmzWPs2LFlTY4PGTKk28+2sbGxYPkog6MJmJD1vA54tUCZJjNLAiOBXcXqunvm7x1m9nOCU1hFg6OnEroDoIj0o9zLqs+ZM6fL8xNOOIFzzz23W73Zs2cze/bsyNoV5TqOdcBEM6s3s6EEk92rcsqsAjI/ienAY+6eDrfPMrMaM6sHJgLPmtlRZjYCwMyOAi4E/hxVB4KV41EdXURkcIosONy9FZgHrAFSwAp332BmN5vZJWGxe4Ax4eT3dcD1Yd0NwAqCifSHgavdvQ04DnjCzP4IPAv8X3d/OKo+aMQhItJdpOs43H01sDpn241Zj5uBGQXqLgAW5GzbBJzW+y3NTxc5FDmypdNpYrF8U67VI92DX451yZEiEvG4LnIocoQaNmwYO3fu7NEH62CRTqfZuXMnw4YNq6ieLjlSRHCRw/5uhYj0h7q6OpqamnjjjTf67DUPHTrEkCFD+uz1IAjIurq6iuooOIoILnKo5BA5Eg0ZMoT6+vo+fc3B8rVqnaoqQnMcIiLdKTiKSITBUc3nOEVEKqXgKCIZD75NodNVIiKdFBxFxMPgaFVwiIh0UHAUkRlx6GZOIiKdFBxFJDTiEBHpRsFRRMcchxZziIh0UHAUoRGHiEh3Co4iEvHgx6M5DhGRTgqOIpIacYiIdKPgKCKhOQ4RkW4UHEV0znHobk4iIhkKjiISWschItKNgqMIzXGIiHSn4Cii41SV5jhERDooOIpI6CKHIiLdKDiK6AgOzXGIiHRQcBSRDBcAasQhItJJwVGE5jhERLpTcBShOQ4Rke4UHEVoAaCISHcKjiJ0IycRke4UHEVojkNEpDsFRxGa4xAR6S4Z5cHNbAqwCEgAS9z9lpz9NcAyYDKwE5jp7pvDfTcAc4E2YL67r8mqlwCeA7a5+9So2q9LjoiIdBfZiCP8cL8TuAiYBFxqZpNyis0F3nL3k4E7gFvDupOAWcCpwBRgcXi8jGuAVFRtz9BFDkVEuovyVNWZwEZ33+Tu7wDLgWk5ZaYBS8PHK4HzzSwWbl/u7i3u/jKwMTweZlYH/C2wJMK2A50LADXHISLSKcpTVeOBrVnPm4CzCpVx91Yz2w2MCbc/k1N3fPj4O8DXgBHlNqSlpYVUqvIByuv7DgGwdds2UsP3Vlx/sGhubu7Rz2cwqfY+Vnv/QH0cSKIMjliebbm/uhcqk3e7mU0Fdrh7o5l9otyG1NTU0NDQUG7xDsfsbga2Mu6499DQcELF9QeLVCrVo5/PYFLtfaz2/oH62NcaGxsL7ovyVFUTMCHreR3waqEyZpYERgK7itQ9B7jEzDYTnPo6z8x+HEHbAV3kUEQknyhHHOuAiWZWD2wjmOz+XE6ZVcAc4GlgOvCYu6fNbBXwEzO7HTgemAg86+5PAzcAhCOOf3D3y6LqQLLjnuNaOS4ikhHZiMPdW4F5wBqCb0CtcPcNZnazmV0SFrsHGGNmG4HrgOvDuhuAFcALwMPA1e7eFlVbC0kk9HVcEZFcka7jcPfVwOqcbTdmPW4GZhSouwBYUOTYvwN+1xvtLCQR0wJAEZFcWjlehOY4RES6U3AU0TnHoeAQEclQcBSR0CVHRES6UXAUEYvFiMc0xyEikk3BUUI8pjkOEZFsCo4SEvGYRhwiIlkUHCUkYrrIoYhINgVHCfFYjDbdc1xEpIOCo4REXHMcIiLZFBwlJGKa4xARyabgKCER1xyHiEg2BUcJcY04RES6UHCUkNA6DhGRLhQcJSTiMV1yREQki4KjhERMFzkUEcmm4CghHtOIQ0Qkm4KjhEQc2jXHISLSQcFRgkYcIiJdKThKSMTRJUdERLIoOEqIx2JaACgikkXBUUJCN3ISEelCwVFCIh7TAkARkSwKjhI04hAR6UrBUYLmOEREulJwlBB8q0rBISKSoeAoIRHTHIeISLZklAc3synAIiABLHH3W3L21wDLgMnATmCmu28O990AzAXagPnuvsbMhgG/B2rCtq9093+Jsg8acYiIdBXZiMPMEsCdwEXAJOBSM5uUU2wu8Ja7nwzcAdwa1p0EzAJOBaYAi8PjtQDnuftpwOnAFDM7O6o+QGbluBYAiohkRHmq6kxgo7tvcvd3gOXAtJwy04Cl4eOVwPlmFgu3L3f3Fnd/GdgInOnuaXffF5YfEv6JdDigq+OKiHQV5amq8cDWrOdNwFmFyrh7q5ntBsaE25/JqTseOkYyjcDJwJ3uvrZUQ1paWkilUj3qRDrdTss7h3pcfzBobm6u6v5B9fex2vsH6uNAUlZwmNk1wI+AvcAS4Azgend/pEi1WJ5tub+6FypTsK67twGnm9ko4Odm9n53/3Ox9tfU1NDQ0FCsSEFDn36DWCLR4/qDQSqVqur+QfX3sdr7B+pjX2tsbCy4r9xTVV9w9z3AhcBY4PPALcWr0ARMyHpeB7xaqIyZJYGRwK5y6rr728DvCOZAIqM7AIqIdFVucGRGABcDP3L3P5J/VJBtHTDRzOrNbCjBZPeqnDKrgDnh4+nAY+6eDrfPMrMaM6sHJgLPmtnYcKSBmdUCfwP8pcw+9EhccxwiIl2UGxyNZvYIQXCsMbMRQNGvGrl7KzAPWAOkgBXuvsHMbjazS8Ji9wBjzGwjcB1wfVh3A7ACeAF4GLg6PEX1HuC3ZvYngmB61N0fKr+7ldM6DhGRrsqdHJ9L8PXXTe5+wMxGE5yuKsrdVwOrc7bdmPW4GZhRoO4CYEHOtj8RzK/0mUQcnaoSEclS7ojjrwB397fN7DLgn4Hd0TVr4EjEYloAKCKSpdzg+B5wwMxOA74GbCFY8V314uHK8bROV4mIAOUHR2s4aT0NWOTui4AR0TVr4EjEgu8AaNAhIhIod45jb3jtqNnAueEivCHRNWvgSITR2treTiKe6N/GiIgMAOWOOGYSXCfqC+7+GsEq7tsia9UAkhlxaJ5DRCRQVnCEYXE/MNLMpgLN7n5kzHGEq1X0zSoRkUBZwWFmnwWeJfjq7GeBtWY2PcqGDRSJMDnaFRwiIkD5cxz/BHzE3XcAmNlY4NcEV7StagmNOEREuih3jiOeCY3QzgrqDmrxuOY4RESylTvieNjM1gAPhM9nkrMivFppxCEi0lW5k+P/HbgL+CBwGnCXu/9jlA0bKOIxzXGIiGQr+0ZO7v4z4GcRtmVA6lzHoeAQEYESwWFme8l/a9YYkHb3oyNp1QDSuY5D9x0XEYESweHuR8RlRYrROg4Rka6OiG9GHY6EvlUlItKFgqOEzLeqFBwiIgEFRwmZEYdOVYmIBBQcJcQ14hAR6ULBUULmW1WtbQoOERFQcJSUWcfRrjsAiogACo6SOkYcOlUlIgIoOErqnOPQAkAREVBwlNTxrSrNcYiIAAqOkjLrODTHISISUHCUENc6DhGRLhQcJWjluIhIV2VfVr0nzGwKsAhIAEvc/Zac/TXAMmAywV0FZ7r75nDfDcBcoA2Y7+5rzGxCWP7dQDvBfUEWRdmHuNZxiIh0EdmIw8wSwJ3ARcAk4FIzm5RTbC7wlrufDNwB3BrWnQTMAk4FpgCLw+O1Al919wbgbODqPMfsVZl1HG2a4xARAaI9VXUmsNHdN7n7O8ByYFpOmWnA0vDxSuB8M4uF25e7e4u7vwxsBM509+3u/gcAd98LpIDxEfYh634cCg4REYj2VNV4YGvW8ybgrEJl3L3VzHYDY8Ltz+TU7RIQZnYScAawtlRDWlpaSKVSFTY/0HqoJWjAtu2kUvt7dIyBrrm5ucc/n8Gi2vtY7f0D9XEgiTI4Ynm25f7aXqhM0bpm9i6C29he6+57SjWkpqaGhoaGUsXy2v38nwEYO24cDQ31PTrGQJdKpXr88xksqr2P1d4/UB/7WmNjY8F9UZ6qagImZD2vA14tVMbMksBIYFexumY2hCA07nf3ByNpeZbOOY6oX0lEZHCIMjjWARPNrN7MhhJMdq/KKbMKmBM+ng485u7pcPssM6sxs3pgIvBsOP9xD5By99sjbHsH3XNcRKSryILD3VuBecAagknsFe6+wcxuNrNLwmL3AGPMbCNwHXB9WHcDsAJ4AXgYuNrd24BzgNnAeWa2PvxzcVR9gM4RhxYAiogEIl3H4e6rgdU5227MetwMzChQdwGwIGfbE+Sf/4hMZh1Hm85ViYgAWjleUsfVcbWOQ0QEUHCUFI/FiMe0jkNEJEPBUYZkPK45DhGRkIKjDPG4RhwiIhkKjjIk43EFh4hISMFRhkQ8puAQEQkpOMqQjMdo1QJAERFAwVGWuEYcIiIdFBxlSCo4REQ6KDjKkIjH9HVcEZGQgqMMGnGIiHRScJQhrhGHiEgHBUcZkvEY7QoOERFAwVGWhC45IiLSQcFRBs1xiIh0UnCUQXMcIiKdFBxlCEYcWjkuIgIKjrLoWlUiIp0UHGXQHIeISCcFRxm0clxEpJOCoww6VSUi0knBUQadqhIR6aTgKINGHCIinRQcZdAch4hIJwVHGRK657iISAcFRxk0xyEi0knBUQbNcYiIdEpGeXAzmwIsAhLAEne/JWd/DbAMmAzsBGa6++Zw3w3AXKANmO/ua8LtPwSmAjvc/f1Rtj8jGY/RqkuOiIgAEY44zCwB3AlcBEwCLjWzSTnF5gJvufvJwB3ArWHdScAs4FRgCrA4PB7AveG2PhPXiENEpEOUp6rOBDa6+yZ3fwdYDkzLKTMNWBo+Xgmcb2axcPtyd29x95eBjeHxcPffA7sibHc3muMQEekU5amq8cDWrOdNwFmFyrh7q5ntBsaE25/JqTu+pw1paWkhlUr1qG5zczO7397HO4faenyMga65ublq+5ZR7X2s9v6B+jiQRBkcsTzbcn9tL1SmnLplq6mpoaGhoUd1U6kU446tIf3S/h4fY6BLpVJV27eMau9jtfcP1Me+1tjYWHBflKeqmoAJWc/rgFcLlTGzJDCS4DRUOXX7jG7kJCLSKcrgWAdMNLN6MxtKMNm9KqfMKmBO+Hg68Ji7p8Pts8ysxszqgYnAsxG2tahkPEa7gkNEBIgwONy9FZgHrAFSwAp332BmN5vZJWGxe4AxZrYRuA64Pqy7AVgBvAA8DFzt7m0AZvYA8HTw0JrMbG5UfchIxOO0tqdJpxUeIiKRruNw99XA6pxtN2Y9bgZmFKi7AFiQZ/ulvdzMkpLxYMqlPQ2JfLMvIiJHEK0cL0MiDA4tAhQRUXCUJRMcyg0REQVHWZIacYiIdFBwlCEz4tDqcRERBUdZOuc4FBwiIgqOMnTOcSg4REQUHGVIasQhItJBwVGGRDz4MWmOQ0REwVGWRPhT0ohDRETBUZbOEYe+jisiouAoQ7Lj67j93BARkQFAwVEGXXJERKSTgqMMiZgWAIqIZCg4ypBI6Ou4IiIZCo4yJLUAUESkg4KjDLrkiIhIJwVHGTTHISLSScFRzP6d0N5KUnMcIiIdFBzF3HMBY//9+x0LADXHISKi4Cju2Ikc/cqvSYb3GdeIQ0REwVGcXcTQA69R+7YDuuSIiAgoOIo7ZQoAI1/5NaARh4gIKDiKG/FuDo6exLs2PwroW1UiIqDgKGnv+HMZtuN5xvK2gkNEBAVHSfuOPxeA8xLPs6+ltZ9bIyLS/xQcJbSMfB/pUScwbdh6bn/0P9j85v7+bpKISL9KRnlwM5sCLAISwBJ3vyVnfw2wDJgM7ARmuvvmcN8NwFygDZjv7mvKOWavi8WI2cWc3Xgvte0tfOHedTx41UcZNXxopC8rIjJQRRYcZpYA7gQuAJqAdWa2yt1fyCo2F3jL3U82s1nArcBMM5sEzAJOBY4Hfm1mp4R1Sh0zgs5cRHzt9/nV+Lv5X6/U8617XmHK5IkcO2IYxxw1jHi4QDAWg1gsHjwAYh1/dw7sMtvCZ3ke0VG/+45A9kxLLF+BnGMUKNFh3+432bVjW76DFDh08SMW2p9/c3Y7C/Wl6MsFwuuJdS0a6/i75eB+Dh44QDyZIB6LE4/Hice6vjcFGigiOaIccZwJbHT3TQBmthyYBmR/yE8DbgofrwS+a2axcPtyd28BXjazjeHxKOOYve/Ej8FfzWNUahU3Jh6HnffCI5G+Yp8a098N6AOnA6zqef32dKwjsNPEwj8Znc8z+7JlP888Tuds7yKWXb77MfId+91peCuWvT3rcGUeo9Lt5dTr+jhbeIwCYZ3v2Een02yL5T927nHLOV73MpUo43ixzve6XMPa02yJ9/QXmO71to89h7O/8oMeHq+wKINjPLA163kTcFahMu7eama7CT7HxgPP5NQdHz4udcxuWlpaSKVSFTU+o7m5mdR/vAgnXAYnXMaQ/a/CjhfYd6CZfS2tNB9qJZ0O/mmk01kfJx3/Wjr/2aRzPmroXqQM5dSr5IBp2traSSRyp7sKHKNXv1hWui/lvFws8/PPUzPzcd7e1koylibW3kbHO5HOekfSmfKFWpHuKNfl4yDdGRfZx+l8/TxtSuf2Kl3gMVl9yDl8zvb29jTxfB846TzHKHDsstqU53iF3qWu/8bz/Nvv9nMoId2eNZqtoG7Oa+cLkcI/h7wHLFmisuNlHTmd7jZij6XTBQM2X5Oy+7g3eWyPP/uKiTI48vU096dZqEyh7fkm80u+QzU1NTQ0NJQqllcqlcqp2wCc36NjDVTd+1h9qr2P1d4/UB/7WmNjY8F9UX6rqgmYkPW8Dni1UBkzSwIjgV1F6pZzTBERiVCUI451wEQzqwe2EUx2fy6nzCpgDvA0MB14zN3TZrYK+ImZ3U4wOT4ReJZgJFLqmCIiEqHIRhzu3grMA9YAKWCFu28ws5vN7JKw2D3AmHDy+zrg+rDuBmAFwaT3w8DV7t5W6JhR9UFERLqLdB2Hu68GVudsuzHrcTMwo0DdBcCCco4pIiJ9RyvHRUSkIgoOERGpiIJDREQqouAQEZGKxNKVrt4chBobG98AtvR3O0REBpETJ0+ePDbfjiMiOEREpPfoVJWIiFREwSEiIhVRcIiISEUUHCIiUhEFh4iIVETBISIiFYn0IoeDmZlNARYBCWCJu9/Sz006bGY2AVgGvBtoB+5y90VmNhr4KXASsBn4rLu/1V/t7A3hPe+fA7a5+9TwUvzLgdHAH4DZ7v5Of7bxcJjZKGAJ8H6Cm5l9AXCq6H00s/8GfJGgf/8OfB54D4P4fTSzHwJTgR3u/v5wW97/f+FttBcBFwMHgCvc/Q/90e5cGnHkEX7o3AlcBEwCLjWzSf3bql7RCnzV3RuAs4Grw35dD/zG3ScCvwmfD3bXEFx6P+NW4I6wj28Bc/ulVb1nEfCwu/8n4DSCvlbN+2hm44H5wIfDD9gEwf13Bvv7eC8wJWdbofftIoJ7EU0EvgR8r4/aWJKCI78zgY3uvin8bWY5MK2f23TY3H175jcWd99L8GEznqBvS8NiS4FP908Le4eZ1QF/S/AbOeFvbucBK8Mig7qPZnY08HGC+9ng7u+4+9tU2ftIcEakNrw76HBgO4P8fXT33xPc5TRbofdtGrDM3dPu/gwwysze0zctLU7Bkd94YGvW86ZwW9Uws+qu1usAAAPXSURBVJOAM4C1wHHuvh2CcAHG9WPTesN3gK8RnI4DGAO8Hd4IDAb/+/le4A3gR2b2vJktMbOjqKL30d23Af8KvEIQGLuBRqrrfcwo9L4N2M8hBUd+sTzbqubaLGb2LuBnwLXuvqe/29ObzCxz/rgxa3O1vZ9J4EPA99z9DGA/g/i0VD5mdgzBb9z1BLePPorg1E2uwfw+ljJg/90qOPJrAiZkPa8DXu2ntvQqMxtCEBr3u/uD4ebXM0Pg8O8d/dW+XnAOcImZbSY4xXgewQhkVHjKAwb/+9kENLn72vD5SoIgqab38W+Al939DXc/BDwIfJTqeh8zCr1vA/ZzSMGR3zpgopnVm9lQgkm5Vf3cpsMWnuu/B0i5++1Zu1YBc8LHc4Bf9HXbeou73+Dude5+EsH79pi7/z3wW2B6WGyw9/E1YKuZWbjpfOAFquh9JDhFdbaZDQ//3Wb6WDXvY5ZC79sq4HIzi5nZ2cDuzCmt/qar4xZgZhcT/KaaAH4Y3gN9UDOzjwH/j+CrjZnz//+DYJ5jBXACwX/YGe6eO4E36JjZJ4B/CL+O+146v8b5PHCZu7f0Z/sOh5mdTjD5PxTYRPBV1ThV9D6a2deBmQTfBnye4Ku54xnE76OZPQB8AjgWeB34F+D/kOd9CwPzuwTfwjoAfN7dn+uPdudScIiISEV0qkpERCqi4BARkYooOEREpCIKDhERqYiCQ0REKqLgEBnAzOwTZvZQf7dDJJuCQ0REKqJ1HCK9wMwuI7gM+FCCBZVXEVyY7wfAJwkuAT7L3d8IF+99n+CKry8BXwjvv3ByuH0s0AbMILjkxE3AmwT33mgkWPSm/7jSbzTiEDlMZtZAsML5HHc/neBD/+8JLsz3B3f/EPA4wSphCG6m9Y/u/kGCVfyZ7fcDd7r7aQTXZcpcXuIM4FqCe8O8l+B6XCL9RncAFDl85wOTgXXh5aNqCS5U105wZzeAHwMPmtlIYJS7Px5uXwr8bzMbAYx3958DuHszQHi8Z929KXy+nuBOcU9E3y2R/BQcIocvBix19xuyN5rZ/8wpV+z0Ur5LaGdkX4upDf2/lX6mU1Uih+83wHQzGwfBPaTN7ESC/1+ZK7l+DnjC3XcDb5nZueH22cDj4X1Rmszs0+ExasxseJ/2QqRM+s1F5DC5+wtm9s/AI2YWBw4BVxPcYOlUM2skmCifGVaZA3w/DIbMlW0hCJEfmNnN4TFm9GE3RMqmb1WJRMTM9rn7u/q7HSK9TaeqRESkIhpxiIhIRTTiEBGRiig4RESkIgoOERGpiIJDREQqouAQEZGK/H/1lEIk4JlcWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_ae_sigmoid_adam_mse  = plot_hist_auto(hist_ae_sigmoid_adam_mse, './Figures/hist_ae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- SPAE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 140)               29540     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 210)               29610     \n",
      "=================================================================\n",
      "Total params: 59,150\n",
      "Trainable params: 59,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "spae_sigmoid_adam_mse,enc_train_x_spsam,enc_test_x_spsam = spae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spae_sigmoid_adam_mse = load_model('spae_sigmoid_adam_mse_ds20bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 24 19:56:50 2019\n",
      "Train on 1330452 samples, validate on 332613 samples\n",
      "Epoch 1/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0888 - acc: 3.1643e-04 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03467, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 2/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0278 - acc: 0.0000e+00 - val_loss: 0.0245 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03467 to 0.02447, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 3/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0236 - acc: 0.0000e+00 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02447 to 0.02314, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 4/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0230 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02314 to 0.02294, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 5/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02294 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 6/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02292 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 7/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02292 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 8/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02292 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 9/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02292 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 10/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02292 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 11/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02292 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 12/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02292 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 13/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02292\n",
      "Epoch 14/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02292\n",
      "Epoch 15/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02292 to 0.02292, saving model to ./H5files/spae_sigmoid_adam_mse_ds100bal_minmax.h5\n",
      "Epoch 16/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02292\n",
      "Epoch 17/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02292\n",
      "Epoch 18/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02292\n",
      "Epoch 19/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02292\n",
      "Epoch 20/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02292\n",
      "Time elapsed (hh:mm:ss.ms) 0:04:59.711131\n"
     ]
    }
   ],
   "source": [
    "hist_spae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/spae_sigmoid_adam_mse_ds\"+str(dsnum)+\"bal_minmax.h5\",\n",
    "                                  autoencoder = spae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size*2,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.022905419232296498\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_spae_sigmoid_adam_mse  = plot_hist_auto(hist_spae_sigmoid_adam_mse, './Figures/hist_spae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valueDict = {\n",
    "    'loss_value_ae_sigmoid_adam_mse': best_loss_value_ae_sigmoid_adam_mse,\n",
    "    'loss_value_spae_sigmoid_adam_mse': best_loss_value_spae_sigmoid_adam_mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_value_ae_sigmoid_adam_mse': 5.2634154415754054e-06,\n",
       " 'loss_value_spae_sigmoid_adam_mse': 0.022905419232296498}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330452, 140)\n",
      "(415767, 140)\n",
      "(1330452, 140)\n",
      "(415767, 140)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train_x_asam.shape)\n",
    "print(enc_test_x_asam.shape)\n",
    "\n",
    "print(enc_train_x_spsam.shape)\n",
    "print(enc_test_x_spsam.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 20:01:52.006308 140578914449216 deprecation.py:323] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_asam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 24 20:01:52 2019\n",
      "Train on 1064361 samples, validate on 266091 samples\n",
      "Epoch 1/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.2472 - acc: 0.8768 - val_loss: 0.1672 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16721, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 2/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1491 - acc: 0.9225 - val_loss: 0.1368 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16721 to 0.13679, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 3/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1353 - acc: 0.9281 - val_loss: 0.1399 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13679\n",
      "Epoch 4/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1287 - acc: 0.9314 - val_loss: 0.1294 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13679 to 0.12945, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 5/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1232 - acc: 0.9340 - val_loss: 0.1399 - val_acc: 0.9223\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12945\n",
      "Epoch 6/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1192 - acc: 0.9362 - val_loss: 0.1158 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12945 to 0.11577, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 7/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1160 - acc: 0.9375 - val_loss: 0.1147 - val_acc: 0.9370\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.11577 to 0.11473, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 8/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.1127 - acc: 0.9394 - val_loss: 0.1087 - val_acc: 0.9428\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11473 to 0.10868, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 9/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1104 - acc: 0.9407 - val_loss: 0.1069 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10868 to 0.10693, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 10/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.1083 - acc: 0.9418 - val_loss: 0.1064 - val_acc: 0.9443\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.10693 to 0.10642, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 11/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1052 - acc: 0.9435 - val_loss: 0.1022 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.10642 to 0.10217, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 12/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1035 - acc: 0.9442 - val_loss: 0.1040 - val_acc: 0.9464\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.10217\n",
      "Epoch 13/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.1015 - acc: 0.9456 - val_loss: 0.0955 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.10217 to 0.09553, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 14/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0993 - acc: 0.9467 - val_loss: 0.1003 - val_acc: 0.9465\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09553\n",
      "Epoch 15/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0972 - acc: 0.9479 - val_loss: 0.0978 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09553\n",
      "Epoch 16/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0961 - acc: 0.9487 - val_loss: 0.0915 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09553 to 0.09150, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 17/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0935 - acc: 0.9498 - val_loss: 0.0944 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09150\n",
      "Epoch 18/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0928 - acc: 0.9504 - val_loss: 0.0900 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09150 to 0.09001, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 19/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0911 - acc: 0.9514 - val_loss: 0.0906 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09001\n",
      "Epoch 20/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0902 - acc: 0.9518 - val_loss: 0.0882 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09001 to 0.08816, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 21/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0895 - acc: 0.9520 - val_loss: 0.0853 - val_acc: 0.9560\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08816 to 0.08535, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 22/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0879 - acc: 0.9530 - val_loss: 0.0934 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.08535\n",
      "Epoch 23/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0875 - acc: 0.9532 - val_loss: 0.0928 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.08535\n",
      "Epoch 24/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0865 - acc: 0.9536 - val_loss: 0.0824 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.08535 to 0.08241, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 25/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0862 - acc: 0.9538 - val_loss: 0.0862 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08241\n",
      "Epoch 26/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0848 - acc: 0.9548 - val_loss: 0.0876 - val_acc: 0.9533\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.08241\n",
      "Epoch 27/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0845 - acc: 0.9548 - val_loss: 0.0826 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.08241\n",
      "Epoch 28/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0838 - acc: 0.9552 - val_loss: 0.0822 - val_acc: 0.9571\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.08241 to 0.08221, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 29/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0837 - acc: 0.9553 - val_loss: 0.0811 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08221 to 0.08112, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 30/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0830 - acc: 0.9558 - val_loss: 0.0808 - val_acc: 0.9568\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.08112 to 0.08083, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 31/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0822 - acc: 0.9562 - val_loss: 0.0815 - val_acc: 0.9564\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.08083\n",
      "Epoch 32/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0813 - acc: 0.9566 - val_loss: 0.0808 - val_acc: 0.9581\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.08083 to 0.08078, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 33/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0811 - acc: 0.9567 - val_loss: 0.0827 - val_acc: 0.9575\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.08078\n",
      "Epoch 34/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0802 - acc: 0.9572 - val_loss: 0.0947 - val_acc: 0.9494\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.08078\n",
      "Epoch 35/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0800 - acc: 0.9576 - val_loss: 0.0830 - val_acc: 0.9562\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.08078\n",
      "Epoch 36/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0797 - acc: 0.9578 - val_loss: 0.0834 - val_acc: 0.9561\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.08078\n",
      "Epoch 37/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0792 - acc: 0.9578 - val_loss: 0.0755 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.08078 to 0.07548, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_ds100bal_minmax.h5\n",
      "Epoch 38/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0789 - acc: 0.9581 - val_loss: 0.0785 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07548\n",
      "Epoch 39/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0774 - acc: 0.9590 - val_loss: 0.0805 - val_acc: 0.9591\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07548\n",
      "Epoch 40/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0773 - acc: 0.9589 - val_loss: 0.0799 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07548\n",
      "Epoch 41/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0775 - acc: 0.9590 - val_loss: 0.0773 - val_acc: 0.9592\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07548\n",
      "Epoch 42/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0763 - acc: 0.9595 - val_loss: 0.0800 - val_acc: 0.9586\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07548\n",
      "Time elapsed (hh:mm:ss.ms) 0:13:32.610283\n"
     ]
    }
   ],
   "source": [
    "hist_ae_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ae_ann_2h_unisoftsigbinlosadam_ds\"+str(dsnum)+\"bal_minmax.h5\",\n",
    "                                        ann = ae_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_asam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_ae_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_ae_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_ae_ann_2h_unisoftsigbinlosadam, './Figures/ae_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_ae_ann_2h_prob_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict(ae_ann_2h_unisoftsigbinlosadam,enc_test_x_asam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_asam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "njobscpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 24 20:33:33 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.4010 - acc: 0.8011\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.2115 - acc: 0.8988\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1780 - acc: 0.9121\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1642 - acc: 0.9171\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1559 - acc: 0.9209\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1503 - acc: 0.9228\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1454 - acc: 0.9240\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1420 - acc: 0.9256\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1396 - acc: 0.9266\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1351 - acc: 0.9284\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1331 - acc: 0.9296\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1308 - acc: 0.9302\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1290 - acc: 0.9313\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1264 - acc: 0.9326\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1263 - acc: 0.9325\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1243 - acc: 0.9336\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1228 - acc: 0.9339\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1217 - acc: 0.9345\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1201 - acc: 0.9354\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1192 - acc: 0.9358\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1186 - acc: 0.9362\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1172 - acc: 0.9371\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1166 - acc: 0.9373\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1156 - acc: 0.9375\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.1146 - acc: 0.9377\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.1140 - acc: 0.9383\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1135 - acc: 0.9383\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1134 - acc: 0.9386\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1114 - acc: 0.9394\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1113 - acc: 0.9395\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1117 - acc: 0.9397\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1104 - acc: 0.9400\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1095 - acc: 0.9405\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1093 - acc: 0.9408\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1091 - acc: 0.9411\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1085 - acc: 0.9409\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1077 - acc: 0.9418\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1076 - acc: 0.9422\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1067 - acc: 0.9420\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1071 - acc: 0.9421\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1065 - acc: 0.9425\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1071 - acc: 0.9422\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1060 - acc: 0.9429\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1057 - acc: 0.9430\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1062 - acc: 0.9425\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1047 - acc: 0.9436\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1047 - acc: 0.9436\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1048 - acc: 0.9436\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1043 - acc: 0.9437\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1041 - acc: 0.9439\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1038 - acc: 0.9441\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1035 - acc: 0.9448\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1040 - acc: 0.9443\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1030 - acc: 0.9449\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1031 - acc: 0.9443\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1025 - acc: 0.9448\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1033 - acc: 0.9448\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.1020 - acc: 0.9454\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.1017 - acc: 0.9454\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.1026 - acc: 0.9453\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.1020 - acc: 0.9455\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.1019 - acc: 0.9457\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.1012 - acc: 0.9460\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1010 - acc: 0.9460\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.1015 - acc: 0.9458\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1018 - acc: 0.9458\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.1008 - acc: 0.9464\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1007 - acc: 0.9466\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0999 - acc: 0.9464\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1003 - acc: 0.9462\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1003 - acc: 0.9465\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0996 - acc: 0.9466\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0998 - acc: 0.9465\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0992 - acc: 0.9471\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0986 - acc: 0.9474\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0995 - acc: 0.9467\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0981 - acc: 0.9475\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0989 - acc: 0.9471\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0984 - acc: 0.9475\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0984 - acc: 0.9477\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0990 - acc: 0.9471\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0981 - acc: 0.9474\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0976 - acc: 0.9478\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0975 - acc: 0.9479\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0975 - acc: 0.9477\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0974 - acc: 0.9479\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0981 - acc: 0.9476\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0968 - acc: 0.9481\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0968 - acc: 0.9483\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0970 - acc: 0.9484\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.0970 - acc: 0.9480\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0962 - acc: 0.9487\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0964 - acc: 0.9483\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0969 - acc: 0.9481\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0961 - acc: 0.9485\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0963 - acc: 0.9488\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0961 - acc: 0.9487\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0959 - acc: 0.9486\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0969 - acc: 0.9486\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0952 - acc: 0.9493\n",
      "83154/83154 [==============================] - 1s 7us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.4037 - acc: 0.7997\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.2141 - acc: 0.8977\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1767 - acc: 0.9114\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1651 - acc: 0.9158\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1546 - acc: 0.9195\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1506 - acc: 0.9211\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1454 - acc: 0.9228\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1412 - acc: 0.9247\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1391 - acc: 0.9257\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1356 - acc: 0.9275\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1336 - acc: 0.9279\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1314 - acc: 0.9290\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1299 - acc: 0.9300\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1287 - acc: 0.9304\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.1254 - acc: 0.9321\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1246 - acc: 0.9319\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1235 - acc: 0.9326\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1235 - acc: 0.9331\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1208 - acc: 0.9342\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1204 - acc: 0.9345\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1191 - acc: 0.9354\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1182 - acc: 0.9358\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1167 - acc: 0.9366\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1166 - acc: 0.9359\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1153 - acc: 0.9373\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1152 - acc: 0.9368\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1140 - acc: 0.9378\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1136 - acc: 0.9379\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1132 - acc: 0.9378\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1116 - acc: 0.9388\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1113 - acc: 0.9386\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1116 - acc: 0.9391\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1101 - acc: 0.9395\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1100 - acc: 0.9398\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1092 - acc: 0.9402\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1085 - acc: 0.9405\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1096 - acc: 0.9396\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1080 - acc: 0.9408\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1078 - acc: 0.9405\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1073 - acc: 0.9410\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1073 - acc: 0.9414\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1068 - acc: 0.9412\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1065 - acc: 0.9413\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1061 - acc: 0.9421\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1048 - acc: 0.9422\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1058 - acc: 0.9420\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1043 - acc: 0.9426\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1042 - acc: 0.9426\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.1051 - acc: 0.9421\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1044 - acc: 0.9426\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1046 - acc: 0.9424\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1040 - acc: 0.9429\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1039 - acc: 0.9429\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1032 - acc: 0.9428\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1031 - acc: 0.9431\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1028 - acc: 0.9432\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1038 - acc: 0.9428\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1020 - acc: 0.9441\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1026 - acc: 0.9437\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1026 - acc: 0.9436\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1021 - acc: 0.9440\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1012 - acc: 0.9443\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1022 - acc: 0.9435\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1013 - acc: 0.9444\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1016 - acc: 0.9439\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.1021 - acc: 0.9438\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1011 - acc: 0.9442\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1009 - acc: 0.9444\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1007 - acc: 0.9448\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1007 - acc: 0.9441\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1005 - acc: 0.9446\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1009 - acc: 0.9444\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.0994 - acc: 0.9451\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1005 - acc: 0.9450\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1000 - acc: 0.9453\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0994 - acc: 0.9457\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0994 - acc: 0.9457\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0996 - acc: 0.9455\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0979 - acc: 0.9462\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0991 - acc: 0.9451\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0985 - acc: 0.9461\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0983 - acc: 0.9464\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0981 - acc: 0.9464\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0975 - acc: 0.9468\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0976 - acc: 0.9467\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0971 - acc: 0.9469\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0972 - acc: 0.9468\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0973 - acc: 0.9469\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0972 - acc: 0.9472\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0966 - acc: 0.9475\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0964 - acc: 0.9476\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0965 - acc: 0.9479\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0955 - acc: 0.9478\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0971 - acc: 0.9476\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0953 - acc: 0.9483\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0951 - acc: 0.9488\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0943 - acc: 0.9490\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0953 - acc: 0.9486\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0946 - acc: 0.9490\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0946 - acc: 0.9490\n",
      "83154/83154 [==============================] - 1s 7us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.3935 - acc: 0.8036\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.2063 - acc: 0.8992\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.1744 - acc: 0.9116\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1600 - acc: 0.9171\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1514 - acc: 0.9201\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1462 - acc: 0.9228\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1406 - acc: 0.9255\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1362 - acc: 0.9271\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1333 - acc: 0.9287\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1310 - acc: 0.9300\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1276 - acc: 0.9314\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1255 - acc: 0.9327\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1231 - acc: 0.9336\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1220 - acc: 0.9344\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1187 - acc: 0.9359\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1175 - acc: 0.9368\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1168 - acc: 0.9374\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1151 - acc: 0.9384\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1142 - acc: 0.9388\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1124 - acc: 0.9400\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1107 - acc: 0.9408\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.1103 - acc: 0.9412\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1085 - acc: 0.9424\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1083 - acc: 0.9427\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1063 - acc: 0.9436\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1060 - acc: 0.9438\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1046 - acc: 0.9449\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1034 - acc: 0.9452\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1026 - acc: 0.9461\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1010 - acc: 0.9465\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1012 - acc: 0.9463\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1003 - acc: 0.9472\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0981 - acc: 0.9481\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0973 - acc: 0.9487\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0974 - acc: 0.9489\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0958 - acc: 0.9499\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0954 - acc: 0.9502\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0945 - acc: 0.9508\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0943 - acc: 0.9507\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0924 - acc: 0.9516\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0931 - acc: 0.9510\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0918 - acc: 0.9524\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0917 - acc: 0.9520\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0900 - acc: 0.9533\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0902 - acc: 0.9529\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0902 - acc: 0.9528\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0896 - acc: 0.9535\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0888 - acc: 0.9540\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0885 - acc: 0.9544\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0885 - acc: 0.9545\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0883 - acc: 0.9544\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0870 - acc: 0.9552\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0871 - acc: 0.9551\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0869 - acc: 0.9555\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0861 - acc: 0.9557\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0854 - acc: 0.9564\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0852 - acc: 0.9564\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0842 - acc: 0.9566\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0844 - acc: 0.9567\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0851 - acc: 0.9563\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0837 - acc: 0.9573\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0831 - acc: 0.9574\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0839 - acc: 0.9576\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0833 - acc: 0.9576\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.0827 - acc: 0.9575\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0818 - acc: 0.9586\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0828 - acc: 0.9581\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0815 - acc: 0.9587\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0815 - acc: 0.9587\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0813 - acc: 0.9587\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0800 - acc: 0.9592\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0811 - acc: 0.9586\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0803 - acc: 0.9593\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0804 - acc: 0.9596\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0808 - acc: 0.9592\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0800 - acc: 0.9594\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0793 - acc: 0.9595\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0792 - acc: 0.9597\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0793 - acc: 0.9601\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.0784 - acc: 0.9604\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0784 - acc: 0.9600\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0782 - acc: 0.9604\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0781 - acc: 0.9605\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0774 - acc: 0.9609\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0779 - acc: 0.9606\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0776 - acc: 0.9612\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0772 - acc: 0.9616\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0766 - acc: 0.9613\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0770 - acc: 0.9612\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.0770 - acc: 0.9616\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0759 - acc: 0.9618\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0762 - acc: 0.9619\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0768 - acc: 0.9616\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0754 - acc: 0.9622\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0757 - acc: 0.9621\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0758 - acc: 0.9616\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0757 - acc: 0.9624\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0747 - acc: 0.9627\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0750 - acc: 0.9625\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0744 - acc: 0.9626\n",
      "83153/83153 [==============================] - 1s 8us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.3950 - acc: 0.8026\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.2083 - acc: 0.8983\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1756 - acc: 0.9112\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.1617 - acc: 0.9167\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1548 - acc: 0.9199\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1488 - acc: 0.9221\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1450 - acc: 0.9243\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1421 - acc: 0.9256\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1379 - acc: 0.9274\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1363 - acc: 0.9281\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1320 - acc: 0.9296\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1314 - acc: 0.9303\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1295 - acc: 0.9309\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1274 - acc: 0.9318\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1268 - acc: 0.9318\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1246 - acc: 0.9332\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1226 - acc: 0.9340\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1226 - acc: 0.9340\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1209 - acc: 0.9345\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1195 - acc: 0.9355\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1189 - acc: 0.9355\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1176 - acc: 0.9360\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1164 - acc: 0.9365\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1163 - acc: 0.9369\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1149 - acc: 0.9374\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1144 - acc: 0.9379\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1135 - acc: 0.9379\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1130 - acc: 0.9386\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1122 - acc: 0.9391\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.1116 - acc: 0.9390\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1109 - acc: 0.9395\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1107 - acc: 0.9395\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1098 - acc: 0.9396\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1085 - acc: 0.9406\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1078 - acc: 0.9410\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1077 - acc: 0.9407\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1075 - acc: 0.9408\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1076 - acc: 0.9413\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1056 - acc: 0.9420\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1057 - acc: 0.9416\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1053 - acc: 0.9417\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1056 - acc: 0.9421\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1036 - acc: 0.9432\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.1046 - acc: 0.9422\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1039 - acc: 0.9430\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1038 - acc: 0.9428\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1032 - acc: 0.9434\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.1037 - acc: 0.9429\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1030 - acc: 0.9433\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1020 - acc: 0.9438\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1029 - acc: 0.9434\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1021 - acc: 0.9440\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1014 - acc: 0.9445\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.1015 - acc: 0.9441\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1010 - acc: 0.9445\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1007 - acc: 0.9448\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1004 - acc: 0.9447\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1005 - acc: 0.9453\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0994 - acc: 0.9456\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1007 - acc: 0.9452\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0997 - acc: 0.9456\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.0996 - acc: 0.9456\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0994 - acc: 0.9457\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0984 - acc: 0.9462\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0985 - acc: 0.9461\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0980 - acc: 0.9467\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0976 - acc: 0.9470\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0983 - acc: 0.9464\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0972 - acc: 0.9470\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0979 - acc: 0.9468\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0967 - acc: 0.9474\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0969 - acc: 0.9474\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0966 - acc: 0.9475\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0966 - acc: 0.9473\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0979 - acc: 0.9462\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0965 - acc: 0.9475\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0961 - acc: 0.9479\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0959 - acc: 0.9474\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0953 - acc: 0.9483\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0950 - acc: 0.9482\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0959 - acc: 0.9478\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0940 - acc: 0.9487\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0947 - acc: 0.9486\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0943 - acc: 0.9485\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0946 - acc: 0.9484\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0941 - acc: 0.9490\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0944 - acc: 0.9494\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0932 - acc: 0.9492\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0933 - acc: 0.9492\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0943 - acc: 0.9486\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0936 - acc: 0.9496\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0926 - acc: 0.9495\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0931 - acc: 0.9493\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0930 - acc: 0.9495\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0936 - acc: 0.9495\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0917 - acc: 0.9502\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0930 - acc: 0.9498\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0921 - acc: 0.9501\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0921 - acc: 0.9506\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0911 - acc: 0.9505\n",
      "83153/83153 [==============================] - 1s 9us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.3959 - acc: 0.8036\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.2080 - acc: 0.9004\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1740 - acc: 0.9130\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1595 - acc: 0.9179\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1502 - acc: 0.9218\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1446 - acc: 0.9243\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1396 - acc: 0.9259\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1355 - acc: 0.9278\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1323 - acc: 0.9291\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1287 - acc: 0.9311\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1275 - acc: 0.9317\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1246 - acc: 0.9332\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.1226 - acc: 0.9344\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1217 - acc: 0.9348\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1194 - acc: 0.9363\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1183 - acc: 0.9368\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1155 - acc: 0.9385\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1139 - acc: 0.9389\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1134 - acc: 0.9396\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1124 - acc: 0.9402\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1106 - acc: 0.9412\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1105 - acc: 0.9415\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1082 - acc: 0.9426\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1078 - acc: 0.9426\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1061 - acc: 0.9439\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1058 - acc: 0.9439\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1040 - acc: 0.9449\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1033 - acc: 0.9452\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1027 - acc: 0.9458\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1015 - acc: 0.9460\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1014 - acc: 0.9466\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0996 - acc: 0.9470\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0988 - acc: 0.9478\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0979 - acc: 0.9485\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0972 - acc: 0.9488\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0959 - acc: 0.9494\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0952 - acc: 0.9504\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0945 - acc: 0.9506\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0946 - acc: 0.9501\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0938 - acc: 0.9510\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0935 - acc: 0.9511\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0924 - acc: 0.9515\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0907 - acc: 0.9524\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0912 - acc: 0.9525\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0905 - acc: 0.9527\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0898 - acc: 0.9528\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0892 - acc: 0.9534\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0892 - acc: 0.9534\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0883 - acc: 0.9540\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0877 - acc: 0.9542\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0876 - acc: 0.9545\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0873 - acc: 0.9544\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0866 - acc: 0.9548\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0860 - acc: 0.9552\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0862 - acc: 0.9553\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0848 - acc: 0.9558\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0847 - acc: 0.9562\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0846 - acc: 0.9564\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0843 - acc: 0.9564\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0840 - acc: 0.9568\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0840 - acc: 0.9563\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0833 - acc: 0.9568\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0831 - acc: 0.9568\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0822 - acc: 0.9575\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0823 - acc: 0.9576\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0820 - acc: 0.9575\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0820 - acc: 0.9580\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0815 - acc: 0.9581\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0811 - acc: 0.9582\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0819 - acc: 0.9582\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0809 - acc: 0.9584\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0805 - acc: 0.9588\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0803 - acc: 0.9587\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0803 - acc: 0.9588\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0797 - acc: 0.9595\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0793 - acc: 0.9592\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0791 - acc: 0.9595\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0793 - acc: 0.9596\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0789 - acc: 0.9595\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0785 - acc: 0.9597\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0796 - acc: 0.9592\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0787 - acc: 0.9599\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0789 - acc: 0.9600\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0776 - acc: 0.9603\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0782 - acc: 0.9603\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0785 - acc: 0.9600\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0775 - acc: 0.9607\n",
      "Epoch 88/100\n",
      "281190/332614 [========================>.....] - ETA: 1s - loss: 0.0775 - acc: 0.9606"
     ]
    }
   ],
   "source": [
    "pred_ae_ann_2h_prob_unisoftsigbinlosadam,pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_spsam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_sp_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_ds\"+str(dsnum)+\"bal_minmax.h5\",\n",
    "                                        ann = sp_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_spsam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_sp_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_sp_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_sp_ann_2h_unisoftsigbinlosadam, './Figures/sp_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict(sp_ann_2h_unisoftsigbinlosadam,enc_test_x_spsam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_spsam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sp_ann_2h_prob_unisoftsigbinlosadam,pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with no encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodr_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=train_x,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_nodr_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_ds\"+str(dsnum)+\"bal_minmax.h5\",\n",
    "                                        ann = nodr_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = train_x,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_nodr_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_nodr_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_nodr_ann_2h_unisoftsigbinlosadam, './Figures/nodr_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict(nodr_ann_2h_unisoftsigbinlosadam,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=train_x\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=test_x\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the PCA algorithm with our Data\n",
    "# pca = PCA().fit(data_rescaled)\n",
    "pca_ = PCA(n_components = 0.95, svd_solver = 'full').fit(train_x)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "n_coml = [pca_.n_components_]\n",
    "\n",
    "plt.plot(np.cumsum(pca_.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Variance (%)', fontsize=14) #for each component\n",
    "plt.title('Pulsar Dataset Explained Variance '+str(dsnum)+' node DS', fontsize=14)\n",
    "\n",
    "n_coml = [*n_coml]\n",
    "\n",
    "for i, v in enumerate(n_coml):\n",
    "    plt.text(v-0.8, i+0.94, '{:.0f}'.format(v), color='navy', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/PCA_components_ds'+str(dsnum)+'bal_minmax.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300, \n",
    "                             criterion='gini', \n",
    "                             max_depth=16, \n",
    "#                              min_samples_split=2, \n",
    "                             #min_samples_leaf=1, \n",
    "                             max_features=0.3, \n",
    "                             #bootstrap=True,\n",
    "                             oob_score=True,\n",
    "                             random_state=23)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(enc_train_x_asam, train_y)\n",
    "\n",
    "pred_y_ae_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(enc_test_x_asam),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=njobscpu)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_ae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_y_ae_RF, pred_y_ae_RF, './Figures/ROC_ae_rf_E100MaxfautoMaxdnoneBootT_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_ae_rf_E100MaxfautoMaxdnoneBootT_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(enc_train_x_spsam, train_y)\n",
    "\n",
    "pred_y_spae_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(enc_test_x_spsam),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=njobscpu)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_spae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_y_spae_RF, pred_y_spae_RF, './Figures/ROC_spae_rf_E100MaxfautoMaxdnoneBootT_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_spae_rf_E100MaxfautoMaxdnoneBootT_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with pca DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(train_x_pca, train_y)\n",
    "\n",
    "pred_y_pca_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(test_x_pca),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=njobscpu)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_pca_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_y_pca_RF, pred_y_pca_RF, './Figures/ROC_pca_rf_E100MaxfautoMaxdnoneBootT_ds'+str(dsnum)+'bal_minmax.png', './Figures/CM_pca_rf_E100MaxfautoMaxdnoneBootT_ds'+str(dsnum)+'bal_minmax.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_ae_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_ae_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_y_ae_RF.shape)\n",
    "print(pred_y_spae_RF.shape)\n",
    "print(pred_y_pca_RF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate_ae_ann, recall_ae_ann, thresholds_ae_ann = roc_curve(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_ae_ann = auc(false_positive_rate_ae_ann, recall_ae_ann)\n",
    "false_positive_rate_sp_ann, recall_sp_ann, thresholds_sp_ann = roc_curve(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_sp_ann = auc(false_positive_rate_sp_ann, recall_sp_ann)\n",
    "false_positive_rate_nodr_ann, recall_nodr_ann, thresholds_nodr_ann = roc_curve(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_nodr_ann = auc(false_positive_rate_nodr_ann, recall_nodr_ann)\n",
    "\n",
    "false_positive_rate_ae_RF, recall_ae_RF, thresholds_ae_RF = roc_curve(test_y, pred_y_ae_RF)\n",
    "roc_auc_ae_RF = auc(false_positive_rate_ae_RF, recall_ae_RF)\n",
    "false_positive_rate_spae_RF, recall_spae_RF, thresholds_spae_RF = roc_curve(test_y, pred_y_spae_RF)\n",
    "roc_auc_spae_RF = auc(false_positive_rate_spae_RF, recall_spae_RF)\n",
    "false_positive_rate_pca_RF, recall_pca_RF, thresholds_pca_RF = roc_curve(test_y, pred_y_pca_RF)\n",
    "roc_auc_pca_RF = auc(false_positive_rate_pca_RF, recall_pca_RF)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC)', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "# plt.ylim([0.97,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal_minmax.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC) Zoom in', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "# plt.ylim([0.0,1.0])\n",
    "plt.ylim([0.955,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal_zoom.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "# labels = ['Normal', 'Malicious']\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=sns.light_palette(\"purple\"), vmin = 0.2);\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('True Class')\n",
    "# plt.xlabel('Predicted Class')\n",
    "# plt.savefig('./Figures/CM_ae_ann_thirdds'+str(dsnum)+'bal_TEST.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ae_ann = \"AE+DNN\"\n",
    "acc_ae_ann = (sm.accuracy_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_ae_ann = (sm.precision_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_ae_ann = (sm.recall_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_ae_ann = (sm.f1_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_sp_ann = \"SAE+DNN\"\n",
    "acc_sp_ann = (sm.accuracy_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_sp_ann = (sm.precision_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_sp_ann = (sm.recall_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_sp_ann = (sm.f1_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_nodr_ann = \"DNN\"\n",
    "acc_nodr_ann = (sm.accuracy_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_nodr_ann = (sm.precision_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_nodr_ann = (sm.recall_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_nodr_ann = (sm.f1_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_ae_RF = \"AE+RF\"\n",
    "acc_ae_RF = (sm.accuracy_score(test_y, pred_y_ae_RF)*100) \n",
    "pre_ae_RF = (sm.precision_score(test_y, pred_y_ae_RF)*100) \n",
    "recall_ae_RF = (sm.recall_score(test_y, pred_y_ae_RF)*100) \n",
    "f1score_ae_RF = (sm.f1_score(test_y, pred_y_ae_RF)*100)\n",
    "\n",
    "classi_spae_RF = \"SAE+RF\"\n",
    "acc_spae_RF = (sm.accuracy_score(test_y, pred_y_spae_RF)*100) \n",
    "pre_spae_RF = (sm.precision_score(test_y, pred_y_spae_RF)*100) \n",
    "recall_spae_RF = (sm.recall_score(test_y, pred_y_spae_RF)*100) \n",
    "f1score_spae_RF = (sm.f1_score(test_y, pred_y_spae_RF)*100)\n",
    "\n",
    "classi_pca_RF = \"PCA+RF\"\n",
    "acc_pca_RF = (sm.accuracy_score(test_y, pred_y_pca_RF)*100) \n",
    "pre_pca_RF = (sm.precision_score(test_y, pred_y_pca_RF)*100) \n",
    "recall_pca_RF = (sm.recall_score(test_y, pred_y_pca_RF)*100) \n",
    "f1score_pca_RF = (sm.f1_score(test_y, pred_y_pca_RF)*100)\n",
    "\n",
    "\n",
    "print('Classifier\\tAcc\\tPreci\\tRecall\\tF1Score')\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_ann, acc_ae_ann, pre_ae_ann, recall_ae_ann, f1score_ae_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_sp_ann, acc_sp_ann, pre_sp_ann, recall_sp_ann, f1score_sp_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_nodr_ann, acc_nodr_ann, pre_nodr_ann, recall_nodr_ann, f1score_nodr_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_RF, acc_ae_RF, pre_ae_RF, recall_ae_RF, f1score_ae_RF))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_spae_RF, acc_spae_RF, pre_spae_RF, recall_spae_RF, f1score_spae_RF))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_pca_RF, acc_pca_RF, pre_pca_RF, recall_pca_RF, f1score_pca_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1list = [[\"AE+DNN\",f1score_ae_ann],[\"SAE+DNN\",f1score_sp_ann],[\"DNN\",f1score_nodr_ann],\n",
    "          [\"AE+RF\",f1score_ae_RF],[\"SAE+RF\",f1score_spae_RF],[\"PCA+RF\",f1score_pca_RF]]\n",
    "\n",
    "xs, ys = [*zip(*f1list)]\n",
    "\n",
    "'{:.2f}'.format(f1score_ae_ann)\n",
    "\n",
    "plt.figure(figsize=(8,6), )\n",
    "plt.barh(xs, ys, color = \"purple\")\n",
    "plt.title(\"F1 score vs Classifier\", fontsize=16)\n",
    "plt.xlabel(\"Classifier\", fontsize=14)\n",
    "plt.ylabel(\"F1 score\", fontsize=14)\n",
    "plt.xticks(np.arange(0, 101, 10), fontsize=12)\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "for i, v in enumerate(ys):\n",
    "    plt.text(v+1, i+0.1, '{:.2f}'.format(v), color='purple', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/F1scoreplot_allmodels'+str(dsnum)+'bal_minmax.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
