{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Donâ€™t pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "from keras import optimizers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Activation, Dense, Dropout, Input\n",
    "from keras import backend as k\n",
    "\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "#k.tensorflow_backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------Import modules------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(23)\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from datetime import datetime \n",
    "import os.path\n",
    "\n",
    "dsnum=20\n",
    "verbose_level=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/01Code/00Datasets_final/00BalancedDS/FullCloneID20bal_stdscal.csv\n"
     ]
    }
   ],
   "source": [
    "pathds = os.path.abspath('/home/user/01Code/00Datasets_final/00BalancedDS')\n",
    "file_name = \"FullCloneID\"+str(dsnum)+\"bal_stdscal.csv\"\n",
    "full_path = os.path.join(pathds,file_name)\n",
    "print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2686202, 67)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "neurons=df.shape[1]-1\n",
    "batch_size=df.shape[1]-1\n",
    "# batch_size=(df.shape[1]-1)*2\n",
    "\n",
    "print(neurons)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Explaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1343101\n",
      "Class 1: 1343101\n",
      "Proportion: 1.0 : 1\n"
     ]
    }
   ],
   "source": [
    "#if you don't have an intuitive sense of how imbalanced these two classes are, let's go visual\n",
    "count_classes = pd.value_counts(df['class'], sort = True)\n",
    "print('Class 0:', count_classes[0])\n",
    "print('Class 1:', count_classes[1])\n",
    "print('Proportion:', round(count_classes[0] / count_classes[1], 3), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.xticks(range(2), ['Normal [0]','Malicious [1]'])\n",
    "plt.title(\"Frequency by observation number\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Observations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed and percentage of test data\n",
    "RANDOM_SEED = 23 #used to help randomly select the data points\n",
    "TEST_PCT = 0.20 # 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_df = train_test_split(df, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ df -> original dataset \n",
    "+ train -> subset of 80% from original dataset \n",
    "+ test_df -> subset of 20% from original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(train, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train -> subset of 80% from original dataset \n",
    "+ train_df -> subset of 80% from train\n",
    "+ dev_df -> subset of 20% from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000465341374433\n",
      "0.49896578120164825\n",
      "0.5006784664610482\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of mal samples in train and test set\n",
    "print(train_df.iloc[:, batch_size].sum()/train_df.shape[0]) \n",
    "print(dev_df.iloc[:, batch_size].sum()/dev_df.shape[0]) \n",
    "print(test_df.iloc[:, batch_size].sum()/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.iloc[:, :batch_size] \n",
    "dev_x = dev_df.iloc[:, :batch_size] \n",
    "test_x = test_df.iloc[:, :batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_x -> features of train_df **Training subset for AE**\n",
    "+ dev_x -> features of dev_df **Validation subset for AE**\n",
    "+ test_x -> features of test_df **Testing subset for ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final train and test sets\n",
    "train_y = train_df.iloc[:,batch_size]\n",
    "dev_y = dev_df.iloc[:,batch_size]\n",
    "test_y = test_df.iloc[:,batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_y -> **Labels for supervised training of ANN**\n",
    "+ dev_y -> labels of dev_df  *not used for AE neither ANN*\n",
    "+ test_y -> labels of test_df  **Ground Truth for predictions of supervised ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "train_x =np.array(train_x)\n",
    "dev_x =np.array(dev_x)\n",
    "test_x = np.array(test_x)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "dev_y = np.array(dev_y)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "print(train_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(factor_enc_dim, enc_activation, dec_activation, \n",
    "                optimizer, loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer #RELU\n",
    "    encoded = Dense(encoding_dim, activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer #SIMOID\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spae(factor_enc_dim,dec_activation,enc_activation,\n",
    "         optimizer,loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer\n",
    "    encoded = Dense(encoding_dim, activity_regularizer=regularizers.l1(1e-4), activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pca(thr):\n",
    "    #train_x_pca,test_x_pca = to_pca(0.95)\n",
    "    pca = PCA(n_components = thr, svd_solver = 'full')\n",
    "    train_x_ = np.array(train_x)\n",
    "    print(type(train_x_))\n",
    "\n",
    "    test_x_ = np.array(test_x)\n",
    "    print(type(test_x_))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(time.ctime(start_time))\n",
    "\n",
    "    train_x_pca = pca.fit_transform(train_x_)\n",
    "    print(train_x_pca.shape)\n",
    "\n",
    "    test_x_pca = pca.fit_transform(test_x_)\n",
    "    print(test_x_pca.shape)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "\n",
    "    print(\"--- PCA spent %s seconds ---\" %elapsed_time )\n",
    "    \n",
    "    return  train_x_pca,test_x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ae(checkpoint_file, autoencoder,\n",
    "           epochs, batch_size, shuffle):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    hist_auto = autoencoder.fit(train_x, train_x,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    verbose=verbose_level,\n",
    "                    callbacks=[early_stopping, cp, tb],\n",
    "                    validation_data=(dev_x, dev_x))\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "    \n",
    "    return hist_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_auto(hist_auto, fig_file):\n",
    "    best_loss_value = hist_auto.history['loss'][-1]\n",
    "    print('Best loss value:', best_loss_value)\n",
    "\n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist_auto.history['loss'])\n",
    "    plt.plot(hist_auto.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.savefig(fig_file)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h(neurons,encoded_train_x,init_mode,activation_input,\n",
    "               weight_constraint,dropout_rate,activation_output,\n",
    "               loss,optimizer):\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=encoded_train_x.shape[1],\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h_():\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=input_dim,\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(checkpoint_file,ann,enc_train_x,train_y,epochs,shuffle,batch_size):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    history = ann.fit(enc_train_x,\n",
    "                      train_y,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[early_stopping, cp, tb],\n",
    "                      epochs=epochs,\n",
    "                      shuffle=shuffle,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=verbose_level)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict(ann,enc_test_x):\n",
    "    pred_ann_prob = ann.predict(enc_test_x)\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "    \n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob, pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict_():\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))  \n",
    "\n",
    "    modelk = KerasClassifier(build_fn=ann_2h_,\n",
    "                             epochs=epochs, \n",
    "                             batch_size=batch_size, \n",
    "                             verbose=verbose_level\n",
    "                            )\n",
    "\n",
    "    pred_ann_prob = cross_val_predict(modelk,\n",
    "                                      enc_test_x,\n",
    "                                      test_y,\n",
    "                                      cv=KFold(n_splits=5, random_state=23),\n",
    "                                      verbose=1)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "\n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob,pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cm(pred_ann_prob, pred_ann_01, roc_file, cm_file):\n",
    "    false_positive_rate, recall, thresholds = roc_curve(test_y, pred_ann_prob)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.savefig(roc_file)\n",
    "    plt.show()\n",
    "    \n",
    "    cm = confusion_matrix(test_y, pred_ann_01)\n",
    "    labels = ['Normal', 'Malicious']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=\"RdYlGn\", vmin = 0.2);\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.savefig(cm_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- PCA Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_pca,test_x_pca = to_pca(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- AE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 66)                0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 44)                2948      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 66)                2970      \n",
      "=================================================================\n",
      "Total params: 5,918\n",
      "Trainable params: 5,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_sigmoid_adam_mse,enc_train_x_asam,enc_test_x_asam = ae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ae_sigmoid_adam_mse = load_model('ae_sigmoid_adam_mse_redds10bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 00:03:46 2019\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1719168 samples, validate on 429793 samples\n",
      "Epoch 1/200\n",
      "1719168/1719168 [==============================] - 23s 14us/step - loss: 0.1646 - acc: 0.2011 - val_loss: 0.1577 - val_acc: 0.2088\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15765, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 2/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1568 - acc: 0.2138 - val_loss: 0.1575 - val_acc: 0.1867\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15765 to 0.15752, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 3/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1568 - acc: 0.1937 - val_loss: 0.1575 - val_acc: 0.1897\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15752 to 0.15752, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 4/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1568 - acc: 0.1966 - val_loss: 0.1575 - val_acc: 0.2050\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15752 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 5/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1568 - acc: 0.2038 - val_loss: 0.1575 - val_acc: 0.2035\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 6/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1568 - acc: 0.2046 - val_loss: 0.1575 - val_acc: 0.2015\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 7/200\n",
      "1719168/1719168 [==============================] - 23s 14us/step - loss: 0.1568 - acc: 0.2075 - val_loss: 0.1575 - val_acc: 0.2014\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 8/200\n",
      "1719168/1719168 [==============================] - 23s 14us/step - loss: 0.1568 - acc: 0.2025 - val_loss: 0.1575 - val_acc: 0.1962\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 9/200\n",
      "1719168/1719168 [==============================] - 23s 14us/step - loss: 0.1568 - acc: 0.2012 - val_loss: 0.1575 - val_acc: 0.1920\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 10/200\n",
      "1719168/1719168 [==============================] - 23s 14us/step - loss: 0.1568 - acc: 0.1915 - val_loss: 0.1575 - val_acc: 0.1836\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 11/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1568 - acc: 0.1849 - val_loss: 0.1575 - val_acc: 0.1789\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 12/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1568 - acc: 0.1798 - val_loss: 0.1575 - val_acc: 0.1735\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 13/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1568 - acc: 0.1769 - val_loss: 0.1575 - val_acc: 0.1756\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 14/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1780 - val_loss: 0.1575 - val_acc: 0.1753\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 15/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1727 - val_loss: 0.1575 - val_acc: 0.1702\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 16/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1739 - val_loss: 0.1575 - val_acc: 0.1702\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 17/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1725 - val_loss: 0.1575 - val_acc: 0.1687\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 18/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1705 - val_loss: 0.1575 - val_acc: 0.1680\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 19/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1693 - val_loss: 0.1575 - val_acc: 0.1666\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 20/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1726 - val_loss: 0.1575 - val_acc: 0.1721\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 21/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1752 - val_loss: 0.1575 - val_acc: 0.1749\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 22/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1800 - val_loss: 0.1575 - val_acc: 0.1801\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 23/200\n",
      "1719168/1719168 [==============================] - 23s 14us/step - loss: 0.1567 - acc: 0.1853 - val_loss: 0.1575 - val_acc: 0.1898\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 24/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1833 - val_loss: 0.1575 - val_acc: 0.1743\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 25/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1763 - val_loss: 0.1575 - val_acc: 0.1840\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15751\n",
      "Epoch 26/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1792 - val_loss: 0.1575 - val_acc: 0.1734\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 27/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1754 - val_loss: 0.1575 - val_acc: 0.1678\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 28/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1751 - val_loss: 0.1575 - val_acc: 0.1742\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 29/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1766 - val_loss: 0.1575 - val_acc: 0.1720\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 30/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1739 - val_loss: 0.1575 - val_acc: 0.1701\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 31/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1711 - val_loss: 0.1575 - val_acc: 0.1709\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 32/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1726 - val_loss: 0.1575 - val_acc: 0.1689\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.15751 to 0.15751, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 33/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1776 - val_loss: 0.1575 - val_acc: 0.1818\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.15751 to 0.15750, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 34/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1910 - val_loss: 0.1575 - val_acc: 0.1913\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.15750 to 0.15750, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 35/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1969 - val_loss: 0.1575 - val_acc: 0.1980\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.15750\n",
      "Epoch 36/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.2003 - val_loss: 0.1574 - val_acc: 0.2174\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.15750 to 0.15743, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 37/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.2176 - val_loss: 0.1574 - val_acc: 0.2081\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.15743 to 0.15743, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 38/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.2075 - val_loss: 0.1574 - val_acc: 0.1992\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.15743 to 0.15743, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 39/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.2006 - val_loss: 0.1574 - val_acc: 0.1931\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.15743 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 40/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1953 - val_loss: 0.1574 - val_acc: 0.1917\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 41/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1963 - val_loss: 0.1574 - val_acc: 0.1883\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 42/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1910 - val_loss: 0.1574 - val_acc: 0.1848\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 43/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1843 - val_loss: 0.1574 - val_acc: 0.1884\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 44/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1901 - val_loss: 0.1574 - val_acc: 0.1850\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 45/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1882 - val_loss: 0.1574 - val_acc: 0.1850\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 46/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1853 - val_loss: 0.1574 - val_acc: 0.1731\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 47/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1792 - val_loss: 0.1574 - val_acc: 0.1760\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 48/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1795 - val_loss: 0.1574 - val_acc: 0.1784\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 49/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1796 - val_loss: 0.1574 - val_acc: 0.1759\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 50/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1791 - val_loss: 0.1574 - val_acc: 0.1761\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 51/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1762 - val_loss: 0.1574 - val_acc: 0.1707\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 52/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1746 - val_loss: 0.1574 - val_acc: 0.1806\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 53/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1816 - val_loss: 0.1574 - val_acc: 0.1790\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 54/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1813 - val_loss: 0.1574 - val_acc: 0.2058\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 55/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1875 - val_loss: 0.1574 - val_acc: 0.1804\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 56/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1792 - val_loss: 0.1574 - val_acc: 0.1787\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 57/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1752 - val_loss: 0.1574 - val_acc: 0.1726\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 58/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1730 - val_loss: 0.1574 - val_acc: 0.1726\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.15742\n",
      "Epoch 59/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1766 - val_loss: 0.1574 - val_acc: 0.1950\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 60/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1863 - val_loss: 0.1574 - val_acc: 0.1856\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 61/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1803 - val_loss: 0.1574 - val_acc: 0.1788\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 62/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1728 - val_loss: 0.1574 - val_acc: 0.1745\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 63/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1717 - val_loss: 0.1574 - val_acc: 0.1739\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 64/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1759 - val_loss: 0.1574 - val_acc: 0.1820\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 65/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1808 - val_loss: 0.1574 - val_acc: 0.1856\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 66/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1862 - val_loss: 0.1574 - val_acc: 0.1887\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 67/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1931 - val_loss: 0.1574 - val_acc: 0.1927\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 68/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1950 - val_loss: 0.1574 - val_acc: 0.1956\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.15742 to 0.15742, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 69/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1958 - val_loss: 0.1574 - val_acc: 0.2079\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.15742\n",
      "Epoch 70/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1971 - val_loss: 0.1574 - val_acc: 0.1983\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.15742 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 71/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.2010 - val_loss: 0.1574 - val_acc: 0.2136\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.15741\n",
      "Epoch 72/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1986 - val_loss: 0.1574 - val_acc: 0.1931\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 73/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1883 - val_loss: 0.1574 - val_acc: 0.1864\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 74/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1871 - val_loss: 0.1574 - val_acc: 0.1862\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 75/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1881 - val_loss: 0.1574 - val_acc: 0.1875\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 76/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1835 - val_loss: 0.1574 - val_acc: 0.1799\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.15741\n",
      "Epoch 77/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1809 - val_loss: 0.1574 - val_acc: 0.1861\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 78/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1825 - val_loss: 0.1574 - val_acc: 0.1793\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 79/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1802 - val_loss: 0.1574 - val_acc: 0.1861\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.15741\n",
      "Epoch 80/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1839 - val_loss: 0.1574 - val_acc: 0.1854\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.15741\n",
      "Epoch 81/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1848 - val_loss: 0.1574 - val_acc: 0.1865\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.15741\n",
      "Epoch 82/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1833 - val_loss: 0.1574 - val_acc: 0.1883\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 83/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1862 - val_loss: 0.1574 - val_acc: 0.1888\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.15741\n",
      "Epoch 84/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1834 - val_loss: 0.1574 - val_acc: 0.1845\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 85/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1808 - val_loss: 0.1574 - val_acc: 0.1793\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.15741\n",
      "Epoch 86/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1743 - val_loss: 0.1574 - val_acc: 0.1700\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 87/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1709 - val_loss: 0.1574 - val_acc: 0.1479\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.15741\n",
      "Epoch 88/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1596 - val_loss: 0.1574 - val_acc: 0.1616\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 89/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1644 - val_loss: 0.1574 - val_acc: 0.1651\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 90/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1622 - val_loss: 0.1574 - val_acc: 0.1646\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 91/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1640 - val_loss: 0.1574 - val_acc: 0.1723\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.15741\n",
      "Epoch 92/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1703 - val_loss: 0.1574 - val_acc: 0.1699\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 93/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1747 - val_loss: 0.1574 - val_acc: 0.1779\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.15741\n",
      "Epoch 94/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1653 - val_loss: 0.1574 - val_acc: 0.1648\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.15741\n",
      "Epoch 95/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1686 - val_loss: 0.1574 - val_acc: 0.1714\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.15741\n",
      "Epoch 96/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1715 - val_loss: 0.1574 - val_acc: 0.1750\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.15741\n",
      "Epoch 97/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1688 - val_loss: 0.1574 - val_acc: 0.1679\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 98/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1679 - val_loss: 0.1574 - val_acc: 0.1814\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.15741\n",
      "Epoch 99/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1714 - val_loss: 0.1574 - val_acc: 0.1629\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.15741\n",
      "Epoch 100/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1541 - val_loss: 0.1574 - val_acc: 0.1577\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 101/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1551 - val_loss: 0.1574 - val_acc: 0.1524\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.15741\n",
      "Epoch 102/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1343 - val_loss: 0.1574 - val_acc: 0.1433\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 103/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1543 - val_loss: 0.1574 - val_acc: 0.1599\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.15741\n",
      "Epoch 104/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1579 - val_loss: 0.1574 - val_acc: 0.1596\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 105/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1597 - val_loss: 0.1574 - val_acc: 0.1577\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 106/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1709 - val_loss: 0.1574 - val_acc: 0.1656\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.15741\n",
      "Epoch 107/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1640 - val_loss: 0.1574 - val_acc: 0.1610\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 108/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1613 - val_loss: 0.1574 - val_acc: 0.1595\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 109/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1623 - val_loss: 0.1574 - val_acc: 0.1644\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 110/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1628 - val_loss: 0.1574 - val_acc: 0.1565\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 111/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1620 - val_loss: 0.1574 - val_acc: 0.1650\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 112/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1433 - val_loss: 0.1574 - val_acc: 0.1525\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 113/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1606 - val_loss: 0.1574 - val_acc: 0.1664\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 114/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1689 - val_loss: 0.1574 - val_acc: 0.1700\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.15741\n",
      "Epoch 115/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1695 - val_loss: 0.1574 - val_acc: 0.1758\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 116/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1693 - val_loss: 0.1574 - val_acc: 0.1688\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.15741\n",
      "Epoch 117/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1724 - val_loss: 0.1574 - val_acc: 0.1660\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.15741\n",
      "Epoch 118/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1688 - val_loss: 0.1574 - val_acc: 0.1742\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.15741\n",
      "Epoch 119/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1700 - val_loss: 0.1574 - val_acc: 0.1655\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.15741\n",
      "Epoch 120/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1663 - val_loss: 0.1574 - val_acc: 0.1708\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 121/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1659 - val_loss: 0.1574 - val_acc: 0.1679\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.15741\n",
      "Epoch 122/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1644 - val_loss: 0.1574 - val_acc: 0.1645\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.15741\n",
      "Epoch 123/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1658 - val_loss: 0.1574 - val_acc: 0.1624\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.15741\n",
      "Epoch 124/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1660 - val_loss: 0.1574 - val_acc: 0.1711\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 125/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1712 - val_loss: 0.1574 - val_acc: 0.1727\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 126/200\n",
      "1719168/1719168 [==============================] - 23s 14us/step - loss: 0.1567 - acc: 0.1703 - val_loss: 0.1574 - val_acc: 0.1670\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 127/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1696 - val_loss: 0.1574 - val_acc: 0.1707\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 128/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1477 - val_loss: 0.1574 - val_acc: 0.1555\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 129/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1579 - val_loss: 0.1574 - val_acc: 0.1650\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.15741\n",
      "Epoch 130/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1741 - val_loss: 0.1574 - val_acc: 0.1652\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.15741\n",
      "Epoch 131/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1742 - val_loss: 0.1574 - val_acc: 0.1779\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 132/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1663 - val_loss: 0.1574 - val_acc: 0.1799\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.15741\n",
      "Epoch 133/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1818 - val_loss: 0.1574 - val_acc: 0.1850\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 134/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1872 - val_loss: 0.1574 - val_acc: 0.1917\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.15741\n",
      "Epoch 135/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1896 - val_loss: 0.1574 - val_acc: 0.1877\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 136/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1634 - val_loss: 0.1574 - val_acc: 0.1709\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 137/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1567 - acc: 0.1685 - val_loss: 0.1574 - val_acc: 0.1755\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 138/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1819 - val_loss: 0.1574 - val_acc: 0.1859\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 139/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1775 - val_loss: 0.1574 - val_acc: 0.1915\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.15741\n",
      "Epoch 140/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1864 - val_loss: 0.1574 - val_acc: 0.1830\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 141/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1868 - val_loss: 0.1574 - val_acc: 0.1871\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 142/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1805 - val_loss: 0.1574 - val_acc: 0.1784\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.15741\n",
      "Epoch 143/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1341 - val_loss: 0.1574 - val_acc: 0.1419\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 144/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1336 - val_loss: 0.1574 - val_acc: 0.1429\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 145/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1508 - val_loss: 0.1574 - val_acc: 0.1620\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 146/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1681 - val_loss: 0.1574 - val_acc: 0.1739\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 147/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1751 - val_loss: 0.1574 - val_acc: 0.1769\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.15741\n",
      "Epoch 148/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1819 - val_loss: 0.1574 - val_acc: 0.1830\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.15741\n",
      "Epoch 149/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1834 - val_loss: 0.1574 - val_acc: 0.1851\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.15741\n",
      "Epoch 150/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1860 - val_loss: 0.1574 - val_acc: 0.1852\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 151/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1829 - val_loss: 0.1574 - val_acc: 0.1835\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 152/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1875 - val_loss: 0.1574 - val_acc: 0.1891\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 153/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1916 - val_loss: 0.1574 - val_acc: 0.1896\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 154/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1859 - val_loss: 0.1574 - val_acc: 0.1869\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 155/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1895 - val_loss: 0.1574 - val_acc: 0.1895\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.15741 to 0.15741, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 156/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1902 - val_loss: 0.1574 - val_acc: 0.1901\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.15741 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 157/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1932 - val_loss: 0.1574 - val_acc: 0.1922\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 158/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1939 - val_loss: 0.1574 - val_acc: 0.1924\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 159/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1992 - val_loss: 0.1574 - val_acc: 0.2059\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 160/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1963 - val_loss: 0.1574 - val_acc: 0.1903\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 161/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1623 - val_loss: 0.1574 - val_acc: 0.1672\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 162/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1737 - val_loss: 0.1574 - val_acc: 0.1637\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.15740\n",
      "Epoch 163/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1801 - val_loss: 0.1574 - val_acc: 0.1849\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 164/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1926 - val_loss: 0.1574 - val_acc: 0.1985\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 165/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1889 - val_loss: 0.1574 - val_acc: 0.1894\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 166/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1931 - val_loss: 0.1574 - val_acc: 0.1963\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 167/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1988 - val_loss: 0.1574 - val_acc: 0.2008\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 168/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2035 - val_loss: 0.1574 - val_acc: 0.2025\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.15740\n",
      "Epoch 169/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2037 - val_loss: 0.1574 - val_acc: 0.2008\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.15740\n",
      "Epoch 170/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1991 - val_loss: 0.1574 - val_acc: 0.1991\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 171/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1979 - val_loss: 0.1574 - val_acc: 0.1901\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.15740\n",
      "Epoch 172/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1919 - val_loss: 0.1574 - val_acc: 0.1906\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 173/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1914 - val_loss: 0.1574 - val_acc: 0.1902\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 174/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1899 - val_loss: 0.1574 - val_acc: 0.1916\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.15740\n",
      "Epoch 175/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1937 - val_loss: 0.1574 - val_acc: 0.1919\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 176/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1927 - val_loss: 0.1574 - val_acc: 0.1917\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 177/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1995 - val_loss: 0.1574 - val_acc: 0.2225\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.15740\n",
      "Epoch 178/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2091 - val_loss: 0.1574 - val_acc: 0.1988\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 179/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1993 - val_loss: 0.1574 - val_acc: 0.1999\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 180/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2017 - val_loss: 0.1574 - val_acc: 0.1993\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 181/200\n",
      "1719168/1719168 [==============================] - 23s 14us/step - loss: 0.1566 - acc: 0.2021 - val_loss: 0.1574 - val_acc: 0.2011\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 182/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2014 - val_loss: 0.1574 - val_acc: 0.1975\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 183/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1969 - val_loss: 0.1574 - val_acc: 0.1941\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.15740\n",
      "Epoch 184/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1966 - val_loss: 0.1574 - val_acc: 0.1954\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.15740\n",
      "Epoch 185/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1957 - val_loss: 0.1574 - val_acc: 0.1956\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 186/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1983 - val_loss: 0.1574 - val_acc: 0.1971\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.15740\n",
      "Epoch 187/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1703 - val_loss: 0.1574 - val_acc: 0.1673\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 188/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1788 - val_loss: 0.1574 - val_acc: 0.1887\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.15740\n",
      "Epoch 189/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.1915 - val_loss: 0.1574 - val_acc: 0.1918\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.15740\n",
      "Epoch 190/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2102 - val_loss: 0.1574 - val_acc: 0.2234\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 191/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2151 - val_loss: 0.1574 - val_acc: 0.2099\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 192/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2084 - val_loss: 0.1574 - val_acc: 0.2054\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.15740\n",
      "Epoch 193/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2092 - val_loss: 0.1574 - val_acc: 0.2082\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.15740\n",
      "Epoch 194/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2099 - val_loss: 0.1574 - val_acc: 0.2065\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 195/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2072 - val_loss: 0.1574 - val_acc: 0.2066\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.15740\n",
      "Epoch 196/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2167 - val_loss: 0.1574 - val_acc: 0.2173\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.15740\n",
      "Epoch 197/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2181 - val_loss: 0.1574 - val_acc: 0.2136\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.15740\n",
      "Epoch 198/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2156 - val_loss: 0.1574 - val_acc: 0.2112\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 199/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2161 - val_loss: 0.1574 - val_acc: 0.2148\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.15740\n",
      "Epoch 200/200\n",
      "1719168/1719168 [==============================] - 23s 13us/step - loss: 0.1566 - acc: 0.2116 - val_loss: 0.1574 - val_acc: 0.2059\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.15740 to 0.15740, saving model to ./H5files/ae_sigmoid_adam_mse_redds20bal.h5\n",
      "Time elapsed (hh:mm:ss.ms) 1:16:44.227868\n"
     ]
    }
   ],
   "source": [
    "hist_ae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/ae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = ae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "#                                   batch_size = batch_size,\n",
    "                                  batch_size = batch_size*4,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.15663898915638208\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_ae_sigmoid_adam_mse  = plot_hist_auto(hist_ae_sigmoid_adam_mse, './Figures/hist_ae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- SPAE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 66)                0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 44)                2948      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 66)                2970      \n",
      "=================================================================\n",
      "Total params: 5,918\n",
      "Trainable params: 5,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "spae_sigmoid_adam_mse,enc_train_x_spsam,enc_test_x_spsam = spae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spae_sigmoid_adam_mse = load_model('spae_sigmoid_adam_mse_redds20bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 01:20:56 2019\n",
      "Train on 1719168 samples, validate on 429793 samples\n",
      "Epoch 1/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2768 - acc: 0.0245 - val_loss: 0.2391 - val_acc: 0.0265\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23910, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 2/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2299 - acc: 0.0264 - val_loss: 0.2252 - val_acc: 0.0269\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.23910 to 0.22517, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 3/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2214 - acc: 0.0284 - val_loss: 0.2196 - val_acc: 0.0295\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22517 to 0.21963, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 4/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2163 - acc: 0.0314 - val_loss: 0.2144 - val_acc: 0.0296\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21963 to 0.21437, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 5/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2117 - acc: 0.0344 - val_loss: 0.2103 - val_acc: 0.0257\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21437 to 0.21028, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 6/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2082 - acc: 0.0285 - val_loss: 0.2078 - val_acc: 0.0294\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.21028 to 0.20780, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 7/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2062 - acc: 0.0335 - val_loss: 0.2061 - val_acc: 0.0364\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20780 to 0.20614, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 8/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2048 - acc: 0.0529 - val_loss: 0.2050 - val_acc: 0.0654\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.20614 to 0.20497, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 9/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2037 - acc: 0.0699 - val_loss: 0.2041 - val_acc: 0.0754\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.20497 to 0.20407, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 10/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2029 - acc: 0.0747 - val_loss: 0.2034 - val_acc: 0.0787\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.20407 to 0.20336, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 11/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2023 - acc: 0.0773 - val_loss: 0.2028 - val_acc: 0.0808\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.20336 to 0.20278, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 12/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2018 - acc: 0.0808 - val_loss: 0.2023 - val_acc: 0.0838\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.20278 to 0.20230, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 13/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2013 - acc: 0.0832 - val_loss: 0.2019 - val_acc: 0.0865\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.20230 to 0.20188, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 14/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2009 - acc: 0.0853 - val_loss: 0.2015 - val_acc: 0.0891\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.20188 to 0.20152, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 15/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.2006 - acc: 0.0870 - val_loss: 0.2012 - val_acc: 0.0913\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.20152 to 0.20121, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 16/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.2003 - acc: 0.0895 - val_loss: 0.2010 - val_acc: 0.0926\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.20121 to 0.20097, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 17/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.2000 - acc: 0.0903 - val_loss: 0.2007 - val_acc: 0.0932\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.20097 to 0.20070, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 18/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1998 - acc: 0.0939 - val_loss: 0.2004 - val_acc: 0.1098\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.20070 to 0.20036, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 19/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1994 - acc: 0.1301 - val_loss: 0.1999 - val_acc: 0.1415\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.20036 to 0.19995, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 20/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1989 - acc: 0.1432 - val_loss: 0.1995 - val_acc: 0.1407\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.19995 to 0.19954, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 21/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1986 - acc: 0.1435 - val_loss: 0.1992 - val_acc: 0.1398\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.19954 to 0.19923, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 22/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1983 - acc: 0.1439 - val_loss: 0.1990 - val_acc: 0.1422\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.19923 to 0.19898, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 23/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1981 - acc: 0.1448 - val_loss: 0.1988 - val_acc: 0.1437\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.19898 to 0.19878, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 24/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1979 - acc: 0.1471 - val_loss: 0.1986 - val_acc: 0.1433\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.19878 to 0.19861, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 25/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1978 - acc: 0.1491 - val_loss: 0.1985 - val_acc: 0.1502\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.19861 to 0.19846, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 26/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1976 - acc: 0.1514 - val_loss: 0.1983 - val_acc: 0.1456\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.19846 to 0.19835, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 27/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1975 - acc: 0.1554 - val_loss: 0.1982 - val_acc: 0.1521\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.19835 to 0.19821, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 28/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1974 - acc: 0.1582 - val_loss: 0.1981 - val_acc: 0.1581\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.19821 to 0.19815, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 29/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1973 - acc: 0.1628 - val_loss: 0.1980 - val_acc: 0.1566\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.19815 to 0.19800, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 30/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1972 - acc: 0.1661 - val_loss: 0.1979 - val_acc: 0.2193\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.19800 to 0.19787, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 31/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1971 - acc: 0.1713 - val_loss: 0.1978 - val_acc: 0.1602\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.19787 to 0.19779, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 32/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1970 - acc: 0.1756 - val_loss: 0.1977 - val_acc: 0.1620\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.19779 to 0.19770, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 33/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1969 - acc: 0.1818 - val_loss: 0.1976 - val_acc: 0.2133\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.19770 to 0.19761, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 34/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1969 - acc: 0.1872 - val_loss: 0.1975 - val_acc: 0.1862\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.19761 to 0.19754, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 35/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1968 - acc: 0.1920 - val_loss: 0.1975 - val_acc: 0.1805\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.19754 to 0.19748, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 36/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1967 - acc: 0.1948 - val_loss: 0.1975 - val_acc: 0.2284\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.19748 to 0.19745, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 37/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1967 - acc: 0.2017 - val_loss: 0.1974 - val_acc: 0.2148\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.19745 to 0.19740, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 38/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1966 - acc: 0.2062 - val_loss: 0.1973 - val_acc: 0.2449\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.19740 to 0.19731, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 39/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1966 - acc: 0.2109 - val_loss: 0.1973 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.19731 to 0.19728, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 40/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1965 - acc: 0.2165 - val_loss: 0.1972 - val_acc: 0.2457\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.19728 to 0.19722, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 41/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1965 - acc: 0.2223 - val_loss: 0.1972 - val_acc: 0.2355\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.19722\n",
      "Epoch 42/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1964 - acc: 0.2262 - val_loss: 0.1971 - val_acc: 0.2215\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.19722 to 0.19712, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 43/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1964 - acc: 0.2307 - val_loss: 0.1971 - val_acc: 0.2385\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.19712\n",
      "Epoch 44/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1963 - acc: 0.2350 - val_loss: 0.1971 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.19712 to 0.19709, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 45/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1963 - acc: 0.2393 - val_loss: 0.1971 - val_acc: 0.2596\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.19709 to 0.19706, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 46/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1962 - acc: 0.2442 - val_loss: 0.1969 - val_acc: 0.2470\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.19706 to 0.19695, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 47/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1962 - acc: 0.2470 - val_loss: 0.1969 - val_acc: 0.2503\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.19695 to 0.19691, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 48/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1962 - acc: 0.2502 - val_loss: 0.1969 - val_acc: 0.2499\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.19691 to 0.19688, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 49/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1961 - acc: 0.2535 - val_loss: 0.1969 - val_acc: 0.2529\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.19688\n",
      "Epoch 50/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1961 - acc: 0.2558 - val_loss: 0.1968 - val_acc: 0.2585\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.19688 to 0.19681, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 51/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1961 - acc: 0.2577 - val_loss: 0.1969 - val_acc: 0.2482\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.19681\n",
      "Epoch 52/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1960 - acc: 0.2608 - val_loss: 0.1968 - val_acc: 0.2491\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.19681 to 0.19680, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 53/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1960 - acc: 0.2615 - val_loss: 0.1967 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.19680 to 0.19675, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 54/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1960 - acc: 0.2631 - val_loss: 0.1967 - val_acc: 0.2583\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.19675 to 0.19674, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 55/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1960 - acc: 0.2649 - val_loss: 0.1967 - val_acc: 0.2598\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.19674 to 0.19673, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 56/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1959 - acc: 0.2675 - val_loss: 0.1967 - val_acc: 0.2615\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.19673 to 0.19668, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 57/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1959 - acc: 0.2680 - val_loss: 0.1967 - val_acc: 0.2653\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.19668 to 0.19668, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 58/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1959 - acc: 0.2689 - val_loss: 0.1966 - val_acc: 0.2673\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.19668 to 0.19661, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 59/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1959 - acc: 0.2692 - val_loss: 0.1968 - val_acc: 0.2446\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.19661\n",
      "Epoch 60/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1958 - acc: 0.2725 - val_loss: 0.1967 - val_acc: 0.2469\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.19661\n",
      "Epoch 61/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1958 - acc: 0.2721 - val_loss: 0.1966 - val_acc: 0.2690\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.19661 to 0.19655, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 62/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1958 - acc: 0.2741 - val_loss: 0.1965 - val_acc: 0.2518\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.19655 to 0.19653, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 63/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1958 - acc: 0.2751 - val_loss: 0.1966 - val_acc: 0.2726\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.19653\n",
      "Epoch 64/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1958 - acc: 0.2759 - val_loss: 0.1965 - val_acc: 0.2708\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.19653 to 0.19649, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 65/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1958 - acc: 0.2773 - val_loss: 0.1967 - val_acc: 0.2312\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.19649\n",
      "Epoch 66/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1957 - acc: 0.2793 - val_loss: 0.1964 - val_acc: 0.2752\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.19649 to 0.19643, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 67/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1957 - acc: 0.2795 - val_loss: 0.1964 - val_acc: 0.2763\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.19643\n",
      "Epoch 68/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1957 - acc: 0.2808 - val_loss: 0.1965 - val_acc: 0.2592\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.19643\n",
      "Epoch 69/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1957 - acc: 0.2815 - val_loss: 0.1964 - val_acc: 0.2786\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.19643 to 0.19638, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 70/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1957 - acc: 0.2818 - val_loss: 0.1967 - val_acc: 0.1903\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19638\n",
      "Epoch 71/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1957 - acc: 0.2822 - val_loss: 0.1964 - val_acc: 0.2742\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.19638 to 0.19638, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 72/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1957 - acc: 0.2841 - val_loss: 0.1964 - val_acc: 0.2821\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19638\n",
      "Epoch 73/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1956 - acc: 0.2859 - val_loss: 0.1965 - val_acc: 0.2787\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19638\n",
      "Epoch 74/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1956 - acc: 0.2847 - val_loss: 0.1964 - val_acc: 0.2772\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.19638 to 0.19636, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 75/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1956 - acc: 0.2857 - val_loss: 0.1963 - val_acc: 0.2865\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.19636 to 0.19631, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 76/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1956 - acc: 0.2878 - val_loss: 0.1963 - val_acc: 0.2847\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19631\n",
      "Epoch 77/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1956 - acc: 0.2868 - val_loss: 0.1963 - val_acc: 0.2705\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.19631\n",
      "Epoch 78/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1956 - acc: 0.2881 - val_loss: 0.1963 - val_acc: 0.2903\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.19631 to 0.19625, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 79/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1956 - acc: 0.2882 - val_loss: 0.1963 - val_acc: 0.2923\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.19625\n",
      "Epoch 80/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1956 - acc: 0.2891 - val_loss: 0.1963 - val_acc: 0.2898\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.19625\n",
      "Epoch 81/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1955 - acc: 0.2884 - val_loss: 0.1962 - val_acc: 0.2915\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.19625 to 0.19624, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 82/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1955 - acc: 0.2910 - val_loss: 0.1963 - val_acc: 0.2923\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.19624\n",
      "Epoch 83/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1955 - acc: 0.2910 - val_loss: 0.1962 - val_acc: 0.2879\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.19624\n",
      "Epoch 84/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1955 - acc: 0.2917 - val_loss: 0.1965 - val_acc: 0.2927\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.19624\n",
      "Epoch 85/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1956 - acc: 0.2922 - val_loss: 0.1962 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.19624 to 0.19619, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 86/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1955 - acc: 0.2905 - val_loss: 0.1964 - val_acc: 0.2864\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.19619\n",
      "Epoch 87/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1955 - acc: 0.2942 - val_loss: 0.1962 - val_acc: 0.2873\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.19619 to 0.19618, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 88/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1955 - acc: 0.2949 - val_loss: 0.1962 - val_acc: 0.2981\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.19618\n",
      "Epoch 89/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1955 - acc: 0.2940 - val_loss: 0.1962 - val_acc: 0.2972\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.19618\n",
      "Epoch 90/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1955 - acc: 0.2949 - val_loss: 0.1962 - val_acc: 0.2996\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.19618 to 0.19617, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 91/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1954 - acc: 0.2960 - val_loss: 0.1962 - val_acc: 0.2921\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.19617\n",
      "Epoch 92/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1954 - acc: 0.2956 - val_loss: 0.1963 - val_acc: 0.3006\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.19617\n",
      "Epoch 93/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1954 - acc: 0.2963 - val_loss: 0.1961 - val_acc: 0.3007\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.19617 to 0.19614, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 94/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1954 - acc: 0.2963 - val_loss: 0.1961 - val_acc: 0.3011\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.19614 to 0.19609, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 95/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1954 - acc: 0.2963 - val_loss: 0.1964 - val_acc: 0.2254\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.19609\n",
      "Epoch 96/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1954 - acc: 0.2977 - val_loss: 0.1961 - val_acc: 0.2929\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.19609 to 0.19608, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 97/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1954 - acc: 0.2972 - val_loss: 0.1961 - val_acc: 0.2969\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.19608 to 0.19607, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 98/200\n",
      "1719168/1719168 [==============================] - 24s 14us/step - loss: 0.1954 - acc: 0.2977 - val_loss: 0.1960 - val_acc: 0.3053\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.19607 to 0.19603, saving model to ./H5files/spae_sigmoid_adam_mse_redds20bal.h5\n",
      "Epoch 99/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1954 - acc: 0.2985 - val_loss: 0.1961 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.19603\n",
      "Epoch 100/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1954 - acc: 0.2973 - val_loss: 0.1961 - val_acc: 0.2887\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.19603\n",
      "Epoch 101/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1954 - acc: 0.2997 - val_loss: 0.1961 - val_acc: 0.3082\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.19603\n",
      "Epoch 102/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1954 - acc: 0.2989 - val_loss: 0.1960 - val_acc: 0.3067\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.19603\n",
      "Epoch 103/200\n",
      "1719168/1719168 [==============================] - 25s 14us/step - loss: 0.1954 - acc: 0.2998 - val_loss: 0.1961 - val_acc: 0.2961\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.19603\n",
      "Time elapsed (hh:mm:ss.ms) 0:42:09.663385\n"
     ]
    }
   ],
   "source": [
    "hist_spae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/spae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = spae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size*4,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.19538502035209457\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_spae_sigmoid_adam_mse  = plot_hist_auto(hist_spae_sigmoid_adam_mse, './Figures/hist_spae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valueDict = {\n",
    "    'loss_value_ae_sigmoid_adam_mse': best_loss_value_ae_sigmoid_adam_mse,\n",
    "    'loss_value_spae_sigmoid_adam_mse': best_loss_value_spae_sigmoid_adam_mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_value_ae_sigmoid_adam_mse': 0.15663898915638208,\n",
       " 'loss_value_spae_sigmoid_adam_mse': 0.19538502035209457}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1719168, 44)\n",
      "(537241, 44)\n",
      "(1719168, 44)\n",
      "(537241, 44)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train_x_asam.shape)\n",
    "print(enc_test_x_asam.shape)\n",
    "\n",
    "print(enc_train_x_spsam.shape)\n",
    "print(enc_test_x_spsam.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_asam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 02:03:07 2019\n",
      "Train on 1375334 samples, validate on 343834 samples\n",
      "Epoch 1/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.4657 - acc: 0.7625 - val_loss: 0.3683 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36835, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 2/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.3199 - acc: 0.8501 - val_loss: 0.2927 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36835 to 0.29274, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 3/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.2814 - acc: 0.8682 - val_loss: 0.2821 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.29274 to 0.28206, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 4/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.2589 - acc: 0.8783 - val_loss: 0.2456 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.28206 to 0.24561, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 5/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.2448 - acc: 0.8849 - val_loss: 0.2511 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24561\n",
      "Epoch 6/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.2346 - acc: 0.8896 - val_loss: 0.2306 - val_acc: 0.8926\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24561 to 0.23057, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 7/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.2267 - acc: 0.8931 - val_loss: 0.2174 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23057 to 0.21743, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 8/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.2208 - acc: 0.8957 - val_loss: 0.2150 - val_acc: 0.8989\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.21743 to 0.21499, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 9/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.2160 - acc: 0.8976 - val_loss: 0.2093 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.21499 to 0.20928, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 10/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.2117 - acc: 0.8996 - val_loss: 0.2081 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.20928 to 0.20811, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 11/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.2066 - acc: 0.9023 - val_loss: 0.1968 - val_acc: 0.9073\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.20811 to 0.19681, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 12/200\n",
      "1375334/1375334 [==============================] - 36s 27us/step - loss: 0.2030 - acc: 0.9039 - val_loss: 0.1961 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.19681 to 0.19609, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 13/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1992 - acc: 0.9056 - val_loss: 0.1969 - val_acc: 0.9076\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19609\n",
      "Epoch 14/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1965 - acc: 0.9069 - val_loss: 0.1970 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19609\n",
      "Epoch 15/200\n",
      "1375334/1375334 [==============================] - 36s 27us/step - loss: 0.1950 - acc: 0.9076 - val_loss: 0.1896 - val_acc: 0.9117\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.19609 to 0.18959, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 16/200\n",
      "1375334/1375334 [==============================] - 36s 27us/step - loss: 0.1924 - acc: 0.9090 - val_loss: 0.1884 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.18959 to 0.18837, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 17/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.1903 - acc: 0.9099 - val_loss: 0.1942 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18837\n",
      "Epoch 18/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.1883 - acc: 0.9108 - val_loss: 0.1848 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.18837 to 0.18476, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 19/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.1856 - acc: 0.9118 - val_loss: 0.1816 - val_acc: 0.9143\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.18476 to 0.18158, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 20/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1835 - acc: 0.9127 - val_loss: 0.1790 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.18158 to 0.17903, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 21/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1822 - acc: 0.9133 - val_loss: 0.1791 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.17903\n",
      "Epoch 22/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1806 - acc: 0.9142 - val_loss: 0.1751 - val_acc: 0.9181\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.17903 to 0.17514, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 23/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1793 - acc: 0.9150 - val_loss: 0.1696 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.17514 to 0.16959, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 24/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1777 - acc: 0.9153 - val_loss: 0.1792 - val_acc: 0.9157\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.16959\n",
      "Epoch 25/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1760 - acc: 0.9164 - val_loss: 0.1740 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.16959\n",
      "Epoch 26/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1749 - acc: 0.9166 - val_loss: 0.1774 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.16959\n",
      "Epoch 27/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.1732 - acc: 0.9176 - val_loss: 0.1714 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.16959\n",
      "Epoch 28/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1728 - acc: 0.9178 - val_loss: 0.1690 - val_acc: 0.9209\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.16959 to 0.16903, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 29/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1712 - acc: 0.9184 - val_loss: 0.1659 - val_acc: 0.9221\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.16903 to 0.16593, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 30/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1706 - acc: 0.9187 - val_loss: 0.1699 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.16593\n",
      "Epoch 31/200\n",
      "1375334/1375334 [==============================] - 36s 27us/step - loss: 0.1695 - acc: 0.9190 - val_loss: 0.1673 - val_acc: 0.9215\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.16593\n",
      "Epoch 32/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1689 - acc: 0.9195 - val_loss: 0.1664 - val_acc: 0.9218\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.16593\n",
      "Epoch 33/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1677 - acc: 0.9202 - val_loss: 0.1594 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.16593 to 0.15941, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 34/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1669 - acc: 0.9206 - val_loss: 0.1670 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.15941\n",
      "Epoch 35/200\n",
      "1375334/1375334 [==============================] - 36s 27us/step - loss: 0.1667 - acc: 0.9205 - val_loss: 0.1564 - val_acc: 0.9260\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.15941 to 0.15635, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 36/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1655 - acc: 0.9212 - val_loss: 0.1567 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.15635\n",
      "Epoch 37/200\n",
      "1375334/1375334 [==============================] - 37s 27us/step - loss: 0.1647 - acc: 0.9217 - val_loss: 0.1578 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.15635\n",
      "Epoch 38/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1638 - acc: 0.9221 - val_loss: 0.1627 - val_acc: 0.9226\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.15635\n",
      "Epoch 39/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1627 - acc: 0.9226 - val_loss: 0.1541 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.15635 to 0.15414, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 40/200\n",
      "1375334/1375334 [==============================] - 36s 27us/step - loss: 0.1626 - acc: 0.9227 - val_loss: 0.1597 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.15414\n",
      "Epoch 41/200\n",
      "1375334/1375334 [==============================] - 36s 27us/step - loss: 0.1618 - acc: 0.9229 - val_loss: 0.1573 - val_acc: 0.9260\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15414\n",
      "Epoch 42/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1613 - acc: 0.9231 - val_loss: 0.1579 - val_acc: 0.9261\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.15414\n",
      "Epoch 43/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1605 - acc: 0.9235 - val_loss: 0.1595 - val_acc: 0.9246\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.15414\n",
      "Epoch 44/200\n",
      "1375334/1375334 [==============================] - 36s 26us/step - loss: 0.1596 - acc: 0.9237 - val_loss: 0.1586 - val_acc: 0.9248\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.15414\n",
      "Time elapsed (hh:mm:ss.ms) 0:26:44.634175\n"
     ]
    }
   ],
   "source": [
    "hist_ae_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ae_ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = ae_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_asam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_ae_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_ae_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_ae_ann_2h_unisoftsigbinlosadam, './Figures/ae_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_ae_ann_2h_prob_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict(ae_ann_2h_unisoftsigbinlosadam,enc_test_x_asam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_asam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 02:29:52 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429792/429792 [==============================] - 42s 99us/step - loss: 0.5667 - acc: 0.6880\n",
      "Epoch 2/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.4398 - acc: 0.7789\n",
      "Epoch 3/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.3854 - acc: 0.8117\n",
      "Epoch 4/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.3527 - acc: 0.8305\n",
      "Epoch 5/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.3335 - acc: 0.8405\n",
      "Epoch 6/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.3197 - acc: 0.8467\n",
      "Epoch 7/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.3077 - acc: 0.8532\n",
      "Epoch 8/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2980 - acc: 0.8584\n",
      "Epoch 9/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2911 - acc: 0.8613\n",
      "Epoch 10/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2860 - acc: 0.8645\n",
      "Epoch 11/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2802 - acc: 0.8677\n",
      "Epoch 12/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2754 - acc: 0.8700\n",
      "Epoch 13/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2714 - acc: 0.8722\n",
      "Epoch 14/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2674 - acc: 0.8742\n",
      "Epoch 15/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2648 - acc: 0.8750\n",
      "Epoch 16/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2617 - acc: 0.8769\n",
      "Epoch 17/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2578 - acc: 0.8794\n",
      "Epoch 18/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2545 - acc: 0.8808\n",
      "Epoch 19/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2520 - acc: 0.8816\n",
      "Epoch 20/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2491 - acc: 0.8832\n",
      "Epoch 21/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2475 - acc: 0.8840\n",
      "Epoch 22/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2448 - acc: 0.8850\n",
      "Epoch 23/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2419 - acc: 0.8866\n",
      "Epoch 24/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2408 - acc: 0.8871\n",
      "Epoch 25/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2393 - acc: 0.8875\n",
      "Epoch 26/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2374 - acc: 0.8889\n",
      "Epoch 27/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2358 - acc: 0.8892\n",
      "Epoch 28/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2340 - acc: 0.8901\n",
      "Epoch 29/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2333 - acc: 0.8902\n",
      "Epoch 30/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2311 - acc: 0.8919\n",
      "Epoch 31/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2289 - acc: 0.8931\n",
      "Epoch 32/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2282 - acc: 0.8925\n",
      "Epoch 33/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2267 - acc: 0.8937\n",
      "Epoch 34/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2268 - acc: 0.8940\n",
      "Epoch 35/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2257 - acc: 0.8943\n",
      "Epoch 36/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2238 - acc: 0.8956\n",
      "Epoch 37/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2221 - acc: 0.8958\n",
      "Epoch 38/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2223 - acc: 0.8959\n",
      "Epoch 39/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2217 - acc: 0.8965\n",
      "Epoch 40/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2204 - acc: 0.8970\n",
      "Epoch 41/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2206 - acc: 0.8966\n",
      "Epoch 42/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2192 - acc: 0.8971\n",
      "Epoch 43/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2184 - acc: 0.8978\n",
      "Epoch 44/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2178 - acc: 0.8984\n",
      "Epoch 45/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2175 - acc: 0.8982\n",
      "Epoch 46/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2166 - acc: 0.8987\n",
      "Epoch 47/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2164 - acc: 0.8988\n",
      "Epoch 48/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2156 - acc: 0.8992\n",
      "Epoch 49/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2151 - acc: 0.8992\n",
      "Epoch 50/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2143 - acc: 0.8998\n",
      "Epoch 51/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2136 - acc: 0.9002\n",
      "Epoch 52/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2130 - acc: 0.9002\n",
      "Epoch 53/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2130 - acc: 0.8999\n",
      "Epoch 54/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2118 - acc: 0.9011\n",
      "Epoch 55/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2126 - acc: 0.9003\n",
      "Epoch 56/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2111 - acc: 0.9008\n",
      "Epoch 57/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2114 - acc: 0.9009\n",
      "Epoch 58/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2106 - acc: 0.9016\n",
      "Epoch 59/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2104 - acc: 0.9017\n",
      "Epoch 60/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2103 - acc: 0.9013\n",
      "Epoch 61/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2093 - acc: 0.9023\n",
      "Epoch 62/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2088 - acc: 0.9023\n",
      "Epoch 63/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2085 - acc: 0.9025\n",
      "Epoch 64/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2084 - acc: 0.9027\n",
      "Epoch 65/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2087 - acc: 0.9025\n",
      "Epoch 66/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2084 - acc: 0.9026\n",
      "Epoch 67/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2075 - acc: 0.9029\n",
      "Epoch 68/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2070 - acc: 0.9030\n",
      "Epoch 69/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2067 - acc: 0.9033\n",
      "Epoch 70/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2064 - acc: 0.9032\n",
      "Epoch 71/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2063 - acc: 0.9035\n",
      "Epoch 72/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2056 - acc: 0.9039\n",
      "Epoch 73/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2059 - acc: 0.9035\n",
      "Epoch 74/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2059 - acc: 0.9034\n",
      "Epoch 75/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2052 - acc: 0.9038\n",
      "Epoch 76/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2047 - acc: 0.9043\n",
      "Epoch 77/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2053 - acc: 0.9042\n",
      "Epoch 78/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2041 - acc: 0.9047\n",
      "Epoch 79/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2049 - acc: 0.9039\n",
      "Epoch 80/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2038 - acc: 0.9046\n",
      "Epoch 81/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2040 - acc: 0.9049\n",
      "Epoch 82/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2043 - acc: 0.9046\n",
      "Epoch 83/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2041 - acc: 0.9050\n",
      "Epoch 84/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2040 - acc: 0.9049\n",
      "Epoch 85/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2035 - acc: 0.9047\n",
      "Epoch 86/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2029 - acc: 0.9054\n",
      "Epoch 87/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2038 - acc: 0.9044\n",
      "Epoch 88/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2027 - acc: 0.9054\n",
      "Epoch 89/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2024 - acc: 0.9056\n",
      "Epoch 90/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2026 - acc: 0.9052\n",
      "Epoch 91/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2022 - acc: 0.9053\n",
      "Epoch 92/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2013 - acc: 0.9059\n",
      "Epoch 93/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2023 - acc: 0.9057\n",
      "Epoch 94/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2014 - acc: 0.9059\n",
      "Epoch 95/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2024 - acc: 0.9053\n",
      "Epoch 96/100\n",
      "429792/429792 [==============================] - 42s 98us/step - loss: 0.2009 - acc: 0.9059\n",
      "Epoch 97/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2012 - acc: 0.9061\n",
      "Epoch 98/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2014 - acc: 0.9055\n",
      "Epoch 99/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2018 - acc: 0.9059\n",
      "Epoch 100/100\n",
      "429792/429792 [==============================] - 42s 97us/step - loss: 0.2017 - acc: 0.9057\n",
      "107449/107449 [==============================] - 2s 16us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.5634 - acc: 0.6926\n",
      "Epoch 2/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.4360 - acc: 0.7822\n",
      "Epoch 3/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3837 - acc: 0.8130\n",
      "Epoch 4/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3556 - acc: 0.8277\n",
      "Epoch 5/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3344 - acc: 0.8396\n",
      "Epoch 6/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3188 - acc: 0.8476\n",
      "Epoch 7/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3070 - acc: 0.8531\n",
      "Epoch 8/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2980 - acc: 0.8580\n",
      "Epoch 9/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2898 - acc: 0.8617\n",
      "Epoch 10/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2832 - acc: 0.8652\n",
      "Epoch 11/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2776 - acc: 0.8681\n",
      "Epoch 12/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2717 - acc: 0.8711\n",
      "Epoch 13/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2676 - acc: 0.8727\n",
      "Epoch 14/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2642 - acc: 0.8746\n",
      "Epoch 15/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2605 - acc: 0.8766\n",
      "Epoch 16/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2576 - acc: 0.8779\n",
      "Epoch 17/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2546 - acc: 0.8799\n",
      "Epoch 18/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2526 - acc: 0.8806\n",
      "Epoch 19/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2501 - acc: 0.8817\n",
      "Epoch 20/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2480 - acc: 0.8832\n",
      "Epoch 21/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2462 - acc: 0.8833\n",
      "Epoch 22/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2452 - acc: 0.8839\n",
      "Epoch 23/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2435 - acc: 0.8848\n",
      "Epoch 24/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2411 - acc: 0.8858\n",
      "Epoch 25/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2396 - acc: 0.8864\n",
      "Epoch 26/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2384 - acc: 0.8869\n",
      "Epoch 27/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2371 - acc: 0.8881\n",
      "Epoch 28/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2359 - acc: 0.8884\n",
      "Epoch 29/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2340 - acc: 0.8892\n",
      "Epoch 30/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2332 - acc: 0.8896\n",
      "Epoch 31/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2315 - acc: 0.8906\n",
      "Epoch 32/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2312 - acc: 0.8903\n",
      "Epoch 33/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2297 - acc: 0.8914\n",
      "Epoch 34/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2285 - acc: 0.8920\n",
      "Epoch 35/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2274 - acc: 0.8923\n",
      "Epoch 36/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2269 - acc: 0.8927\n",
      "Epoch 37/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2258 - acc: 0.8933\n",
      "Epoch 38/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2251 - acc: 0.8932\n",
      "Epoch 39/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2251 - acc: 0.8937\n",
      "Epoch 40/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2234 - acc: 0.8944\n",
      "Epoch 41/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2233 - acc: 0.8939\n",
      "Epoch 42/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2220 - acc: 0.8953\n",
      "Epoch 43/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2216 - acc: 0.8951\n",
      "Epoch 44/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2211 - acc: 0.8949\n",
      "Epoch 45/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2210 - acc: 0.8955\n",
      "Epoch 46/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2198 - acc: 0.8960\n",
      "Epoch 47/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2189 - acc: 0.8963\n",
      "Epoch 48/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2184 - acc: 0.8964\n",
      "Epoch 49/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2187 - acc: 0.8966\n",
      "Epoch 50/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2179 - acc: 0.8969\n",
      "Epoch 51/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2172 - acc: 0.8969\n",
      "Epoch 52/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2172 - acc: 0.8967\n",
      "Epoch 53/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2172 - acc: 0.8969\n",
      "Epoch 54/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2157 - acc: 0.8978\n",
      "Epoch 55/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2156 - acc: 0.8979\n",
      "Epoch 56/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2159 - acc: 0.8975\n",
      "Epoch 57/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2150 - acc: 0.8982\n",
      "Epoch 58/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2152 - acc: 0.8981\n",
      "Epoch 59/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2138 - acc: 0.8986\n",
      "Epoch 60/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2136 - acc: 0.8993\n",
      "Epoch 61/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2131 - acc: 0.8995\n",
      "Epoch 62/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2125 - acc: 0.8993\n",
      "Epoch 63/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2122 - acc: 0.8998\n",
      "Epoch 64/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2117 - acc: 0.8999\n",
      "Epoch 65/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2119 - acc: 0.8996\n",
      "Epoch 66/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2117 - acc: 0.9002\n",
      "Epoch 67/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2115 - acc: 0.9000\n",
      "Epoch 68/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2106 - acc: 0.9006\n",
      "Epoch 69/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2112 - acc: 0.9007\n",
      "Epoch 70/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2098 - acc: 0.9015\n",
      "Epoch 71/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2098 - acc: 0.9012\n",
      "Epoch 72/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2100 - acc: 0.9006\n",
      "Epoch 73/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2099 - acc: 0.9013\n",
      "Epoch 74/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2099 - acc: 0.9010\n",
      "Epoch 75/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2098 - acc: 0.9010\n",
      "Epoch 76/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2088 - acc: 0.9013\n",
      "Epoch 77/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2083 - acc: 0.9018\n",
      "Epoch 78/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2086 - acc: 0.9011\n",
      "Epoch 79/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2077 - acc: 0.9021\n",
      "Epoch 80/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2077 - acc: 0.9025\n",
      "Epoch 81/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2070 - acc: 0.9024\n",
      "Epoch 82/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2073 - acc: 0.9027\n",
      "Epoch 83/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2070 - acc: 0.9031\n",
      "Epoch 84/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2069 - acc: 0.9023\n",
      "Epoch 85/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2064 - acc: 0.9030\n",
      "Epoch 86/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2060 - acc: 0.9028\n",
      "Epoch 87/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2065 - acc: 0.9026\n",
      "Epoch 88/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2065 - acc: 0.9032\n",
      "Epoch 89/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2059 - acc: 0.9031\n",
      "Epoch 90/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2050 - acc: 0.9032\n",
      "Epoch 91/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2057 - acc: 0.9034\n",
      "Epoch 92/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2049 - acc: 0.9034\n",
      "Epoch 93/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2048 - acc: 0.9040\n",
      "Epoch 94/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2051 - acc: 0.9031\n",
      "Epoch 95/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2052 - acc: 0.9031\n",
      "Epoch 96/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2047 - acc: 0.9036\n",
      "Epoch 97/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2044 - acc: 0.9039\n",
      "Epoch 98/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2041 - acc: 0.9041\n",
      "Epoch 99/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2047 - acc: 0.9041\n",
      "Epoch 100/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2045 - acc: 0.9035\n",
      "107448/107448 [==============================] - 2s 17us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.5695 - acc: 0.6878\n",
      "Epoch 2/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.4501 - acc: 0.7748\n",
      "Epoch 3/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3992 - acc: 0.8052\n",
      "Epoch 4/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3702 - acc: 0.8223\n",
      "Epoch 5/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3521 - acc: 0.8334\n",
      "Epoch 6/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3381 - acc: 0.8408\n",
      "Epoch 7/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3278 - acc: 0.8465\n",
      "Epoch 8/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3177 - acc: 0.8516\n",
      "Epoch 9/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3107 - acc: 0.8549\n",
      "Epoch 10/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3043 - acc: 0.8574\n",
      "Epoch 11/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.3001 - acc: 0.8599\n",
      "Epoch 12/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2954 - acc: 0.8626\n",
      "Epoch 13/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2917 - acc: 0.8644\n",
      "Epoch 14/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2875 - acc: 0.8662\n",
      "Epoch 15/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2841 - acc: 0.8677\n",
      "Epoch 16/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2802 - acc: 0.8701\n",
      "Epoch 17/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2775 - acc: 0.8705\n",
      "Epoch 18/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2753 - acc: 0.8721\n",
      "Epoch 19/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2729 - acc: 0.8729\n",
      "Epoch 20/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2704 - acc: 0.8748\n",
      "Epoch 21/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.2686 - acc: 0.8753\n",
      "Epoch 22/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2662 - acc: 0.8766\n",
      "Epoch 23/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2642 - acc: 0.8775\n",
      "Epoch 24/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2616 - acc: 0.8791\n",
      "Epoch 25/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2605 - acc: 0.8796\n",
      "Epoch 26/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2587 - acc: 0.8805\n",
      "Epoch 27/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2573 - acc: 0.8813\n",
      "Epoch 28/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2559 - acc: 0.8821\n",
      "Epoch 29/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2543 - acc: 0.8823\n",
      "Epoch 30/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2531 - acc: 0.8839\n",
      "Epoch 31/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2513 - acc: 0.8847\n",
      "Epoch 32/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2507 - acc: 0.8849\n",
      "Epoch 33/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2488 - acc: 0.8856\n",
      "Epoch 34/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2479 - acc: 0.8856\n",
      "Epoch 35/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2460 - acc: 0.8863\n",
      "Epoch 36/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2452 - acc: 0.8868\n",
      "Epoch 37/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2440 - acc: 0.8879\n",
      "Epoch 38/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2424 - acc: 0.8893\n",
      "Epoch 39/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2423 - acc: 0.8889\n",
      "Epoch 40/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2412 - acc: 0.8890\n",
      "Epoch 41/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2398 - acc: 0.8899\n",
      "Epoch 42/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2393 - acc: 0.8903\n",
      "Epoch 43/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2382 - acc: 0.8911\n",
      "Epoch 44/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2373 - acc: 0.8912\n",
      "Epoch 45/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2370 - acc: 0.8916\n",
      "Epoch 46/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2362 - acc: 0.8914\n",
      "Epoch 47/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2358 - acc: 0.8920\n",
      "Epoch 48/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2349 - acc: 0.8921\n",
      "Epoch 49/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2348 - acc: 0.8923\n",
      "Epoch 50/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2338 - acc: 0.8931\n",
      "Epoch 51/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2342 - acc: 0.8923\n",
      "Epoch 52/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2334 - acc: 0.8929\n",
      "Epoch 53/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.2327 - acc: 0.8934\n",
      "Epoch 54/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2319 - acc: 0.8935\n",
      "Epoch 55/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2318 - acc: 0.8937\n",
      "Epoch 56/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2308 - acc: 0.8934\n",
      "Epoch 57/100\n",
      "429793/429793 [==============================] - 42s 97us/step - loss: 0.2316 - acc: 0.8934\n",
      "Epoch 58/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2310 - acc: 0.8939\n",
      "Epoch 59/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2302 - acc: 0.8941\n",
      "Epoch 60/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2298 - acc: 0.8950\n",
      "Epoch 61/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2299 - acc: 0.8941\n",
      "Epoch 62/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2293 - acc: 0.8944\n",
      "Epoch 63/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2296 - acc: 0.8941\n",
      "Epoch 64/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2297 - acc: 0.8941\n",
      "Epoch 65/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2278 - acc: 0.8949\n",
      "Epoch 66/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2282 - acc: 0.8954\n",
      "Epoch 67/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2277 - acc: 0.8950\n",
      "Epoch 68/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2282 - acc: 0.8948\n",
      "Epoch 69/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2278 - acc: 0.8953\n",
      "Epoch 70/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2272 - acc: 0.8953\n",
      "Epoch 71/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2266 - acc: 0.8959\n",
      "Epoch 72/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2274 - acc: 0.8961\n",
      "Epoch 73/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2267 - acc: 0.8962\n",
      "Epoch 74/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2271 - acc: 0.8958\n",
      "Epoch 75/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2264 - acc: 0.8950\n",
      "Epoch 76/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.2257 - acc: 0.8963\n",
      "Epoch 77/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2267 - acc: 0.8958\n",
      "Epoch 78/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2253 - acc: 0.8962\n",
      "Epoch 79/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2255 - acc: 0.8961\n",
      "Epoch 80/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2258 - acc: 0.8960\n",
      "Epoch 81/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2262 - acc: 0.8957\n",
      "Epoch 82/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2254 - acc: 0.8964\n",
      "Epoch 83/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2249 - acc: 0.8965\n",
      "Epoch 84/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2245 - acc: 0.8973\n",
      "Epoch 85/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2251 - acc: 0.8967\n",
      "Epoch 86/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2252 - acc: 0.8962\n",
      "Epoch 87/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2244 - acc: 0.8969\n",
      "Epoch 88/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2244 - acc: 0.8968\n",
      "Epoch 89/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2233 - acc: 0.8972\n",
      "Epoch 90/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2240 - acc: 0.8968\n",
      "Epoch 91/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2233 - acc: 0.8972\n",
      "Epoch 92/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2246 - acc: 0.8963\n",
      "Epoch 93/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.2234 - acc: 0.8973\n",
      "Epoch 94/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2234 - acc: 0.8974\n",
      "Epoch 95/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2227 - acc: 0.8973\n",
      "Epoch 96/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2227 - acc: 0.8974\n",
      "Epoch 97/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2230 - acc: 0.8976\n",
      "Epoch 98/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2238 - acc: 0.8971\n",
      "Epoch 99/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2225 - acc: 0.8974\n",
      "Epoch 100/100\n",
      "429793/429793 [==============================] - 42s 98us/step - loss: 0.2232 - acc: 0.8974\n",
      "107448/107448 [==============================] - 2s 18us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.5723 - acc: 0.6838\n",
      "Epoch 2/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.4449 - acc: 0.7760\n",
      "Epoch 3/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.3901 - acc: 0.8098\n",
      "Epoch 4/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.3607 - acc: 0.8265\n",
      "Epoch 5/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.3423 - acc: 0.8351\n",
      "Epoch 6/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.3286 - acc: 0.8415\n",
      "Epoch 7/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.3206 - acc: 0.8458\n",
      "Epoch 8/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.3119 - acc: 0.8488\n",
      "Epoch 9/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.3039 - acc: 0.8535\n",
      "Epoch 10/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2963 - acc: 0.8569\n",
      "Epoch 11/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2907 - acc: 0.8602\n",
      "Epoch 12/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2853 - acc: 0.8626\n",
      "Epoch 13/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2803 - acc: 0.8647\n",
      "Epoch 14/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.2758 - acc: 0.8669\n",
      "Epoch 15/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2725 - acc: 0.8688\n",
      "Epoch 16/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2669 - acc: 0.8717\n",
      "Epoch 17/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2647 - acc: 0.8726\n",
      "Epoch 18/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2609 - acc: 0.8746\n",
      "Epoch 19/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2591 - acc: 0.8754\n",
      "Epoch 20/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2573 - acc: 0.8768\n",
      "Epoch 21/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2550 - acc: 0.8772\n",
      "Epoch 22/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2533 - acc: 0.8781\n",
      "Epoch 23/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2507 - acc: 0.8793\n",
      "Epoch 24/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2502 - acc: 0.8800\n",
      "Epoch 25/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2483 - acc: 0.8803\n",
      "Epoch 26/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2468 - acc: 0.8816\n",
      "Epoch 27/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2451 - acc: 0.8818\n",
      "Epoch 28/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2441 - acc: 0.8826\n",
      "Epoch 29/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2440 - acc: 0.8830\n",
      "Epoch 30/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2420 - acc: 0.8839\n",
      "Epoch 31/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2412 - acc: 0.8846\n",
      "Epoch 32/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2396 - acc: 0.8850\n",
      "Epoch 33/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2386 - acc: 0.8853\n",
      "Epoch 34/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2374 - acc: 0.8868\n",
      "Epoch 35/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2357 - acc: 0.8874\n",
      "Epoch 36/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2357 - acc: 0.8873\n",
      "Epoch 37/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2347 - acc: 0.8875\n",
      "Epoch 38/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2337 - acc: 0.8885\n",
      "Epoch 39/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2322 - acc: 0.8893\n",
      "Epoch 40/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2312 - acc: 0.8895\n",
      "Epoch 41/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2302 - acc: 0.8899\n",
      "Epoch 42/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2289 - acc: 0.8909\n",
      "Epoch 43/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2285 - acc: 0.8908\n",
      "Epoch 44/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2284 - acc: 0.8908\n",
      "Epoch 45/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2278 - acc: 0.8907\n",
      "Epoch 46/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2261 - acc: 0.8921\n",
      "Epoch 47/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2262 - acc: 0.8924\n",
      "Epoch 48/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2248 - acc: 0.8934\n",
      "Epoch 49/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2253 - acc: 0.8927\n",
      "Epoch 50/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2244 - acc: 0.8935\n",
      "Epoch 51/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2226 - acc: 0.8941\n",
      "Epoch 52/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.2219 - acc: 0.8943\n",
      "Epoch 53/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2222 - acc: 0.8943\n",
      "Epoch 54/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2210 - acc: 0.8943\n",
      "Epoch 55/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.2204 - acc: 0.8950\n",
      "Epoch 56/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2204 - acc: 0.8952\n",
      "Epoch 57/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2206 - acc: 0.8948\n",
      "Epoch 58/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2193 - acc: 0.8950\n",
      "Epoch 59/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2202 - acc: 0.8953\n",
      "Epoch 60/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2189 - acc: 0.8957\n",
      "Epoch 61/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2192 - acc: 0.8960\n",
      "Epoch 62/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2181 - acc: 0.8966\n",
      "Epoch 63/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2175 - acc: 0.8963\n",
      "Epoch 64/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2181 - acc: 0.8960\n",
      "Epoch 65/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2166 - acc: 0.8969\n",
      "Epoch 66/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2167 - acc: 0.8968\n",
      "Epoch 67/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2173 - acc: 0.8962\n",
      "Epoch 68/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2157 - acc: 0.8972\n",
      "Epoch 69/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2160 - acc: 0.8976\n",
      "Epoch 70/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2155 - acc: 0.8974\n",
      "Epoch 71/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2154 - acc: 0.8970\n",
      "Epoch 72/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2153 - acc: 0.8973\n",
      "Epoch 73/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2143 - acc: 0.8983\n",
      "Epoch 74/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2153 - acc: 0.8979\n",
      "Epoch 75/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2156 - acc: 0.8977\n",
      "Epoch 76/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2135 - acc: 0.8988\n",
      "Epoch 77/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2151 - acc: 0.8976\n",
      "Epoch 78/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2144 - acc: 0.8980\n",
      "Epoch 79/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2138 - acc: 0.8978\n",
      "Epoch 80/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2141 - acc: 0.8977\n",
      "Epoch 81/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2126 - acc: 0.8986\n",
      "Epoch 82/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2133 - acc: 0.8987\n",
      "Epoch 83/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2138 - acc: 0.8981\n",
      "Epoch 84/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2128 - acc: 0.8985\n",
      "Epoch 85/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2118 - acc: 0.8994\n",
      "Epoch 86/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2117 - acc: 0.8992\n",
      "Epoch 87/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2126 - acc: 0.8990\n",
      "Epoch 88/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2125 - acc: 0.8992\n",
      "Epoch 89/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2117 - acc: 0.8990\n",
      "Epoch 90/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2115 - acc: 0.8992\n",
      "Epoch 91/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2120 - acc: 0.9000\n",
      "Epoch 92/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2122 - acc: 0.8995\n",
      "Epoch 93/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2115 - acc: 0.8995\n",
      "Epoch 94/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2120 - acc: 0.8995\n",
      "Epoch 95/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2121 - acc: 0.8993\n",
      "Epoch 96/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2103 - acc: 0.9008\n",
      "Epoch 97/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2112 - acc: 0.9001\n",
      "Epoch 98/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2104 - acc: 0.9002\n",
      "Epoch 99/100\n",
      "429793/429793 [==============================] - 42s 99us/step - loss: 0.2099 - acc: 0.8998\n",
      "Epoch 100/100\n",
      "429793/429793 [==============================] - 43s 99us/step - loss: 0.2107 - acc: 0.9004\n",
      "107448/107448 [==============================] - 2s 18us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.5676 - acc: 0.6904\n",
      "Epoch 2/100\n",
      "429793/429793 [==============================] - 43s 101us/step - loss: 0.4396 - acc: 0.7803\n",
      "Epoch 3/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.3866 - acc: 0.8114\n",
      "Epoch 4/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.3580 - acc: 0.8273\n",
      "Epoch 5/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.3369 - acc: 0.8383\n",
      "Epoch 6/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.3241 - acc: 0.8442\n",
      "Epoch 7/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.3138 - acc: 0.8493\n",
      "Epoch 8/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.3051 - acc: 0.8534\n",
      "Epoch 9/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2975 - acc: 0.8581\n",
      "Epoch 10/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2903 - acc: 0.8622\n",
      "Epoch 11/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2857 - acc: 0.8636\n",
      "Epoch 12/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2804 - acc: 0.8673\n",
      "Epoch 13/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2759 - acc: 0.8691\n",
      "Epoch 14/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2724 - acc: 0.8713\n",
      "Epoch 15/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2682 - acc: 0.8728\n",
      "Epoch 16/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2646 - acc: 0.8745\n",
      "Epoch 17/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2612 - acc: 0.8770\n",
      "Epoch 18/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2589 - acc: 0.8768\n",
      "Epoch 19/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2559 - acc: 0.8788\n",
      "Epoch 20/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2533 - acc: 0.8805\n",
      "Epoch 21/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2511 - acc: 0.8817\n",
      "Epoch 22/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2496 - acc: 0.8830\n",
      "Epoch 23/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2470 - acc: 0.8840\n",
      "Epoch 24/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2458 - acc: 0.8841\n",
      "Epoch 25/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2432 - acc: 0.8860\n",
      "Epoch 26/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2423 - acc: 0.8861\n",
      "Epoch 27/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2404 - acc: 0.8863\n",
      "Epoch 28/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2396 - acc: 0.8868\n",
      "Epoch 29/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2373 - acc: 0.8889\n",
      "Epoch 30/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2359 - acc: 0.8895\n",
      "Epoch 31/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2348 - acc: 0.8901\n",
      "Epoch 32/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2336 - acc: 0.8904\n",
      "Epoch 33/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2321 - acc: 0.8910\n",
      "Epoch 34/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2317 - acc: 0.8915\n",
      "Epoch 35/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2303 - acc: 0.8922\n",
      "Epoch 36/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2289 - acc: 0.8928\n",
      "Epoch 37/100\n",
      "429793/429793 [==============================] - 43s 101us/step - loss: 0.2278 - acc: 0.8939\n",
      "Epoch 38/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2263 - acc: 0.8943\n",
      "Epoch 39/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2259 - acc: 0.8946\n",
      "Epoch 40/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2252 - acc: 0.8951\n",
      "Epoch 41/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2242 - acc: 0.8951\n",
      "Epoch 42/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2236 - acc: 0.8950\n",
      "Epoch 43/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2229 - acc: 0.8960\n",
      "Epoch 44/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2225 - acc: 0.8956\n",
      "Epoch 45/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2219 - acc: 0.8962\n",
      "Epoch 46/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2210 - acc: 0.8965\n",
      "Epoch 47/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2196 - acc: 0.8975\n",
      "Epoch 48/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2182 - acc: 0.8980\n",
      "Epoch 49/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2189 - acc: 0.8970\n",
      "Epoch 50/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2187 - acc: 0.8972\n",
      "Epoch 51/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2185 - acc: 0.8978\n",
      "Epoch 52/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2173 - acc: 0.8985\n",
      "Epoch 53/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2176 - acc: 0.8979\n",
      "Epoch 54/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2166 - acc: 0.8992\n",
      "Epoch 55/100\n",
      "429793/429793 [==============================] - 43s 101us/step - loss: 0.2155 - acc: 0.8991\n",
      "Epoch 56/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2156 - acc: 0.8991\n",
      "Epoch 57/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2152 - acc: 0.8993\n",
      "Epoch 58/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2155 - acc: 0.8991\n",
      "Epoch 59/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2152 - acc: 0.8996\n",
      "Epoch 60/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2140 - acc: 0.8995\n",
      "Epoch 61/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2146 - acc: 0.8997\n",
      "Epoch 62/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2131 - acc: 0.9008\n",
      "Epoch 63/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2125 - acc: 0.9006\n",
      "Epoch 64/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2127 - acc: 0.9007\n",
      "Epoch 65/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2120 - acc: 0.9013\n",
      "Epoch 66/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2117 - acc: 0.9011\n",
      "Epoch 67/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2121 - acc: 0.9015\n",
      "Epoch 68/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2113 - acc: 0.9020\n",
      "Epoch 69/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2106 - acc: 0.9021\n",
      "Epoch 70/100\n",
      "429793/429793 [==============================] - 43s 101us/step - loss: 0.2102 - acc: 0.9024\n",
      "Epoch 71/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2100 - acc: 0.9022\n",
      "Epoch 72/100\n",
      "429793/429793 [==============================] - 43s 101us/step - loss: 0.2099 - acc: 0.9020\n",
      "Epoch 73/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2094 - acc: 0.9025\n",
      "Epoch 74/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2088 - acc: 0.9026\n",
      "Epoch 75/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2088 - acc: 0.9030\n",
      "Epoch 76/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2082 - acc: 0.9033\n",
      "Epoch 77/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2082 - acc: 0.9027\n",
      "Epoch 78/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2081 - acc: 0.9027\n",
      "Epoch 79/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2082 - acc: 0.9033\n",
      "Epoch 80/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2070 - acc: 0.9041\n",
      "Epoch 81/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2084 - acc: 0.9034\n",
      "Epoch 82/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2071 - acc: 0.9044\n",
      "Epoch 83/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2074 - acc: 0.9035\n",
      "Epoch 84/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2066 - acc: 0.9039\n",
      "Epoch 85/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2060 - acc: 0.9035\n",
      "Epoch 86/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2070 - acc: 0.9038\n",
      "Epoch 87/100\n",
      "429793/429793 [==============================] - 43s 101us/step - loss: 0.2057 - acc: 0.9041\n",
      "Epoch 88/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2062 - acc: 0.9045\n",
      "Epoch 89/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2060 - acc: 0.9044\n",
      "Epoch 90/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2060 - acc: 0.9042\n",
      "Epoch 91/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2047 - acc: 0.9046\n",
      "Epoch 92/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2050 - acc: 0.9043\n",
      "Epoch 93/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2044 - acc: 0.9047\n",
      "Epoch 94/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2045 - acc: 0.9046\n",
      "Epoch 95/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2050 - acc: 0.9049\n",
      "Epoch 96/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2048 - acc: 0.9041\n",
      "Epoch 97/100\n",
      "429793/429793 [==============================] - 43s 101us/step - loss: 0.2042 - acc: 0.9046\n",
      "Epoch 98/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2046 - acc: 0.9047\n",
      "Epoch 99/100\n",
      "429793/429793 [==============================] - 43s 101us/step - loss: 0.2034 - acc: 0.9051\n",
      "Epoch 100/100\n",
      "429793/429793 [==============================] - 43s 100us/step - loss: 0.2036 - acc: 0.9054\n",
      "107448/107448 [==============================] - 2s 19us/step\n",
      "Time elapsed (hh:mm:ss.ms) 5:53:01.812106\n",
      "Overall accuracy of Neural Network model: 0.9045046822561942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 353.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8984    0.9119    0.9051    268256\n",
      "           1     0.9108    0.8971    0.9039    268985\n",
      "\n",
      "    accuracy                         0.9045    537241\n",
      "   macro avg     0.9046    0.9045    0.9045    537241\n",
      "weighted avg     0.9046    0.9045    0.9045    537241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_ae_ann_2h_prob_unisoftsigbinlosadam,pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sp_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_spsam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 08:22:57 2019\n",
      "Train on 1375334 samples, validate on 343834 samples\n",
      "Epoch 1/200\n",
      "1375334/1375334 [==============================] - 39s 28us/step - loss: 0.4439 - acc: 0.7742 - val_loss: 0.3835 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38346, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 2/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.3045 - acc: 0.8566 - val_loss: 0.2750 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38346 to 0.27499, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 3/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.2676 - acc: 0.8749 - val_loss: 0.2686 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.27499 to 0.26860, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 4/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.2470 - acc: 0.8849 - val_loss: 0.2387 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26860 to 0.23874, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 5/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.2326 - acc: 0.8911 - val_loss: 0.2291 - val_acc: 0.8959\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23874 to 0.22912, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 6/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.2207 - acc: 0.8971 - val_loss: 0.2169 - val_acc: 0.8996\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.22912 to 0.21686, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 7/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.2119 - acc: 0.9015 - val_loss: 0.2069 - val_acc: 0.9051\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.21686 to 0.20686, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 8/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.2054 - acc: 0.9045 - val_loss: 0.1995 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.20686 to 0.19947, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 9/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1996 - acc: 0.9068 - val_loss: 0.1963 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.19947 to 0.19627, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 10/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1946 - acc: 0.9095 - val_loss: 0.1964 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.19627\n",
      "Epoch 11/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1909 - acc: 0.9114 - val_loss: 0.1933 - val_acc: 0.9122\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.19627 to 0.19328, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 12/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1872 - acc: 0.9130 - val_loss: 0.1812 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.19328 to 0.18125, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 13/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1836 - acc: 0.9150 - val_loss: 0.1851 - val_acc: 0.9152\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.18125\n",
      "Epoch 14/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1805 - acc: 0.9164 - val_loss: 0.1810 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.18125 to 0.18098, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 15/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1782 - acc: 0.9176 - val_loss: 0.1779 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.18098 to 0.17789, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 16/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1759 - acc: 0.9186 - val_loss: 0.1723 - val_acc: 0.9226\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.17789 to 0.17225, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 17/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1739 - acc: 0.9196 - val_loss: 0.1669 - val_acc: 0.9248\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.17225 to 0.16689, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 18/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1714 - acc: 0.9207 - val_loss: 0.1667 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.16689 to 0.16671, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 19/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1693 - acc: 0.9217 - val_loss: 0.1650 - val_acc: 0.9247\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.16671 to 0.16496, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 20/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1677 - acc: 0.9226 - val_loss: 0.1641 - val_acc: 0.9256\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.16496 to 0.16406, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 21/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1664 - acc: 0.9233 - val_loss: 0.1604 - val_acc: 0.9285\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.16406 to 0.16042, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 22/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1648 - acc: 0.9238 - val_loss: 0.1652 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.16042\n",
      "Epoch 23/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1633 - acc: 0.9246 - val_loss: 0.1631 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.16042\n",
      "Epoch 24/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1622 - acc: 0.9252 - val_loss: 0.1594 - val_acc: 0.9285\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.16042 to 0.15940, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 25/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1610 - acc: 0.9256 - val_loss: 0.1570 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.15940 to 0.15701, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 26/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1596 - acc: 0.9263 - val_loss: 0.1581 - val_acc: 0.9285\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15701\n",
      "Epoch 27/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1584 - acc: 0.9270 - val_loss: 0.1520 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.15701 to 0.15199, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds20bal.h5\n",
      "Epoch 28/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1576 - acc: 0.9274 - val_loss: 0.1606 - val_acc: 0.9255\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15199\n",
      "Epoch 29/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1567 - acc: 0.9278 - val_loss: 0.1536 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15199\n",
      "Epoch 30/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1561 - acc: 0.9282 - val_loss: 0.1527 - val_acc: 0.9301\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15199\n",
      "Epoch 31/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1546 - acc: 0.9287 - val_loss: 0.1544 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.15199\n",
      "Epoch 32/200\n",
      "1375334/1375334 [==============================] - 38s 28us/step - loss: 0.1535 - acc: 0.9294 - val_loss: 0.1529 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.15199\n",
      "Time elapsed (hh:mm:ss.ms) 0:20:26.441675\n"
     ]
    }
   ],
   "source": [
    "hist_sp_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = sp_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_spsam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_sp_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_sp_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_sp_ann_2h_unisoftsigbinlosadam, './Figures/sp_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict(sp_ann_2h_unisoftsigbinlosadam,enc_test_x_spsam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_spsam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 08:43:24 2019\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "429792/429792 [==============================] - 45s 104us/step - loss: 0.5664 - acc: 0.6893\n",
      "Epoch 2/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.4378 - acc: 0.7853\n",
      "Epoch 3/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.3849 - acc: 0.8167\n",
      "Epoch 4/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.3571 - acc: 0.8318\n",
      "Epoch 5/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.3388 - acc: 0.8408\n",
      "Epoch 6/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.3240 - acc: 0.8496\n",
      "Epoch 7/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.3131 - acc: 0.8556\n",
      "Epoch 8/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.3036 - acc: 0.8599\n",
      "Epoch 9/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2962 - acc: 0.8639\n",
      "Epoch 10/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2891 - acc: 0.8670\n",
      "Epoch 11/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2838 - acc: 0.8702\n",
      "Epoch 12/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2776 - acc: 0.8730\n",
      "Epoch 13/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2730 - acc: 0.8755\n",
      "Epoch 14/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2696 - acc: 0.8774\n",
      "Epoch 15/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2655 - acc: 0.8791\n",
      "Epoch 16/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2619 - acc: 0.8807\n",
      "Epoch 17/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2598 - acc: 0.8812\n",
      "Epoch 18/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2578 - acc: 0.8825\n",
      "Epoch 19/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2553 - acc: 0.8834\n",
      "Epoch 20/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2532 - acc: 0.8841\n",
      "Epoch 21/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2514 - acc: 0.8851\n",
      "Epoch 22/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2495 - acc: 0.8864\n",
      "Epoch 23/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2475 - acc: 0.8870\n",
      "Epoch 24/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2465 - acc: 0.8879\n",
      "Epoch 25/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2442 - acc: 0.8889\n",
      "Epoch 26/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2428 - acc: 0.8892\n",
      "Epoch 27/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2413 - acc: 0.8900\n",
      "Epoch 28/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2389 - acc: 0.8913\n",
      "Epoch 29/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2381 - acc: 0.8920\n",
      "Epoch 30/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2375 - acc: 0.8919\n",
      "Epoch 31/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2361 - acc: 0.8927\n",
      "Epoch 32/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2357 - acc: 0.8929\n",
      "Epoch 33/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2341 - acc: 0.8937\n",
      "Epoch 34/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2325 - acc: 0.8942\n",
      "Epoch 35/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2324 - acc: 0.8943\n",
      "Epoch 36/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2313 - acc: 0.8949\n",
      "Epoch 37/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2295 - acc: 0.8956\n",
      "Epoch 38/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2288 - acc: 0.8959\n",
      "Epoch 39/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2281 - acc: 0.8960\n",
      "Epoch 40/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2276 - acc: 0.8965\n",
      "Epoch 41/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2265 - acc: 0.8966\n",
      "Epoch 42/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2253 - acc: 0.8975\n",
      "Epoch 43/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2252 - acc: 0.8976\n",
      "Epoch 44/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2242 - acc: 0.8983\n",
      "Epoch 45/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2233 - acc: 0.8985\n",
      "Epoch 46/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2229 - acc: 0.8987\n",
      "Epoch 47/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2214 - acc: 0.8994\n",
      "Epoch 48/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2216 - acc: 0.8993\n",
      "Epoch 49/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2211 - acc: 0.8994\n",
      "Epoch 50/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2200 - acc: 0.8998\n",
      "Epoch 51/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2204 - acc: 0.9000\n",
      "Epoch 52/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2187 - acc: 0.9002\n",
      "Epoch 53/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2178 - acc: 0.9010\n",
      "Epoch 54/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2171 - acc: 0.9014\n",
      "Epoch 55/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2162 - acc: 0.9016\n",
      "Epoch 56/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2169 - acc: 0.9009\n",
      "Epoch 57/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2155 - acc: 0.9019\n",
      "Epoch 58/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2149 - acc: 0.9018\n",
      "Epoch 59/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2151 - acc: 0.9019\n",
      "Epoch 60/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2150 - acc: 0.9018\n",
      "Epoch 61/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2136 - acc: 0.9023\n",
      "Epoch 62/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2135 - acc: 0.9026\n",
      "Epoch 63/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2140 - acc: 0.9028\n",
      "Epoch 64/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2129 - acc: 0.9028\n",
      "Epoch 65/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2122 - acc: 0.9034\n",
      "Epoch 66/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2120 - acc: 0.9029\n",
      "Epoch 67/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2119 - acc: 0.9033\n",
      "Epoch 68/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2112 - acc: 0.9036\n",
      "Epoch 69/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2112 - acc: 0.9039\n",
      "Epoch 70/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2105 - acc: 0.9033\n",
      "Epoch 71/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2106 - acc: 0.9033\n",
      "Epoch 72/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2105 - acc: 0.9042\n",
      "Epoch 73/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2097 - acc: 0.9041\n",
      "Epoch 74/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2092 - acc: 0.9044\n",
      "Epoch 75/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2089 - acc: 0.9050\n",
      "Epoch 76/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2082 - acc: 0.9050\n",
      "Epoch 77/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2085 - acc: 0.9049\n",
      "Epoch 78/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2083 - acc: 0.9050\n",
      "Epoch 79/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2080 - acc: 0.9047\n",
      "Epoch 80/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2086 - acc: 0.9046\n",
      "Epoch 81/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2083 - acc: 0.9051\n",
      "Epoch 82/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2083 - acc: 0.9049\n",
      "Epoch 83/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2066 - acc: 0.9059\n",
      "Epoch 84/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2074 - acc: 0.9048\n",
      "Epoch 85/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2075 - acc: 0.9044\n",
      "Epoch 86/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2076 - acc: 0.9045\n",
      "Epoch 87/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2064 - acc: 0.9054\n",
      "Epoch 88/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2071 - acc: 0.9052\n",
      "Epoch 89/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2064 - acc: 0.9054\n",
      "Epoch 90/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2063 - acc: 0.9061\n",
      "Epoch 91/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2062 - acc: 0.9054\n",
      "Epoch 92/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2056 - acc: 0.9059\n",
      "Epoch 93/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2055 - acc: 0.9058\n",
      "Epoch 94/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2052 - acc: 0.9062\n",
      "Epoch 95/100\n",
      "429792/429792 [==============================] - 44s 101us/step - loss: 0.2052 - acc: 0.9059\n",
      "Epoch 96/100\n",
      "429792/429792 [==============================] - 43s 101us/step - loss: 0.2047 - acc: 0.9061\n",
      "Epoch 97/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2048 - acc: 0.9067\n",
      "Epoch 98/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2040 - acc: 0.9067\n",
      "Epoch 99/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2038 - acc: 0.9060\n",
      "Epoch 100/100\n",
      "429792/429792 [==============================] - 44s 102us/step - loss: 0.2035 - acc: 0.9068\n",
      "107449/107449 [==============================] - 2s 21us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.5611 - acc: 0.6924\n",
      "Epoch 2/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.4351 - acc: 0.7850\n",
      "Epoch 3/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.3827 - acc: 0.8157\n",
      "Epoch 4/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.3533 - acc: 0.8326\n",
      "Epoch 5/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.3311 - acc: 0.8450\n",
      "Epoch 6/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.3155 - acc: 0.8536\n",
      "Epoch 7/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.3030 - acc: 0.8598\n",
      "Epoch 8/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2937 - acc: 0.8644\n",
      "Epoch 9/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2870 - acc: 0.8670\n",
      "Epoch 10/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2794 - acc: 0.8710\n",
      "Epoch 11/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2722 - acc: 0.8743\n",
      "Epoch 12/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2677 - acc: 0.8765\n",
      "Epoch 13/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2621 - acc: 0.8788\n",
      "Epoch 14/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2585 - acc: 0.8806\n",
      "Epoch 15/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2552 - acc: 0.8818\n",
      "Epoch 16/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2527 - acc: 0.8831\n",
      "Epoch 17/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2494 - acc: 0.8842\n",
      "Epoch 18/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2462 - acc: 0.8860\n",
      "Epoch 19/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2438 - acc: 0.8875\n",
      "Epoch 20/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2412 - acc: 0.8889\n",
      "Epoch 21/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2393 - acc: 0.8893\n",
      "Epoch 22/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2362 - acc: 0.8902\n",
      "Epoch 23/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2340 - acc: 0.8915\n",
      "Epoch 24/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2330 - acc: 0.8923\n",
      "Epoch 25/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2314 - acc: 0.8924\n",
      "Epoch 26/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2301 - acc: 0.8941\n",
      "Epoch 27/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2286 - acc: 0.8941\n",
      "Epoch 28/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2257 - acc: 0.8959\n",
      "Epoch 29/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2251 - acc: 0.8957\n",
      "Epoch 30/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2244 - acc: 0.8962\n",
      "Epoch 31/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2231 - acc: 0.8973\n",
      "Epoch 32/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2224 - acc: 0.8972\n",
      "Epoch 33/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2212 - acc: 0.8974\n",
      "Epoch 34/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2201 - acc: 0.8986\n",
      "Epoch 35/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2200 - acc: 0.8983\n",
      "Epoch 36/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2184 - acc: 0.8989\n",
      "Epoch 37/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2176 - acc: 0.8996\n",
      "Epoch 38/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2166 - acc: 0.8998\n",
      "Epoch 39/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2157 - acc: 0.9005\n",
      "Epoch 40/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2154 - acc: 0.9006\n",
      "Epoch 41/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2150 - acc: 0.9004\n",
      "Epoch 42/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2141 - acc: 0.9014\n",
      "Epoch 43/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2132 - acc: 0.9019\n",
      "Epoch 44/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2125 - acc: 0.9016\n",
      "Epoch 45/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2116 - acc: 0.9030\n",
      "Epoch 46/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2120 - acc: 0.9028\n",
      "Epoch 47/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2104 - acc: 0.9028\n",
      "Epoch 48/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2097 - acc: 0.9031\n",
      "Epoch 49/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2089 - acc: 0.9035\n",
      "Epoch 50/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2086 - acc: 0.9032\n",
      "Epoch 51/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2085 - acc: 0.9038\n",
      "Epoch 52/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2072 - acc: 0.9046\n",
      "Epoch 53/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2068 - acc: 0.9046\n",
      "Epoch 54/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2070 - acc: 0.9047\n",
      "Epoch 55/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2068 - acc: 0.9045\n",
      "Epoch 56/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2061 - acc: 0.9048\n",
      "Epoch 57/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2050 - acc: 0.9050\n",
      "Epoch 58/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2050 - acc: 0.9054\n",
      "Epoch 59/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2040 - acc: 0.9055\n",
      "Epoch 60/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2048 - acc: 0.9055\n",
      "Epoch 61/100\n",
      "429793/429793 [==============================] - 44s 104us/step - loss: 0.2030 - acc: 0.9062\n",
      "Epoch 62/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2035 - acc: 0.9065\n",
      "Epoch 63/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2026 - acc: 0.9068\n",
      "Epoch 64/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2033 - acc: 0.9064\n",
      "Epoch 65/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2026 - acc: 0.9070\n",
      "Epoch 66/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2026 - acc: 0.9070\n",
      "Epoch 67/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2017 - acc: 0.9072\n",
      "Epoch 68/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2021 - acc: 0.9069\n",
      "Epoch 69/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2012 - acc: 0.9072\n",
      "Epoch 70/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2016 - acc: 0.9072\n",
      "Epoch 71/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2016 - acc: 0.9071\n",
      "Epoch 72/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2002 - acc: 0.9074\n",
      "Epoch 73/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2017 - acc: 0.9072\n",
      "Epoch 74/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2010 - acc: 0.9072\n",
      "Epoch 75/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1997 - acc: 0.9083\n",
      "Epoch 76/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2004 - acc: 0.9078\n",
      "Epoch 77/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.2001 - acc: 0.9082\n",
      "Epoch 78/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2001 - acc: 0.9078\n",
      "Epoch 79/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1998 - acc: 0.9081\n",
      "Epoch 80/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1994 - acc: 0.9087\n",
      "Epoch 81/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1990 - acc: 0.9083\n",
      "Epoch 82/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.1982 - acc: 0.9093\n",
      "Epoch 83/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.1984 - acc: 0.9091\n",
      "Epoch 84/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.1985 - acc: 0.9087\n",
      "Epoch 85/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.1990 - acc: 0.9089\n",
      "Epoch 86/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.1972 - acc: 0.9093\n",
      "Epoch 87/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1979 - acc: 0.9091\n",
      "Epoch 88/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1979 - acc: 0.9094\n",
      "Epoch 89/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1969 - acc: 0.9096\n",
      "Epoch 90/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1972 - acc: 0.9094\n",
      "Epoch 91/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.1972 - acc: 0.9095\n",
      "Epoch 92/100\n",
      "429793/429793 [==============================] - 44s 102us/step - loss: 0.1970 - acc: 0.9099\n",
      "Epoch 93/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1970 - acc: 0.9095\n",
      "Epoch 94/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1967 - acc: 0.9100\n",
      "Epoch 95/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1962 - acc: 0.9099\n",
      "Epoch 96/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1971 - acc: 0.9092\n",
      "Epoch 97/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1960 - acc: 0.9103\n",
      "Epoch 98/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1956 - acc: 0.9102\n",
      "Epoch 99/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1965 - acc: 0.9098\n",
      "Epoch 100/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1957 - acc: 0.9103\n",
      "107448/107448 [==============================] - 2s 22us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429793/429793 [==============================] - 46s 107us/step - loss: 0.5610 - acc: 0.6926\n",
      "Epoch 2/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.4348 - acc: 0.7840\n",
      "Epoch 3/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.3796 - acc: 0.8155\n",
      "Epoch 4/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.3499 - acc: 0.8307\n",
      "Epoch 5/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.3301 - acc: 0.8424\n",
      "Epoch 6/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.3152 - acc: 0.8503\n",
      "Epoch 7/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.3043 - acc: 0.8556\n",
      "Epoch 8/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2954 - acc: 0.8600\n",
      "Epoch 9/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2874 - acc: 0.8647\n",
      "Epoch 10/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2819 - acc: 0.8667\n",
      "Epoch 11/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2751 - acc: 0.8705\n",
      "Epoch 12/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2692 - acc: 0.8734\n",
      "Epoch 13/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2645 - acc: 0.8756\n",
      "Epoch 14/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2608 - acc: 0.8776\n",
      "Epoch 15/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2570 - acc: 0.8794\n",
      "Epoch 16/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2542 - acc: 0.8807\n",
      "Epoch 17/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2515 - acc: 0.8817\n",
      "Epoch 18/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2482 - acc: 0.8838\n",
      "Epoch 19/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2458 - acc: 0.8851\n",
      "Epoch 20/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2424 - acc: 0.8868\n",
      "Epoch 21/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2418 - acc: 0.8872\n",
      "Epoch 22/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2395 - acc: 0.8884\n",
      "Epoch 23/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2374 - acc: 0.8892\n",
      "Epoch 24/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2349 - acc: 0.8900\n",
      "Epoch 25/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2341 - acc: 0.8909\n",
      "Epoch 26/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2311 - acc: 0.8922\n",
      "Epoch 27/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2313 - acc: 0.8927\n",
      "Epoch 28/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2298 - acc: 0.8928\n",
      "Epoch 29/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2286 - acc: 0.8937\n",
      "Epoch 30/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2276 - acc: 0.8938\n",
      "Epoch 31/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2258 - acc: 0.8947\n",
      "Epoch 32/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2252 - acc: 0.8949\n",
      "Epoch 33/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2242 - acc: 0.8954\n",
      "Epoch 34/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2231 - acc: 0.8964\n",
      "Epoch 35/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2226 - acc: 0.8958\n",
      "Epoch 36/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2213 - acc: 0.8968\n",
      "Epoch 37/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2200 - acc: 0.8971\n",
      "Epoch 38/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2186 - acc: 0.8984\n",
      "Epoch 39/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2193 - acc: 0.8979\n",
      "Epoch 40/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2182 - acc: 0.8982\n",
      "Epoch 41/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2166 - acc: 0.8992\n",
      "Epoch 42/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2168 - acc: 0.8992\n",
      "Epoch 43/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2161 - acc: 0.9001\n",
      "Epoch 44/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2154 - acc: 0.8998\n",
      "Epoch 45/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2153 - acc: 0.8998\n",
      "Epoch 46/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2151 - acc: 0.9001\n",
      "Epoch 47/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2146 - acc: 0.9002\n",
      "Epoch 48/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2135 - acc: 0.9006\n",
      "Epoch 49/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2134 - acc: 0.9010\n",
      "Epoch 50/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2125 - acc: 0.9011\n",
      "Epoch 51/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2119 - acc: 0.9012\n",
      "Epoch 52/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2116 - acc: 0.9011\n",
      "Epoch 53/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2108 - acc: 0.9018\n",
      "Epoch 54/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2109 - acc: 0.9018\n",
      "Epoch 55/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2103 - acc: 0.9018\n",
      "Epoch 56/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2092 - acc: 0.9026\n",
      "Epoch 57/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2094 - acc: 0.9025\n",
      "Epoch 58/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2094 - acc: 0.9025\n",
      "Epoch 59/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2087 - acc: 0.9032\n",
      "Epoch 60/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2081 - acc: 0.9029\n",
      "Epoch 61/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2076 - acc: 0.9031\n",
      "Epoch 62/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2076 - acc: 0.9035\n",
      "Epoch 63/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2069 - acc: 0.9040\n",
      "Epoch 64/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2067 - acc: 0.9035\n",
      "Epoch 65/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2066 - acc: 0.9037\n",
      "Epoch 66/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2058 - acc: 0.9045\n",
      "Epoch 67/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2053 - acc: 0.9041\n",
      "Epoch 68/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2052 - acc: 0.9045\n",
      "Epoch 69/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2042 - acc: 0.9046\n",
      "Epoch 70/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2035 - acc: 0.9050\n",
      "Epoch 71/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2043 - acc: 0.9052\n",
      "Epoch 72/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2036 - acc: 0.9052\n",
      "Epoch 73/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2037 - acc: 0.9052\n",
      "Epoch 74/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2032 - acc: 0.9051\n",
      "Epoch 75/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2039 - acc: 0.9051\n",
      "Epoch 76/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2022 - acc: 0.9055\n",
      "Epoch 77/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2017 - acc: 0.9059\n",
      "Epoch 78/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2013 - acc: 0.9063\n",
      "Epoch 79/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2016 - acc: 0.9061\n",
      "Epoch 80/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2013 - acc: 0.9061\n",
      "Epoch 81/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2015 - acc: 0.9061\n",
      "Epoch 82/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2015 - acc: 0.9058\n",
      "Epoch 83/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2005 - acc: 0.9065\n",
      "Epoch 84/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.2003 - acc: 0.9066\n",
      "Epoch 85/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2004 - acc: 0.9069\n",
      "Epoch 86/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2012 - acc: 0.9066\n",
      "Epoch 87/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1993 - acc: 0.9070\n",
      "Epoch 88/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1997 - acc: 0.9071\n",
      "Epoch 89/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1995 - acc: 0.9069\n",
      "Epoch 90/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1991 - acc: 0.9069\n",
      "Epoch 91/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1997 - acc: 0.9070\n",
      "Epoch 92/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1984 - acc: 0.9075\n",
      "Epoch 93/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1992 - acc: 0.9074\n",
      "Epoch 94/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1989 - acc: 0.9076\n",
      "Epoch 95/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1984 - acc: 0.9077\n",
      "Epoch 96/100\n",
      "429793/429793 [==============================] - 44s 104us/step - loss: 0.1989 - acc: 0.9074\n",
      "Epoch 97/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1983 - acc: 0.9077\n",
      "Epoch 98/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1986 - acc: 0.9074\n",
      "Epoch 99/100\n",
      "429793/429793 [==============================] - 44s 103us/step - loss: 0.1976 - acc: 0.9081\n",
      "Epoch 100/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1978 - acc: 0.9076\n",
      "107448/107448 [==============================] - 3s 24us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429793/429793 [==============================] - 46s 107us/step - loss: 0.5536 - acc: 0.6968\n",
      "Epoch 2/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.4250 - acc: 0.7904\n",
      "Epoch 3/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.3721 - acc: 0.8212\n",
      "Epoch 4/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.3419 - acc: 0.8386\n",
      "Epoch 5/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.3202 - acc: 0.8493\n",
      "Epoch 6/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.3033 - acc: 0.8577\n",
      "Epoch 7/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2913 - acc: 0.8633\n",
      "Epoch 8/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2822 - acc: 0.8676\n",
      "Epoch 9/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2748 - acc: 0.8717\n",
      "Epoch 10/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2688 - acc: 0.8747\n",
      "Epoch 11/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2628 - acc: 0.8772\n",
      "Epoch 12/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2590 - acc: 0.8786\n",
      "Epoch 13/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2534 - acc: 0.8820\n",
      "Epoch 14/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2509 - acc: 0.8830\n",
      "Epoch 15/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2466 - acc: 0.8850\n",
      "Epoch 16/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2441 - acc: 0.8863\n",
      "Epoch 17/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2407 - acc: 0.8885\n",
      "Epoch 18/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2385 - acc: 0.8895\n",
      "Epoch 19/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2364 - acc: 0.8905\n",
      "Epoch 20/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2339 - acc: 0.8914\n",
      "Epoch 21/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2317 - acc: 0.8930\n",
      "Epoch 22/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2292 - acc: 0.8941\n",
      "Epoch 23/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2283 - acc: 0.8944\n",
      "Epoch 24/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2266 - acc: 0.8954\n",
      "Epoch 25/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2238 - acc: 0.8966\n",
      "Epoch 26/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2234 - acc: 0.8970\n",
      "Epoch 27/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2220 - acc: 0.8975\n",
      "Epoch 28/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2197 - acc: 0.8986\n",
      "Epoch 29/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2181 - acc: 0.8997\n",
      "Epoch 30/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2166 - acc: 0.9008\n",
      "Epoch 31/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2168 - acc: 0.9002\n",
      "Epoch 32/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2147 - acc: 0.9013\n",
      "Epoch 33/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2136 - acc: 0.9021\n",
      "Epoch 34/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2121 - acc: 0.9027\n",
      "Epoch 35/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2107 - acc: 0.9033\n",
      "Epoch 36/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2095 - acc: 0.9041\n",
      "Epoch 37/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2087 - acc: 0.9045\n",
      "Epoch 38/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2077 - acc: 0.9049\n",
      "Epoch 39/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2070 - acc: 0.9053\n",
      "Epoch 40/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2059 - acc: 0.9056\n",
      "Epoch 41/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2052 - acc: 0.9062\n",
      "Epoch 42/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2044 - acc: 0.9067\n",
      "Epoch 43/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2036 - acc: 0.9070\n",
      "Epoch 44/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2037 - acc: 0.9065\n",
      "Epoch 45/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2023 - acc: 0.9076\n",
      "Epoch 46/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2018 - acc: 0.9078\n",
      "Epoch 47/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.2007 - acc: 0.9087\n",
      "Epoch 48/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2008 - acc: 0.9085\n",
      "Epoch 49/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2000 - acc: 0.9087\n",
      "Epoch 50/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1995 - acc: 0.9093\n",
      "Epoch 51/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1985 - acc: 0.9095\n",
      "Epoch 52/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1984 - acc: 0.9098\n",
      "Epoch 53/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1976 - acc: 0.9101\n",
      "Epoch 54/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1968 - acc: 0.9100\n",
      "Epoch 55/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1971 - acc: 0.9101\n",
      "Epoch 56/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1968 - acc: 0.9101\n",
      "Epoch 57/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1957 - acc: 0.9104\n",
      "Epoch 58/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1962 - acc: 0.9104\n",
      "Epoch 59/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1955 - acc: 0.9110\n",
      "Epoch 60/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1936 - acc: 0.9122\n",
      "Epoch 61/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1928 - acc: 0.9125\n",
      "Epoch 62/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1937 - acc: 0.9121\n",
      "Epoch 63/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1941 - acc: 0.9114\n",
      "Epoch 64/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1930 - acc: 0.9123\n",
      "Epoch 65/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1924 - acc: 0.9127\n",
      "Epoch 66/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1923 - acc: 0.9123\n",
      "Epoch 67/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1912 - acc: 0.9132\n",
      "Epoch 68/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1913 - acc: 0.9132\n",
      "Epoch 69/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1915 - acc: 0.9129\n",
      "Epoch 70/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1913 - acc: 0.9126\n",
      "Epoch 71/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1905 - acc: 0.9134\n",
      "Epoch 72/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1904 - acc: 0.9135\n",
      "Epoch 73/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1896 - acc: 0.9138\n",
      "Epoch 74/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1896 - acc: 0.9135\n",
      "Epoch 75/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1894 - acc: 0.9138\n",
      "Epoch 76/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1898 - acc: 0.9137\n",
      "Epoch 77/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1894 - acc: 0.9133\n",
      "Epoch 78/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1889 - acc: 0.9138\n",
      "Epoch 79/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1887 - acc: 0.9140\n",
      "Epoch 80/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1883 - acc: 0.9147\n",
      "Epoch 81/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1878 - acc: 0.9148\n",
      "Epoch 82/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1879 - acc: 0.9143\n",
      "Epoch 83/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1873 - acc: 0.9145\n",
      "Epoch 84/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1871 - acc: 0.9148\n",
      "Epoch 85/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1869 - acc: 0.9150\n",
      "Epoch 86/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1864 - acc: 0.9152\n",
      "Epoch 87/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1874 - acc: 0.9147\n",
      "Epoch 88/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1864 - acc: 0.9148\n",
      "Epoch 89/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1861 - acc: 0.9148\n",
      "Epoch 90/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1858 - acc: 0.9155\n",
      "Epoch 91/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1862 - acc: 0.9154\n",
      "Epoch 92/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1858 - acc: 0.9156\n",
      "Epoch 93/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1851 - acc: 0.9160\n",
      "Epoch 94/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1854 - acc: 0.9157\n",
      "Epoch 95/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1847 - acc: 0.9156\n",
      "Epoch 96/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1849 - acc: 0.9160\n",
      "Epoch 97/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1846 - acc: 0.9158\n",
      "Epoch 98/100\n",
      "429793/429793 [==============================] - 45s 104us/step - loss: 0.1849 - acc: 0.9158\n",
      "Epoch 99/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1842 - acc: 0.9161\n",
      "Epoch 100/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.1847 - acc: 0.9158\n",
      "107448/107448 [==============================] - 3s 24us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 66)                2970      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 50)                3350      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 33)                1683      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 33)                132       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,103\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "429793/429793 [==============================] - 47s 109us/step - loss: 0.5628 - acc: 0.6921\n",
      "Epoch 2/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.4363 - acc: 0.7841\n",
      "Epoch 3/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.3821 - acc: 0.8152\n",
      "Epoch 4/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.3518 - acc: 0.8331\n",
      "Epoch 5/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.3311 - acc: 0.8436\n",
      "Epoch 6/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.3165 - acc: 0.8508\n",
      "Epoch 7/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.3037 - acc: 0.8580\n",
      "Epoch 8/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2946 - acc: 0.8621\n",
      "Epoch 9/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2863 - acc: 0.8660\n",
      "Epoch 10/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.2788 - acc: 0.8696\n",
      "Epoch 11/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2724 - acc: 0.8731\n",
      "Epoch 12/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.2680 - acc: 0.8749\n",
      "Epoch 13/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2632 - acc: 0.8771\n",
      "Epoch 14/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2598 - acc: 0.8790\n",
      "Epoch 15/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2551 - acc: 0.8809\n",
      "Epoch 16/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2519 - acc: 0.8823\n",
      "Epoch 17/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2480 - acc: 0.8843\n",
      "Epoch 18/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2463 - acc: 0.8852\n",
      "Epoch 19/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2446 - acc: 0.8860\n",
      "Epoch 20/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2415 - acc: 0.8880\n",
      "Epoch 21/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2387 - acc: 0.8887\n",
      "Epoch 22/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2381 - acc: 0.8892\n",
      "Epoch 23/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2361 - acc: 0.8901\n",
      "Epoch 24/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2346 - acc: 0.8907\n",
      "Epoch 25/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2329 - acc: 0.8912\n",
      "Epoch 26/100\n",
      "429793/429793 [==============================] - 46s 106us/step - loss: 0.2306 - acc: 0.8929\n",
      "Epoch 27/100\n",
      "429793/429793 [==============================] - 46s 107us/step - loss: 0.2297 - acc: 0.8941\n",
      "Epoch 28/100\n",
      "429793/429793 [==============================] - 46s 106us/step - loss: 0.2285 - acc: 0.8940\n",
      "Epoch 29/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.2262 - acc: 0.8953\n",
      "Epoch 30/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.2259 - acc: 0.8955\n",
      "Epoch 31/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2240 - acc: 0.8959\n",
      "Epoch 32/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2234 - acc: 0.8965\n",
      "Epoch 33/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2226 - acc: 0.8968\n",
      "Epoch 34/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.2218 - acc: 0.8972\n",
      "Epoch 35/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.2209 - acc: 0.8976\n",
      "Epoch 36/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2187 - acc: 0.8985\n",
      "Epoch 37/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2183 - acc: 0.8989\n",
      "Epoch 38/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.2181 - acc: 0.8991\n",
      "Epoch 39/100\n",
      "429793/429793 [==============================] - 45s 106us/step - loss: 0.2173 - acc: 0.8995\n",
      "Epoch 40/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2158 - acc: 0.8998\n",
      "Epoch 41/100\n",
      "429793/429793 [==============================] - 45s 105us/step - loss: 0.2159 - acc: 0.9002\n",
      "Epoch 42/100\n",
      "429793/429793 [==============================] - 46s 107us/step - loss: 0.2144 - acc: 0.9008\n",
      "Epoch 43/100\n",
      "429793/429793 [==============================] - 46s 107us/step - loss: 0.2142 - acc: 0.9010\n",
      "Epoch 44/100\n",
      "341088/429793 [======================>.......] - ETA: 9s - loss: 0.2134 - acc: 0.9012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-57-473b12d56939>\", line 1, in <module>\n",
      "    pred_sp_ann_2h_prob_unisoftsigbinlosadam,pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict_()\n",
      "  File \"<ipython-input-27-1dc353c873dd>\", line 17, in ann_predict_\n",
      "    verbose=1)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 789, in cross_val_predict\n",
      "    for train, test in cv.split(X, y, groups))\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/joblib/parallel.py\", line 924, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/joblib/parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/joblib/parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/joblib/parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 876, in _fit_and_predict\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\", line 210, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\", line 152, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/keras/engine/training.py\", line 1039, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2715, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2675, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/user/anaconda3/envs/deepl/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "pred_sp_ann_2h_prob_unisoftsigbinlosadam,pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with no encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodr_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=train_x,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_nodr_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = nodr_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = train_x,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_nodr_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_nodr_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_nodr_ann_2h_unisoftsigbinlosadam, './Figures/nodr_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict(nodr_ann_2h_unisoftsigbinlosadam,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=train_x\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=test_x\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_ = PCA(n_components = 0.95, svd_solver = 'full').fit(train_x)\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# n_coml = [pca_.n_components_]\n",
    "\n",
    "# plt.plot(np.cumsum(pca_.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components', fontsize=14)\n",
    "# plt.ylabel('Variance (%)', fontsize=14) #for each component\n",
    "# plt.title('Pulsar Dataset Explained Variance '+str(dsnum)+' node DS', fontsize=14)\n",
    "\n",
    "# n_coml = [*n_coml]\n",
    "\n",
    "# for i, v in enumerate(n_coml):\n",
    "#     plt.text(v-0.8, i+0.94, '{:.0f}'.format(v), color='navy', fontsize=14)\n",
    "\n",
    "# plt.savefig('./Figures/PCA_components_ds'+str(dsnum)+'bal.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators=300, \n",
    "#                              criterion='gini', \n",
    "#                              max_depth=16, \n",
    "#                              #min_samples_split=2, \n",
    "#                              #min_samples_leaf=1, \n",
    "#                              max_features=0.3, \n",
    "#                              #bootstrap=True,\n",
    "#                              oob_score=True,\n",
    "#                              random_state=23)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(enc_train_x_asam, train_y)\n",
    "\n",
    "# pred_y_ae_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(enc_test_x_asam),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_ae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_ae_RF, pred_y_ae_RF, './Figures/ROC_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(enc_train_x_spsam, train_y)\n",
    "\n",
    "# pred_y_spae_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(enc_test_x_spsam),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_spae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_spae_RF, pred_y_spae_RF, './Figures/ROC_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with pca DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(train_x_pca, train_y)\n",
    "\n",
    "# pred_y_pca_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(test_x_pca),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_pca_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_pca_RF, pred_y_pca_RF, './Figures/ROC_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_ae_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_ae_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "# print(pred_y_ae_RF.shape)\n",
    "# print(pred_y_spae_RF.shape)\n",
    "# print(pred_y_pca_RF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate_ae_ann, recall_ae_ann, thresholds_ae_ann = roc_curve(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_ae_ann = auc(false_positive_rate_ae_ann, recall_ae_ann)\n",
    "false_positive_rate_sp_ann, recall_sp_ann, thresholds_sp_ann = roc_curve(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_sp_ann = auc(false_positive_rate_sp_ann, recall_sp_ann)\n",
    "false_positive_rate_nodr_ann, recall_nodr_ann, thresholds_nodr_ann = roc_curve(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_nodr_ann = auc(false_positive_rate_nodr_ann, recall_nodr_ann)\n",
    "\n",
    "# false_positive_rate_ae_RF, recall_ae_RF, thresholds_ae_RF = roc_curve(test_y, pred_y_ae_RF)\n",
    "# roc_auc_ae_RF = auc(false_positive_rate_ae_RF, recall_ae_RF)\n",
    "# false_positive_rate_spae_RF, recall_spae_RF, thresholds_spae_RF = roc_curve(test_y, pred_y_spae_RF)\n",
    "# roc_auc_spae_RF = auc(false_positive_rate_spae_RF, recall_spae_RF)\n",
    "# false_positive_rate_pca_RF, recall_pca_RF, thresholds_pca_RF = roc_curve(test_y, pred_y_pca_RF)\n",
    "# roc_auc_pca_RF = auc(false_positive_rate_pca_RF, recall_pca_RF)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC)', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "# plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "# plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "# plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "# plt.ylim([0.97,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC) Zoom in', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "# plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "# plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "# plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "# plt.ylim([0.0,1.0])\n",
    "plt.ylim([0.955,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal_zoom.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ae_ann = \"AE+DNN\"\n",
    "acc_ae_ann = (sm.accuracy_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_ae_ann = (sm.precision_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_ae_ann = (sm.recall_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_ae_ann = (sm.f1_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_sp_ann = \"SAE+DNN\"\n",
    "acc_sp_ann = (sm.accuracy_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_sp_ann = (sm.precision_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_sp_ann = (sm.recall_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_sp_ann = (sm.f1_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_nodr_ann = \"DNN\"\n",
    "acc_nodr_ann = (sm.accuracy_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_nodr_ann = (sm.precision_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_nodr_ann = (sm.recall_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_nodr_ann = (sm.f1_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "# classi_ae_RF = \"AE+RF\"\n",
    "# acc_ae_RF = (sm.accuracy_score(test_y, pred_y_ae_RF)*100) \n",
    "# pre_ae_RF = (sm.precision_score(test_y, pred_y_ae_RF)*100) \n",
    "# recall_ae_RF = (sm.recall_score(test_y, pred_y_ae_RF)*100) \n",
    "# f1score_ae_RF = (sm.f1_score(test_y, pred_y_ae_RF)*100)\n",
    "\n",
    "# classi_spae_RF = \"SAE+RF\"\n",
    "# acc_spae_RF = (sm.accuracy_score(test_y, pred_y_spae_RF)*100) \n",
    "# pre_spae_RF = (sm.precision_score(test_y, pred_y_spae_RF)*100) \n",
    "# recall_spae_RF = (sm.recall_score(test_y, pred_y_spae_RF)*100) \n",
    "# f1score_spae_RF = (sm.f1_score(test_y, pred_y_spae_RF)*100)\n",
    "\n",
    "# classi_pca_RF = \"PCA+RF\"\n",
    "# acc_pca_RF = (sm.accuracy_score(test_y, pred_y_pca_RF)*100) \n",
    "# pre_pca_RF = (sm.precision_score(test_y, pred_y_pca_RF)*100) \n",
    "# recall_pca_RF = (sm.recall_score(test_y, pred_y_pca_RF)*100) \n",
    "# f1score_pca_RF = (sm.f1_score(test_y, pred_y_pca_RF)*100)\n",
    "\n",
    "\n",
    "print('Classifier\\tAcc\\tPreci\\tRecall\\tF1Score')\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_ann, acc_ae_ann, pre_ae_ann, recall_ae_ann, f1score_ae_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_sp_ann, acc_sp_ann, pre_sp_ann, recall_sp_ann, f1score_sp_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_nodr_ann, acc_nodr_ann, pre_nodr_ann, recall_nodr_ann, f1score_nodr_ann))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_RF, acc_ae_RF, pre_ae_RF, recall_ae_RF, f1score_ae_RF))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_spae_RF, acc_spae_RF, pre_spae_RF, recall_spae_RF, f1score_spae_RF))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_pca_RF, acc_pca_RF, pre_pca_RF, recall_pca_RF, f1score_pca_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1list = [[\"AE+DNN\",f1score_ae_ann],[\"SAE+DNN\",f1score_sp_ann],[\"DNN\",f1score_nodr_ann]]#,\n",
    "#           [\"AE+RF\",f1score_ae_RF],[\"SAE+RF\",f1score_spae_RF],[\"PCA+RF\",f1score_pca_RF]]\n",
    "\n",
    "xs, ys = [*zip(*f1list)]\n",
    "\n",
    "'{:.2f}'.format(f1score_ae_ann)\n",
    "\n",
    "plt.figure(figsize=(8,6), )\n",
    "plt.barh(xs, ys, color = \"purple\")\n",
    "plt.title(\"F1 score vs Classifier\", fontsize=16)\n",
    "plt.xlabel(\"Classifier\", fontsize=14)\n",
    "plt.ylabel(\"F1 score\", fontsize=14)\n",
    "plt.xticks(np.arange(0, 101, 10), fontsize=12)\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "for i, v in enumerate(ys):\n",
    "    plt.text(v+1, i+0.1, '{:.2f}'.format(v), color='purple', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/F1scoreplot_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
