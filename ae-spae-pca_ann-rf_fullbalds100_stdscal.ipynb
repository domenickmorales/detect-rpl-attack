{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Donâ€™t pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "from keras import optimizers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Activation, Dense, Dropout, Input\n",
    "from keras import backend as k\n",
    "\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "#k.tensorflow_backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------Import modules------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(23)\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from datetime import datetime \n",
    "import os.path\n",
    "\n",
    "dsnum=100\n",
    "verbose_level=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/01Code/00Datasets_final/00BalancedDS/FullCloneID100bal_stdscal.csv\n"
     ]
    }
   ],
   "source": [
    "pathds = os.path.abspath('/home/user/01Code/00Datasets_final/00BalancedDS')\n",
    "file_name = \"FullCloneID\"+str(dsnum)+\"bal_stdscal.csv\"\n",
    "full_path = os.path.join(pathds,file_name)\n",
    "print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2078832, 211)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "neurons=df.shape[1]-1\n",
    "batch_size=df.shape[1]-1\n",
    "print(neurons)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Explaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1039416\n",
      "Class 1: 1039416\n",
      "Proportion: 1.0 : 1\n"
     ]
    }
   ],
   "source": [
    "#if you don't have an intuitive sense of how imbalanced these two classes are, let's go visual\n",
    "count_classes = pd.value_counts(df['class'], sort = True)\n",
    "print('Class 0:', count_classes[0])\n",
    "print('Class 1:', count_classes[1])\n",
    "print('Proportion:', round(count_classes[0] / count_classes[1], 3), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.xticks(range(2), ['Normal [0]','Malicious [1]'])\n",
    "plt.title(\"Frequency by observation number\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Observations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed and percentage of test data\n",
    "RANDOM_SEED = 23 #used to help randomly select the data points\n",
    "TEST_PCT = 0.20 # 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_df = train_test_split(df, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ df -> original dataset \n",
    "+ train -> subset of 80% from original dataset \n",
    "+ test_df -> subset of 20% from original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(train, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train -> subset of 80% from original dataset \n",
    "+ train_df -> subset of 80% from train\n",
    "+ dev_df -> subset of 20% from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49990228884619664\n",
      "0.500260061993969\n",
      "0.5001046259082611\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of mal samples in train and test set\n",
    "print(train_df.iloc[:, batch_size].sum()/train_df.shape[0]) \n",
    "print(dev_df.iloc[:, batch_size].sum()/dev_df.shape[0]) \n",
    "print(test_df.iloc[:, batch_size].sum()/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.iloc[:, :batch_size] \n",
    "dev_x = dev_df.iloc[:, :batch_size] \n",
    "test_x = test_df.iloc[:, :batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_x -> features of train_df **Training subset for AE**\n",
    "+ dev_x -> features of dev_df **Validation subset for AE**\n",
    "+ test_x -> features of test_df **Testing subset for ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final train and test sets\n",
    "train_y = train_df.iloc[:,batch_size]\n",
    "dev_y = dev_df.iloc[:,batch_size]\n",
    "test_y = test_df.iloc[:,batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_y -> **Labels for supervised training of ANN**\n",
    "+ dev_y -> labels of dev_df  *not used for AE neither ANN*\n",
    "+ test_y -> labels of test_df  **Ground Truth for predictions of supervised ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "train_x =np.array(train_x)\n",
    "dev_x =np.array(dev_x)\n",
    "test_x = np.array(test_x)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "dev_y = np.array(dev_y)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "print(train_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(factor_enc_dim, enc_activation, dec_activation, \n",
    "                optimizer, loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer #RELU\n",
    "    encoded = Dense(encoding_dim, activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer #SIMOID\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spae(factor_enc_dim,dec_activation,enc_activation,\n",
    "         optimizer,loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer\n",
    "    encoded = Dense(encoding_dim, activity_regularizer=regularizers.l1(1e-4), activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pca(thr):\n",
    "    #train_x_pca,test_x_pca = to_pca(0.95)\n",
    "    pca = PCA(n_components = thr, svd_solver = 'full')\n",
    "    train_x_ = np.array(train_x)\n",
    "    print(type(train_x_))\n",
    "\n",
    "    test_x_ = np.array(test_x)\n",
    "    print(type(test_x_))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(time.ctime(start_time))\n",
    "\n",
    "    train_x_pca = pca.fit_transform(train_x_)\n",
    "    print(train_x_pca.shape)\n",
    "\n",
    "    test_x_pca = pca.fit_transform(test_x_)\n",
    "    print(test_x_pca.shape)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "\n",
    "    print(\"--- PCA spent %s seconds ---\" %elapsed_time )\n",
    "    \n",
    "    return  train_x_pca,test_x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ae(checkpoint_file, autoencoder,\n",
    "           epochs, batch_size, shuffle):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    hist_auto = autoencoder.fit(train_x, train_x,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    verbose=verbose_level,\n",
    "                    callbacks=[early_stopping, cp, tb],\n",
    "                    validation_data=(dev_x, dev_x))\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "    \n",
    "    return hist_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_auto(hist_auto, fig_file):\n",
    "    best_loss_value = hist_auto.history['loss'][-1]\n",
    "    print('Best loss value:', best_loss_value)\n",
    "\n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist_auto.history['loss'])\n",
    "    plt.plot(hist_auto.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.savefig(fig_file)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h(neurons,encoded_train_x,init_mode,activation_input,\n",
    "               weight_constraint,dropout_rate,activation_output,\n",
    "               loss,optimizer):\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=encoded_train_x.shape[1],\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h_():\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=input_dim,\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(checkpoint_file,ann,enc_train_x,train_y,epochs,shuffle,batch_size):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    history = ann.fit(enc_train_x,\n",
    "                      train_y,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[early_stopping, cp, tb],\n",
    "                      epochs=epochs,\n",
    "                      shuffle=shuffle,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=verbose_level)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict(ann,enc_test_x):\n",
    "    pred_ann_prob = ann.predict(enc_test_x)\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "    \n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob, pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict_():\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))  \n",
    "\n",
    "    modelk = KerasClassifier(build_fn=ann_2h_,\n",
    "                             epochs=epochs, \n",
    "                             batch_size=batch_size, \n",
    "                             verbose=verbose_level\n",
    "                            )\n",
    "\n",
    "    pred_ann_prob = cross_val_predict(modelk,\n",
    "                                      enc_test_x,\n",
    "                                      test_y,\n",
    "                                      cv=KFold(n_splits=5, random_state=23),\n",
    "                                      verbose=1)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "\n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob,pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cm(pred_ann_prob, pred_ann_01, roc_file, cm_file):\n",
    "    false_positive_rate, recall, thresholds = roc_curve(test_y, pred_ann_prob)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.savefig(roc_file)\n",
    "    plt.show()\n",
    "    \n",
    "    cm = confusion_matrix(test_y, pred_ann_01)\n",
    "    labels = ['Normal', 'Malicious']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=\"RdYlGn\", vmin = 0.2);\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.savefig(cm_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- PCA Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_pca,test_x_pca = to_pca(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- AE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 140)               29540     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 210)               29610     \n",
      "=================================================================\n",
      "Total params: 59,150\n",
      "Trainable params: 59,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_sigmoid_adam_mse,enc_train_x_asam,enc_test_x_asam = ae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ae_sigmoid_adam_mse = load_model('ae_sigmoid_adam_mse_redds10bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 17:49:05 2019\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1330452 samples, validate on 332613 samples\n",
      "Epoch 1/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0533 - acc: 0.3268 - val_loss: 0.0512 - val_acc: 0.2802\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05117, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0493 - acc: 0.2178 - val_loss: 0.0510 - val_acc: 0.2006\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05117 to 0.05102, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1801 - val_loss: 0.0510 - val_acc: 0.1502\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05102 to 0.05100, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1547 - val_loss: 0.0510 - val_acc: 0.1920\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05100 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0720 - val_loss: 0.0510 - val_acc: 0.0791\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1063 - val_loss: 0.0510 - val_acc: 0.1379\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1229 - val_loss: 0.0510 - val_acc: 0.0632\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1235 - val_loss: 0.0510 - val_acc: 0.0579\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0975 - val_loss: 0.0510 - val_acc: 0.0821\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0837 - val_loss: 0.0510 - val_acc: 0.0926\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0492 - acc: 0.1086 - val_loss: 0.0510 - val_acc: 0.1202\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.05098\n",
      "Epoch 12/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1168 - val_loss: 0.0510 - val_acc: 0.1176\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1214 - val_loss: 0.0510 - val_acc: 0.1342\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1314 - val_loss: 0.0510 - val_acc: 0.1292\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1298 - val_loss: 0.0510 - val_acc: 0.1277\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 16/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1229 - val_loss: 0.0510 - val_acc: 0.1194\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1224 - val_loss: 0.0510 - val_acc: 0.1196\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1198 - val_loss: 0.0510 - val_acc: 0.1090\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.05098\n",
      "Epoch 19/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0983 - val_loss: 0.0510 - val_acc: 0.0927\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0975 - val_loss: 0.0510 - val_acc: 0.0997\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05098 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1065 - val_loss: 0.0510 - val_acc: 0.1061\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 22/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1044 - val_loss: 0.0510 - val_acc: 0.1023\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 23/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1117 - val_loss: 0.0510 - val_acc: 0.1162\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 24/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1187 - val_loss: 0.0510 - val_acc: 0.1116\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.05097\n",
      "Epoch 25/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1079 - val_loss: 0.0510 - val_acc: 0.1043\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 26/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1036 - val_loss: 0.0510 - val_acc: 0.0951\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 27/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0967 - val_loss: 0.0510 - val_acc: 0.0977\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 28/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0981 - val_loss: 0.0510 - val_acc: 0.0956\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 29/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0997 - val_loss: 0.0510 - val_acc: 0.0988\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 30/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0933 - val_loss: 0.0510 - val_acc: 0.0906\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 31/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0975 - val_loss: 0.0510 - val_acc: 0.1036\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 32/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0990 - val_loss: 0.0510 - val_acc: 0.0953\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 33/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1002 - val_loss: 0.0510 - val_acc: 0.1046\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.05097\n",
      "Epoch 34/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0972 - val_loss: 0.0510 - val_acc: 0.0956\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.05097\n",
      "Epoch 35/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1014 - val_loss: 0.0510 - val_acc: 0.1019\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 36/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0985 - val_loss: 0.0510 - val_acc: 0.0985\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05097\n",
      "Epoch 37/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1075 - val_loss: 0.0510 - val_acc: 0.0921\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.05097\n",
      "Epoch 38/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0869 - val_loss: 0.0510 - val_acc: 0.0926\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 39/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.0910 - val_loss: 0.0510 - val_acc: 0.0888\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.05097\n",
      "Epoch 40/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0989 - val_loss: 0.0510 - val_acc: 0.0955\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 41/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1024 - val_loss: 0.0510 - val_acc: 0.0957\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.05097\n",
      "Epoch 42/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1035 - val_loss: 0.0510 - val_acc: 0.0973\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.05097\n",
      "Epoch 43/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1024 - val_loss: 0.0510 - val_acc: 0.1153\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.05097\n",
      "Epoch 44/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1122 - val_loss: 0.0510 - val_acc: 0.1145\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 45/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1064 - val_loss: 0.0510 - val_acc: 0.1056\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.05097\n",
      "Epoch 46/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1153 - val_loss: 0.0510 - val_acc: 0.1125\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.05097\n",
      "Epoch 47/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1014 - val_loss: 0.0510 - val_acc: 0.0936\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 48/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.0944 - val_loss: 0.0510 - val_acc: 0.0982\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.05097\n",
      "Epoch 49/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1004 - val_loss: 0.0510 - val_acc: 0.1100\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.05097\n",
      "Epoch 50/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1074 - val_loss: 0.0510 - val_acc: 0.1134\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 51/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1077 - val_loss: 0.0510 - val_acc: 0.1221\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.05097\n",
      "Epoch 52/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0492 - acc: 0.1318 - val_loss: 0.0510 - val_acc: 0.1432\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.05097\n",
      "Epoch 53/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1419 - val_loss: 0.0510 - val_acc: 0.1334\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.05097\n",
      "Epoch 54/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1319 - val_loss: 0.0510 - val_acc: 0.1391\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.05097\n",
      "Epoch 55/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1115 - val_loss: 0.0510 - val_acc: 0.1088\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 56/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1064 - val_loss: 0.0510 - val_acc: 0.1081\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 57/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1156 - val_loss: 0.0510 - val_acc: 0.1337\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.05097\n",
      "Epoch 58/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1229 - val_loss: 0.0510 - val_acc: 0.1221\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.05097\n",
      "Epoch 59/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1238 - val_loss: 0.0510 - val_acc: 0.1292\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 60/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1337 - val_loss: 0.0510 - val_acc: 0.1383\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 61/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1398 - val_loss: 0.0510 - val_acc: 0.1382\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.05097\n",
      "Epoch 62/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1307 - val_loss: 0.0510 - val_acc: 0.1303\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.05097\n",
      "Epoch 63/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1198 - val_loss: 0.0510 - val_acc: 0.1213\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.05097\n",
      "Epoch 64/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1035 - val_loss: 0.0510 - val_acc: 0.0952\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 65/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1007 - val_loss: 0.0510 - val_acc: 0.1009\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.05097\n",
      "Epoch 66/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1071 - val_loss: 0.0510 - val_acc: 0.1035\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.05097\n",
      "Epoch 67/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1101 - val_loss: 0.0510 - val_acc: 0.1116\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.05097\n",
      "Epoch 68/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1196 - val_loss: 0.0510 - val_acc: 0.1221\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 69/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1207 - val_loss: 0.0510 - val_acc: 0.1059\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.05097\n",
      "Epoch 70/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.0972 - val_loss: 0.0510 - val_acc: 0.0911\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 71/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.0944 - val_loss: 0.0510 - val_acc: 0.0943\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 72/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0943 - val_loss: 0.0510 - val_acc: 0.0936\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.05097\n",
      "Epoch 73/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1001 - val_loss: 0.0510 - val_acc: 0.1011\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 74/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0990 - val_loss: 0.0510 - val_acc: 0.0986\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 75/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.0975 - val_loss: 0.0510 - val_acc: 0.1007\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.05097\n",
      "Epoch 76/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1062 - val_loss: 0.0510 - val_acc: 0.1076\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.05097\n",
      "Epoch 77/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1066 - val_loss: 0.0510 - val_acc: 0.1048\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.05097\n",
      "Epoch 78/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.1017 - val_loss: 0.0510 - val_acc: 0.0999\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.05097\n",
      "Epoch 79/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1000 - val_loss: 0.0510 - val_acc: 0.1055\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.05097\n",
      "Time elapsed (hh:mm:ss.ms) 0:20:00.791017\n"
     ]
    }
   ],
   "source": [
    "hist_ae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/ae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = ae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.04916991403010735\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X1cVGX++P/XmRlmRLnVhCEF07SWQtPNNqyUFXaiDV1whdoe2WpptmUqadrWfsJi12yLROjz2+9mbjfbt213qZSSWrzBZLestd2Kn9t0Y0mBwZCCCsrtzPn+McyRYQbEgRO0vZ8PfTDnnOtc533N3Xuuc3MdRVVVFSGEEGKAGQY7ACGEEP+dJMEIIYTQhSQYIYQQupAEI4QQQheSYIQQQuhCEowQQghdSIIR4htWXV3NhRdeSEdHxxnLvvzyy9xwww39rkeIwSAJRoheJCcnk5CQQH19vdf89PR0LrzwQqqrqwcpMiGGPkkwQpzBmDFjKCkp0aY//vhjWlpaBjEiIb4dJMEIcQbp6els27ZNm962bRsZGRleZRobG1m7di2JiYnMnj2b3/3ud7hcLgCcTie//e1vufzyy0lJSWHv3r0+6953331cddVVzJw5k/z8fJxO51nH6XA4+MUvfsEPfvADbDYbf/3rX7VlFRUV/PSnP+X73/8+V1xxBRs2bACgtbWVu+++m8svv5zp06czf/58jhw5ctbbFsIf02AHIMRQN3XqVIqLi/nss88477zzeO211/jTn/7Epk2btDK//vWvaWxsZNeuXRw7dozFixczevRosrKy+Otf/8qePXvYtm0bwcHBLF++3Kv+e+65h3POOYcdO3bQ3NzMbbfdRkxMDD/72c/OKs7Vq1czceJE/v73v/P5559z8803Exsby4wZM1i/fj0///nPycjI4OTJk3z66acAbN26laamJt544w3MZjN2u51hw4b1/0kTAunBCNEnnl7Mm2++yYQJE4iOjtaWOZ1OXnvtNVavXk1ISAhjx47l5ptv5pVXXgHg9ddfZ+HChcTExBAREcFtt92mrXvkyBHKy8u57777GD58OKNGjWLRokVeu+T6oqamhn/961/cfffdWCwW4uPjycrKori4GACTycSXX35JfX09I0aMYOrUqdr8Y8eO8cUXX2A0GklISCAkJKS/T5cQgPRghOiT9PR0FixYQHV1Nenp6V7LGhoaaG9v59xzz9XmnXvuuTgcDgDq6uqIiYnxWubx1Vdf0dHRwVVXXaXNc7lcXuX7oq6ujvDwcK/kcO6553LgwAEA1q9fT2FhIT/+8Y8ZO3Ysd955J7NnzyY9PZ3a2lpWrVrFiRMn+MlPfsJdd91FUFDQWW1fCH8kwQjRB2PGjGHs2LHs3buX9evXey2LjIwkKCiIr776iokTJwLuHoWnlzN69Ghqamq08l0fW61WzGYzb7/9NiZT4B/HqKgojh8/TlNTk5ZkusZw3nnnsXHjRlwuFzt27GDFihW88847DB8+nDvvvJM777yT6upqli5dyvjx48nKygo4FiE8ZBeZEH20fv16nn32WYYPH+4132g0cs0115Cfn09TUxOHDx/m6aef5ic/+QkAP/7xj3nuueeora3l+PHjbN68WVs3KiqKK6+8kocffpimpiZcLhdffvkl//znP88qtpiYGKZNm8bGjRtpbW3lo48+4sUXX2Tu3LkAFBcXU19fj8FgICwsTIv77bff5uOPP8bpdBISEoLJZMJoNPbnaRJCIz0YIfooLi6ux2X3338/v/71r/nRj36ExWIhKyuL+fPnA3DddddRWVlJeno6I0aMYPHixbz99tvauo888gh5eXlce+21nDx5ktjYWG699dazjm/jxo2sW7eOmTNnEhYWxvLly7nyyisB+Pvf/87DDz9MS0sL5557Lvn5+VgsFo4cOcK6detwOBwMHz6ca6+9VkuMQvSXIjccE0IIoQfZRSaEEEIXkmCEEELoQhKMEEIIXUiCEUIIoYvv9Flk77//PhaLJaB1W1tbA15XbxJbYCS2wEhsgfk2x9ba2qqNBtGb73SC8QypEQi73R7wunqT2AIjsQVGYgvMtzk2u93ep3pkF5kQQghdSIIRQgihC0kwQgghdPGdPgYjhBCBaG9vp7q6ul93Nm1vb+/zsYxvmie2YcOGMXbs2IBH15YEI4QQZ6m6uprQ0FDOO+88FEUJqI7m5maCg4MHOLKB0dzczLBhwzh69CjV1dWMHz8+oHpkF5kQQpyllpYWRo0aFXBy+TZQFIVRo0b1q5cmCUYIIQLw35xcPPrbRkkwATja1MqbX5wc7DCEEGJIkwQTgG3vf8Vv3nBwqq1jsEMRQnwHnThxgueff/6s17v11ls5ceKEDhH5JwkmAJ5OY1uHa1DjEEJ8N504cYIXXnjBZ77T6ex1vSeffFK7o+k3Qc4iC0CQ0Z1i2p1yrzYhxDfvscce48svvyQ9PR2TycTw4cOJiorCbrfz2muvcccdd1BbW0trays///nPuf766wFITk7mxRdf5NSpU9x6661ceumlvPfee0RHR/O73/2OYcOGDWickmACYDK6O34dLunBCPFd99K/qvnru1VnvZ7L5cJg8L8T6brpscy/dGyP665evZpPP/2U4uJi3nnnHW677TZeffVVYmNjAXjooYeIiIigpaWFzMxMrr76aiIjI73q+OKLL9i4cSO/+c1vWLlyJaWlpaSnp591O3ojCSYAJoO7B9MhPRghxBAwefJkLbkAPPfcc+zcuROAmpoavvjiC58EM3bsWG1Ay4svvpjDhw8PeFySYAIQ1NmDaXdKD0aI77r5l47ttbfRk4G80HL48OHa43feeYe33nqLv/zlLwQHB3PTTTfR2trqs47ZbNYeG41Gv2X6Sw7yB8DUeQymwyU9GCHEN2/EiBGcPOn/UonGxkbCw8MJDg7ms88+4/333/+GoztNejAB8PRg5CwyIcRgiIyM5Pvf/z5z5szBYrFwzjnnaMtmzZrFn//8Z+bOncv48eP7dGMwvUiCCUCQ9GCEEIPsscce8zvfbDazZcsWv8vKysoAGDlyJNu3b9fmL168eOADRHaRBcTUeeZHhxyDEUKIHunagykvL2f9+vW4XC6ysrJYunSp1/K2tjbWrl3Lf/7zHyIiIsjPz2fs2LFUV1dz7bXXaiN4XnLJJeTm5gLuTPv111/jdDq59NJLWbduHUajkWPHjnHXXXdx+PBhxowZw6ZNmwgPD9elXSa5DkYIIc5Itx6M0+kkNzeXLVu2UFJSwvbt2zl48KBXmaKiIsLCwti5cyeLFi0iLy9PWxYXF0dxcTHFxcVacgEoKCjglVdeYfv27TQ0NPC3v/0NgM2bNzNjxgx27NjBjBkz2Lx5s15N047ByHUwQgjRM90STEVFBePGjSM2Nhaz2UxaWhq7d+/2KlNWVsa8efMASE1NZd++fahq772CkJAQADo6Omhvb9dG+9y9ezcZGRkAZGRksGvXroFukkaugxFCiDPTbReZw+HAarVq09HR0VRUVPiUiYmJcQdiMhEaGkpDQwPgvqFPRkYGISEhZGdnM336dG29xYsXU1FRwaxZs0hNTQXg6NGjREVFARAVFUV9ff0ZY2xtbQ3ojnLVR93ni3/+xZdY1aNnvb7eWlpahuyd8iS2wEhsgdErtvb2dpqbm/tVh6qq/a5DL11j68+dN3VLMP56It3vLdBTmaioKPbs2UNkZCQHDhxg2bJllJSUaL2XP/zhD7S2tnL33Xfz9ttvc+WVVwYUo8Vi0a5kPRuG2kbgMNaYMcTHxwS0bT3Z7faA2vVNkNgCI7EFRq/Y7HZ7vy+SHOp3tPTEFhQU5PMc9jXh6LaLzGq1Ultbq007HA6th9G1TE1NDeDe5dXY2EhERARms1kb1iAhIYG4uDgOHTrkta7FYiE5OVnb7TZq1Cjq6uoAqKurY+TIkXo1rcuFlnIMRgjxzQt0uH6AZ5555hvrOemWYCZPnkxlZSVVVVW0tbVRUlJCcnKyV5nk5GS2bt0KQGlpKYmJiSiKQn19vTbsdFVVFZWVlcTGxnLy5EktiXR0dLB3714mTJig1bVt2zYAtm3bRkpKil5NI8jgGSpGjsEIIb55PQ3X3xd//OMfv7EEo9suMpPJRE5ODkuWLMHpdDJ//nwmTZpEQUEBCQkJpKSkkJmZyZo1a7DZbISHh5Ofnw/A/v37KSwsxGg0YjQaefDBB4mIiODIkSPcfvvttLW14XK5SExM5Gc/+xkAS5cuJTs7mxdffJGYmBgKCgr0atrpHoxcByOEGARdh+u/4oorGDVqFK+//jptbW3YbDZWrFjBqVOnyM7Opra2FpfLxR133MGRI0eoq6tj4cKFRERE8Nxzz+kap67XwSQlJZGUlOQ1b+XKldpji8VCYWGhz3qpqanawfuuzjnnHF566SW/24qMjOTZZ5/tZ8R9o10HI1fyCyHefwHe+79nvZrZ5QSD0f/CaQtg6g09rtt1uP5//OMflJaW8uKLL6KqKrfffjv79++nvr6eqKgo7ZKNxsZGQkNDeeaZZ3j22Wd1PYzgIVfyB8DsGU1ZxiITQgyyN998kzfffJOMjAzmzZvH559/TmVlJRdccAFvvfUWjz76KO+++y6hoaHfeGwyFlkA5IZjQgjN1Bt67W30pG2AziJTVZWlS5dqhwu6evnll9m7dy+PPfYYV155JXfeeWe/t3c2pAcTAM+FlnKQXwgxGLoO13/VVVfx0ksvadMOh4OjR4/icDgIDg4mPT2dxYsX8+GHH/qsqzfpwQRAGypGEowQYhB0Ha5/5syZzJkzR+vBDB8+nEcffZQvvviCRx55BIPBgMlk4oEHHgDguuuu49Zbb2X06NHf7oP8/62MBgUF2UUmhBg83YfrX7hwodd0XFwcM2fO9Fnvpptu4qabbtI1Ng/ZRRYgo0F2kQkhRG8kwQQoyKDQLtfBCCFEjyTBBMhoUORCSyG+w8408vt/g/62URJMgEwGRS60FOI7atiwYRw9evS/OsmoqsrRo0cZNmxYwHXIQf4AGQ0yVIwQ31WeO+9+/fXXAdfR3t5OUFDQAEY1cDyxDRs2jLFjxwZcjySYAJkMipymLMR3VFBQkHZL90B9F25zILvIAiS7yIQQoneSYAJkMshYZEII0RtJMAEyGhS50FIIIXohCSZAJkWRCy2FEKIXuiaY8vJyUlNTsdls2j0JumprayM7OxubzUZWVhbV1dUAVFdXM2XKFNLT00lPTycnJwdw3yd66dKlXHPNNaSlpZGXl6fV9fLLL5OYmKitU1RUpGfTMBlkqBghhOiNbmeROZ1OcnNzefrpp4mOjiYzM5Pk5GQmTpyolSkqKiIsLIydO3dSUlJCXl4emzZtAtzj6BQXF/vUe8stt5CYmEhbWxuLFi1i79692k3Nrr32Wi0Z6c1okB6MEEL0RrceTEVFBePGjSM2Nhaz2UxaWhq7d+/2KlNWVsa8efMA910s9+3b1+uFS8HBwSQmJgJgNpu56KKLcDgcejWhVyYZKkYIIXqlWw/G4XBgtVq16ejoaCoqKnzKxMTEuAMxmQgNDaWhoQFw7ybLyMggJCSE7Oxspk+f7rXuiRMn2LNnj9cIojt27GD//v2MHz+ee++9V6u7J62trdjt9oDaZ8DFiZOnAl5fTy0tLUMyLpDYAiWxBUZiC8xAxaZbgvHXE1EUpU9loqKi2LNnD5GRkRw4cIBly5ZRUlJCSEgIAB0dHaxatYqbbrqJ2NhYAGbPns2cOXMwm8288MIL3HPPPfzxj3/sNUaLxRLwxUTmslqMqnFIXij1XbiASw8SW2AktsB8m2Pra/LRbReZ1WqltrZWm3Y4HERFRfmUqampAdxJo7GxkYiICMxmM5GRkQAkJCQQFxfHoUOHtPXuv/9+zjvvPBYtWqTNi4yMxGw2A+4b6vznP//Rq2mA5zRlOQYjhBA90S3BTJ48mcrKSqqqqmhra6OkpITk5GSvMsnJyWzduhWA0tJSEhMTURSF+vp6nE4nAFVVVVRWVmo9lfz8fJqamrjvvvu86qqrq9Mel5WVcf755+vVNKDzLDI5BiOEED3SbReZyWQiJyeHJUuW4HQ6mT9/PpMmTaKgoICEhARSUlLIzMxkzZo12Gw2wsPDyc/PB2D//v0UFhZiNBoxGo08+OCDREREUFtby+9//3smTJignRywYMECsrKyeO655ygrK8NoNBIeHs6GDRv0ahrgOYtMEowQQvRE18Euk5KStFOIPVauXKk9tlgsFBYW+qyXmppKamqqz3yr1crHH3/sd1urV69m9erV/Yy47+SGY0II0Tu5kj9AcgxGCCF6JwkmQCYD0oMRQoheSIIJkFGR+8EIIURvJMEEyCSjKQshRK8kwQTIaIB2p/pffU9uIYToD0kwAQoyuEclkAP9QgjhnySYABk9CUaOwwghhF+SYAJk6nzm2uU4jBBC+CUJJkDSgxFCiN5JggmQSUsw0oMRQgh/JMEE6PQuMunBCCGEP5JgAuTpwbR3SA9GCCH8kQQTIO0YjBzkF0IIvyTBBEjbRSYH+YUQwi9JMAEyKXIWmRBC9EbXBFNeXk5qaio2m43Nmzf7LG9rayM7OxubzUZWVhbV1dUAVFdXM2XKFNLT00lPTycnJweA5uZmli5dyjXXXENaWhp5eXlnrEsvnl1kch2MEEL4p1uCcTqd5ObmsmXLFkpKSti+fTsHDx70KlNUVERYWBg7d+5k0aJFXgkjLi6O4uJiiouLyc3N1ebfcsst/O1vf2Pr1q38+9//Zu/evWesSw8mo/RghBCiN7olmIqKCsaNG0dsbCxms5m0tDR2797tVaasrEy79XFqair79u3rdfDI4OBgEhMTATCbzVx00UU4HI6A6uovkzu/yD1hhBCiB7rdMtnhcGC1WrXp6OhoKioqfMrExMS4AzGZCA0NpaGhAXDvJsvIyCAkJITs7GymT5/ute6JEyfYs2cPCxcu7LWukSNH9hhja2srdrs9oPY5O9oB+LzyC0a2fx1QHXppaWkJuF16k9gCI7EFRmILzEDFpluC8dd7UDoPjJ+pTFRUFHv27CEyMpIDBw6wbNkySkpKCAkJAaCjo4NVq1Zx0003ERsb2+ftdWexWIiPj+9zm7r65Mj7AMScO5b4+OiA6tCL3W4PuF16k9gCI7EFRmILzJli62vy0W0XmdVqpba2Vpt2OBxERUX5lKmpqQHcSaOxsZGIiAjMZjORkZEAJCQkEBcXx6FDh7T17r//fs477zwWLVp0xrr04jlNWa6DEUII/3RLMJMnT6ayspKqqira2tooKSkhOTnZq0xycjJbt24FoLS0lMTERBRFob6+HqfTCUBVVRWVlZVaTyU/P5+mpibuu+++PtWlF+0sMjnIL4QQfum2i8xkMpGTk8OSJUtwOp3Mnz+fSZMmUVBQQEJCAikpKWRmZrJmzRpsNhvh4eHk5+cDsH//fgoLCzEajRiNRh588EEiIiKora3l97//PRMmTNAO6C9YsICsrKwe69JLkJZgpAcjhBD+6JZgAJKSkkhKSvKat3LlSu2xxWKhsLDQZ73U1FRSU1N95lutVj7++GO/2+qpLr0YPbvIpAcjhBB+yZX8ATLJhZZCCNErSTABkhuOCSFE7yTBBOj0YJfSgxFCCH8kwQRIG+xSbjgmhBB+SYIJkGcsMrnhmBBC+CcJJkBGz1hk0oMRQgi/JMEESFEUTAaFDjkGI4QQfkmC6QeTUZFjMEII0QNJMP0QZDDIWWRCCNEDSTD9YDIqch2MEEL0QBJMPwQZpQcjhBA9kQTTD+4EIz0YIYTwRxJMP7gP8ksPRggh/JEE0w/u05SlByOEEP5IgukHOQYjhBA90zXBlJeXk5qais1mY/PmzT7L29rayM7OxmazkZWVRXV1NQDV1dVMmTKF9PR00tPTycnJ0dbJz88nKSmJadOmedX18ssvk5iYqK1TVFSkZ9MAuQ5GCCF6o9sNx5xOJ7m5uTz99NNER0eTmZlJcnIyEydO1MoUFRURFhbGzp07KSkpIS8vj02bNgEQFxdHcXGxT72zZ8/mxhtv9HtDsmuvvdYrGelNejBCCNEz3XowFRUVjBs3jtjYWMxmM2lpaezevdurTFlZmXbr49TUVPbt24eq9t4jmDp1KlFRUXqFfVbkQkshhOiZbj0Yh8OB1WrVpqOjo6moqPApExMT4w7EZCI0NJSGhgbAvZssIyODkJAQsrOzmT59+hm3uWPHDvbv38/48eO59957tbp70trait1uP9umAdDS0kJryynaXWrAdeilpaVlyMXkIbEFRmILjMQWmIGKTbcE468nonTeQ+VMZaKiotizZw+RkZEcOHCAZcuWUVJSQkhISI/bmz17NnPmzMFsNvPCCy9wzz338Mc//rHXGC0WC/Hx8X1skTe73U54WCjHm9sDrkMvdrt9yMXkIbEFRmILjMQWmDPF1tfko9suMqvVSm1trTbtcDh8dm1ZrVZqamoA6OjooLGxkYiICMxmM5GRkQAkJCQQFxfHoUOHet1eZGQkZrMZgOuuu47//Oc/A9kcv4JkNGUhhOiRbglm8uTJVFZWUlVVRVtbGyUlJSQnJ3uVSU5OZuvWrQCUlpaSmJiIoijU19fjdDoBqKqqorKyktjY2F63V1dXpz0uKyvj/PPPH+AW+QoyGuQ6GCGE6IFuu8hMJhM5OTksWbIEp9PJ/PnzmTRpEgUFBSQkJJCSkkJmZiZr1qzBZrMRHh5Ofn4+APv376ewsBCj0YjRaOTBBx8kIiICgEceeYTt27fT3NzMrFmzyMrKYvny5Tz33HOUlZVhNBoJDw9nw4YNejXtdBuNihzkF0KIHuiWYACSkpJISkrymrdy5UrtscViobCw0Ge91NRUv6chA6xdu5a1a9f6zF+9ejWrV6/uZ8RnJ8hooF2GihFCCL/kSv5+kKFihBCiZ5Jg+sEkoykLIUSP+pRgnn32WZqamlBVlfvuu4958+bxj3/8Q+/YhrwgGU1ZCCF61KcE89JLLxESEsI//vEP6uvr2bBhA4899pjesQ15QUYD7R2SYIQQwp8+JRjPBZF79+5l/vz5fO973zvjkC7fBSajQrsMdimEEH71KcEkJCRwyy23UF5ezlVXXUVTUxMGgxy+CTIY5EJLIYToQZ9OU16/fj12u53Y2FiCg4M5duwYDz30kN6xDXkmo4JLBZdLxWBQzryCEEJ8h/SpG/Lee+8xfvx4wsLCKC4u5v/8n/9DaGio3rENeUFG99Mn18IIIYSvPiWYBx54gODgYD766CO2bNnCueeeyz333KN3bEOeqbPXItfCCCGErz4lGJPJhKIo7Nq1i5///OcsXLiQkydP6h3b0FV/iJEf/V+COp89GS5GCCF89SnBjBgxgieeeIJXXnmFH/7whzidTjo6OvSObej6+mOiP/hfJh7ZBSAXWwohhB99SjD5+fmYzWYeeughRo8ejcPhYPHixXrHNnRNstEaNp6pB3+HEadcbCmEEH70KcGMHj2auXPn0tjYyJ49e7BYLGRkZOgd29BlMPL15KWEnawkw/CmHIMRQgg/+pRgXnvtNbKysvjb3/7G66+/rj3+Lmsc80OOhV9Etukl2ttaBjscIYQYcvp0Hczvf/97XnzxRUaNGgVAfX09ixYt4pprrtE1uCFNUfj44pVc/tZtOD78M1jvGOyIhBBiSOnzUDGe5AIQERHRp6FiysvLSU1NxWazsXnzZp/lbW1tZGdnY7PZyMrKorq6GoDq6mqmTJlCeno66enp5OTkaOvk5+eTlJTEtGnT+lSXno6dm8R+1wWMfHcTtDfrvj0hhPg26VOCueqqq1i8eDEvv/wyL7/8MkuXLmXWrFm9ruN0OsnNzWXLli2UlJSwfft2Dh486FWmqKiIsLAwdu7cyaJFi8jLy9OWxcXFUVxcTHFxMbm5udr82bNnU1RU5LO93urSS5DJQF779QSdcsC7T+m+PSGE+DbpU4K55557uO666/j444/56KOPuP7661mzZk2v61RUVDBu3DhiY2Mxm82kpaWxe/durzJlZWXMmzcPcN/Fct++fWfsGU2dOpWoqCif+YHU1V8mg4F31HhOxFwJO/4HnkiC0l/Bx3+Dk0d13bYQQgx1fb5lcm+3MfbH4XBgtVq16ejoaCoqKnzKxMTEuAMxmQgNDaWhoQFw7ybLyMggJCSE7Oxspk+ffsbt+atr5MiRPa7T2tqK3W7vc5u6amlp4auGKgDKzlvNlSNLGF73b4LfeQLDvv8FoMMcRntILK2hcTiDR+IyWFCNQagGM6rBBIoBVTGe/osCioKqdM37XeZ1/neXxT2NgoqCggtUF6gqlvY2qr56E9Vgcv/v3IanLIpyelpRgM56DQbA0DmPbjGcLq+ggqqC6kJB7azT2CVGxb0OdC47XU9HayufvNeglVG7tAOlM77uPPV1qddD7Trf6+/Za2lpCfj90CtVBdWpvX6B0C22ASCxBea7EFuvCWbatGkofj6sqqqiKAr//ve/e1zXX++he109lYmKimLPnj1ERkZy4MABli1bRklJCSEhIf3aXncWi4X4+Phey/TEbrdz/oRooIZRE6YwelKKe0F7M1Tvh5oKTPWfYTp6kOCj/z8cPgIdrYCc0vzNUHySne9jTzn3fKeqYjQYuyWyzmTqeQxdpjmdrBXFvVh1dfnvBFeH+7GHwQRGCxiDutTfpS6DETw/CFQnONvB5cTlbMdgNHkv93p/d8ZhOP1jwr1d1Xv7XdvvqUMxdLbJ88NBdcftau/863LXa7J4x95Zf1tbG+agILz08qOg13KKov140f5rywzd2q34r6PLa9XS0swwy7AuG1RPt1WLwfMDpzP5q05wOd1/VdX3edW2x+nprs+5V+xGMJrcr7shyOs5bzp1ipARPXyn+W0j3rF3b4dvJT3UCZiGQep6GDnB75p2u73X78a+Jp9eE8x7773Xp0r8sVqt1NbWatMOh8Nn15bVaqWmpgar1UpHRweNjY1ERESgKApmsxlw3yogLi6OQ4cOMXny5F63568uPfkdiywoGMbPcv/vTlXdXxjOVvdf1dX5AXaefkOiuqe7ruP5kvC86V1OvL7FYPg7AAAd8UlEQVQMULt8+Ax8fuhzJoyL7dxWm/uv9kXm+fB2+xCrnTG4OmPw2X6X8l225f5C8BNb13W1x1BbW4s1OrrL/C6xaF+GXT8YXdro9UXZNbZu7aJr29Ru9eAbX+f8Y0ePMGrkyG7lun1xdf/R4qnD5fR+ThTF/aWiGDu/XIzuMs620//dFXb58ujyPKquzvXc/xsajjEqMtJ/0tKexy5t1l6nbl/c2nOB9/ut65ekJ3aDyZ1MFKM72XS0don99BfrqeMnMIeH0y0g79fDb5Lp9h7wvBbd319dy6jOLs97t9e1+7YUhfbGRoZ1H5jX5znx81nSEnmXpOFyem/H56/nOe/yA8DzGXd2Jmu18/lW2zF0tEBbb8+Ln/erTy+9t/elnzo9On+46K3Pu8jO1uTJk6msrKSqqoro6GhKSkp87oKZnJzM1q1bmTZtGqWlpSQmJqIoCvX19YSHh2M0GqmqqqKyspLY2Nhet9dTXXryjKbc1texyBQFTGb3fx21HguCMYH1zPTWYLdjDbDXqLc6u51REttZq7HbiRiisVWf4Zf4YPpiCMc2UHRLMCaTiZycHJYsWYLT6WT+/PlMmjSJgoICEhISSElJITMzkzVr1mCz2QgPDyc/Px+A/fv3U1hYiNFoxGg08uCDD2q9kUceeYTt27fT3NzMrFmzyMrKYvny5T3WpSdPgpEr+YUQwpduCQYgKSmJpKQkr3krV67UHlssFgoLC33W6+2EgrVr17J27Vqf+T3VpSeTsXMXmYxFJoQQPuS+x/0Q1HnbaBlNWQghfEmC6QetByP3gxFCCB+SYPrBk2DaXdKDEUKI7iTB9IO58yB/e4f0YIQQojtJMP1g8pxFJgf5hRDChySYfvBcaCkH+YUQwpckmH6Q62CEEKJnkmD6wWhQUBTZRSaEEP5IgumnIKOh70PFCCHEd4gkmH4KMiiyi0wIIfyQBNNPJqNBLrQUQgg/JMH0U5BRkQsthRDCD0kw/WQySA9GCCH8kQTTTyajHIMRQgh/JMH0k1nOIhNCCL90TTDl5eWkpqZis9nYvHmzz/K2tjays7Ox2WxkZWVRXV0NQHV1NVOmTCE9PZ309HRycnK0dQ4cOMDcuXOx2Wz85je/Qe28Nejjjz/OzJkztXX27t2rZ9M00oMRQgj/dLvhmNPpJDc3l6effpro6GgyMzNJTk5m4sSJWpmioiLCwsLYuXMnJSUl5OXlsWnTJgDi4uIoLi72qfeBBx4gNzeXqVOncuutt1JeXq7d1GzRokUsXrxYryb5ZTIY5EJLIYTwQ7ceTEVFBePGjSM2Nhaz2UxaWhq7d+/2KlNWVsa8efMA910s9+3bp/VI/Kmrq6OpqYlp06ahKAoZGRk+dX7TgoyKjEUmhBB+6JZgHA4HVqtVm46OjsbhcPiUiYmJAcBkMhEaGkpDQwPg3k2WkZHBggULePfdd/3WabVavep8/vnnmTt3Lvfeey/Hjx/Xq2leTEbpwQghhD+67SLz1xNRFKVPZaKiotizZw+RkZEcOHCAZcuWUVJS0mudN9xwA3fccQeKolBQUMDDDz/Mhg0beo2xtbUVu91+Ns3StLS0YLfbaWtpprWFgOvRgye2oUhiC4zEFhiJLTADFZtuCcZqtVJbW6tNOxwOoqKifMrU1NRgtVrp6OigsbGRiIgIFEXBbDYDkJCQQFxcHIcOHfKps7a2VqvznHPO0eZnZWXxi1/84owxWiwW4uPjA2qf3W4nPj6eiLdO0NTaEXA9evDENhRJbIGR2AIjsQXmTLH1Nfnotots8uTJVFZWUlVVRVtbGyUlJSQnJ3uVSU5OZuvWrQCUlpaSmJiIoijU19fjdDoBqKqqorKyktjYWKKiohgxYgTvv/8+qqqybds2UlJSAPfxGY9du3YxadIkvZrmxSRjkQkhhF+69WBMJhM5OTksWbIEp9PJ/PnzmTRpEgUFBSQkJJCSkkJmZiZr1qzBZrMRHh5Ofn4+APv376ewsBCj0YjRaOTBBx8kIiICcJ9Fdu+999LS0sKsWbOYNWsWAI8++igfffQRAGPGjCE3N1evpnm302igXa6DEUIIH7olGICkpCTtFGKPlStXao8tFguFhYU+66WmppKamuq3zsmTJ7N9+3af+Y8++mg/ow1MkFGhQ8YiE0IIH3Ilfz/JWGRCCOGfJJh+Msl1MEII4ZckmH4yyzEYIYTwSxJMP5nkGIwQQvglCaafTAbpwQghhD+SYPopSEZTFkIIvyTB9JOMRSaEEP5JgumnIIP7LLLeRoEWQojvIkkw/RRkdD+FcqBfCCG8SYLpJ5MnwchxGCGE8CIJpp+CjO7bBbTLcRghhPAiCaafTAZ3gpEejBBCeJME00+nd5FJD0YIIbqSBNNP5s4E0yYJRgghvEiC6SeTUXaRCSGEP5Jg+knbRSYH+YUQwouuCaa8vJzU1FRsNhubN2/2Wd7W1kZ2djY2m42srCyqq6sBqK6uZsqUKaSnp5Oenk5OTo62zoEDB5g7dy42m43f/OY32gWOx44d4+abb+bqq6/m5ptv5vjx43o2TRPUeZBfhuwXQghvuiUYp9NJbm4uW7ZsoaSkhO3bt3Pw4EGvMkVFRYSFhbFz504WLVpEXl6etiwuLo7i4mKKi4u9bn/8wAMPkJuby44dO6isrKS8vByAzZs3M2PGDHbs2MGMGTP8JjQ9yHUwQgjhn24JpqKignHjxhEbG4vZbCYtLY3du3d7lSkrK2PevHmA+zbJ+/bt63XIlbq6Opqampg2bRqKopCRkaHVuXv3bjIyMgDIyMhg165dOrXMm0mugxFCCL9MelXscDiwWq3adHR0NBUVFT5lYmJi3IGYTISGhtLQ0AC4d5NlZGQQEhJCdnY206dP96nTarXicDgAOHr0KFFRUQBERUVRX19/xhhbW1ux2+0Bta+lpQW73U5NTTMABz87RPDJ2oDqGmie2IYiiS0wEltgJLbADFRsuiUYfz0RRVH6VCYqKoo9e/YQGRnJgQMHWLZsGSUlJX2q82xYLBbi4+MDWtdutxMfH88Jy1GghjGxccRPPCfgWAaSJ7ahSGILjMQWGIktMGeKra/JR7ddZFarldra07/oHQ6H1sPoWqampgaAjo4OGhsbiYiIwGw2ExkZCUBCQgJxcXEcOnTIp87a2lqtzlGjRlFXVwe4d6WNHDlSr6Z58RyDkZuOCSGEN90SzOTJk6msrKSqqoq2tjZKSkpITk72KpOcnMzWrVsBKC0tJTExEUVRqK+vx+l0AlBVVUVlZSWxsbFERUUxYsQI3n//fVRVZdu2baSkpGh1bdu2DcBrvt6C5DoYIYTwS7ddZCaTiZycHJYsWYLT6WT+/PlMmjSJgoICEhISSElJITMzkzVr1mCz2QgPDyc/Px+A/fv3U1hYiNFoxGg08uCDDxIREQG4zyK79957aWlpYdasWcyaNQuApUuXkp2dzYsvvkhMTAwFBQV6Nc27nQa5DkYIIfzRLcEAJCUlkZSU5DVv5cqV2mOLxUJhYaHPeqmpqaSmpvqtc/LkyWzfvt1nfmRkJM8++2w/Iz572mjK0oMRQggvciV/PwXJMRghhPBLEkw/yVhkQgjhnySYftJ6MHIMRgghvEiC6Se54ZgQQvgnCaaf5DoYIYTwTxJMP2nXwbikByOEEF1Jgukn7RhMh/RghBCiK0kw/eQ5BtMuPRghhPAiCaafFEXBZFDokGMwQgjhRRLMADAZFTkGI4QQ3UiCGQBBBoOcRSaEEN1IghkAJqMi18EIIUQ3kmAGQJBRejBCCNGdJJgB4E4w0oMRQoiuJMEMAPdBfunBCCFEV7ommPLyclJTU7HZbGzevNlneVtbG9nZ2dhsNrKysqiurvZa/tVXXzFt2jT+8Ic/aPOeffZZ5syZQ1paGs8884w2//HHH2fmzJmkp6eTnp7O3r17dWtXd+7TlKUHI4QQXemWYJxOJ7m5uWzZsoWSkhK2b9/OwYMHvcoUFRURFhbGzp07WbRoEXl5eV7LN2zYwMyZM7XpTz75hKKiIoqKiiguLuaNN96gsrJSW75o0SKKi4spLi72udGZnuQYjBBC+NItwVRUVDBu3DhiY2Mxm82kpaWxe/durzJlZWXMmzcPcN/Fct++faiquyewa9cuxo4dy6RJk7Tyn332GZdccgnBwcGYTCYuu+wydu7cqVcT+izIaJDrYIQQohvdbpnscDiwWq3adHR0NBUVFT5lYmJi3IGYTISGhtLQ0MCwYcN48skneeqpp3jqqae08hdccAGbNm3SypSXl5OQkKAtf/7559m2bRsJCQn88pe/JDw8vNcYW1tbsdvtAbWvpaVFW7e9rYWG420B1zXQusY21EhsgZHYAiOxBWagYtMtwXh6Il0pitKnMo8//jgLFy5kxIgRXsvOP/98lixZwi233MLw4cO58MILMRqNANxwww3ccccdKIpCQUEBDz/8MBs2bOg1RovFQnx8/Nk2DQC73a6tG7b3GAYDAdc10LrGNtRIbIGR2AIjsQXmTLH1NfnolmCsViu1tbXatMPhICoqyqdMTU0NVquVjo4OGhsbiYiI4IMPPqC0tJS8vDxOnDiBwWDAYrGwYMECsrKyyMrKAmDjxo1ER0cDcM4552j1ZmVl8Ytf/EKvpvkwGRXaZDRlIYTwoluCmTx5MpWVlVRVVREdHU1JSQmPPfaYV5nk5GS2bt3KtGnTKC0tJTExEUVR+NOf/qSVefzxxxk+fDgLFiwA4OjRo4waNYqvvvqKHTt28Je//AWAuro6LYHt2rXL69iN3kxGAyfbnN/Y9oQQ4ttAtwRjMpnIyclhyZIlOJ1O5s+fz6RJkygoKCAhIYGUlBQyMzNZs2YNNpuN8PBw8vPzz1jv8uXLOXbsGCaTiXXr1mnHWR599FE++ugjAMaMGUNubq5eTfMRJKMpCyGED90SDEBSUpLP6cIrV67UHlssFgoLC3utY/ny5V7TXXs3XT366KMBRtl/cpqyEEL4kiv5B4AMdimEEL4kwQyAIKOBdhkqRgghvEiCGQAyVIwQQviSBDMATDKashBC+JAEMwCCjIoc5BdCiG4kwQyAmPBgjje3c3fRBzS2tA92OEIIMSToepryd8WSmeM51dbB/7fnIG9/fpSN103lB+NHDnZYQggxqCTBDIAgo4HVV1/IDy8czV1/+YDrN+8jY+oY4mNCGTdqBOeNGkHcyOEEm42DHaoQQnxjJMEMoEvHjeS1lTPZ8Jqd1w/UsvW9w17LzwkxMyZyOLGRwYQHB2EyKBgNBoKMCgaDglE5/ddocA/8aVAUDAoYFAWl869BAaOhyzqKAu5/KIpCbU0jH7ccRuksp+Be170cQMFocG/DXa/SZRm4p9DWwbN9wGDwLD3NU6enfNdYuzt0tBXn4eOdZTvX6VK/p66uDAq9xugbi3d5f+3x1NF1/WPNTo42tfpdt0d+Fmvb8Ve88/Xz1KvifXJI93Z5YuhwqbR1uHD5HSDWty2e0LTt+XkthNCbJJgBFmIxsX7eZNbPm8zx5na+OHqSQ0dOUt3QTHXDKaobmjlw+DhNrR10uFQ6nCrtTvcXh9OlMnC3lfl6oCrSweEzFxk0Xwx2AL04pFvNZ5t/PMUVRUFVVRTFf2yny/VUT9cfR6d/lHjyaPcE3GMd3ed11ulSXRgNX/qM3O75EaGt6m8zfXxOPInc3w8Lzw84owGMnY892tvbMZtr/MStdPkhdZonxN6eG8+6nh9s/oJVgGCzkY3XTeWC6NC+NTJAkmB0FB4cxJSxEUwZG9HndVTVnWRcqopLVVFVcLrcbyWXqqK6wKklI1VLTNqbToVPDx5kwvnnd85316dyuoxLVXF11uPeRpf1OV2Pqrq3q3ZZv/svaPey02XpLOtyocXc9Y1eXV3NmLFju8R7ehsu1fcj4/licKnebfSOVfWapku8Xu2hWzu7taWmtpboaKtXTL3xt1iLRfX9UvV+HvH5EukxVlWlru5roqPdg7l6voS6l/HdFn5fM5/4u69L79+t3b/ovj5yhNFdRjM/XU71KtdTPa7OYFXA5VK9eqH0MRaveerp194zOG7Xerq/p7v3irvH3lvy7f4+7r5MRcXpcrfLqXo/H8eOH/O+Z1WXz1JPbeua2LtOn26X57vCX6ynP6MWk4ERFv2//iXBDDGKomBUwNjXn09+nKwLYvw5I85ccBDYDQ3Ex1vPXHAQ2O3NxMefN9hh+GW3dxAfP3Gww/DLbncRH3/hYIfh17f5niv/DeQ0ZSGEELqQBCOEEEIXkmCEEELoQtcEU15eTmpqKjabjc2bN/ssb2trIzs7G5vNRlZWFtXV1V7Lv/rqK6ZNm8Yf/vAHbd6zzz7LnDlzSEtL45lnntHmHzt2jJtvvpmrr76am2++mePHj+vWLiGEEGemW4JxOp3k5uayZcsWSkpK2L59OwcPHvQqU1RURFhYGDt37mTRokXk5eV5Ld+wYQMzZ87Upj/55BOKioooKiqiuLiYN954g8rKSgA2b97MjBkz2LFjBzNmzPCb0IQQQnxzdEswFRUVjBs3jtjYWMxmM2lpaezevdurTFlZGfPmzQMgNTWVffv2aafo7dq1i7FjxzJp0iSt/GeffcYll1xCcHAwJpOJyy67jJ07dwKwe/duMjIyAMjIyGDXrl16NU0IIUQf6HaassPhwGo9fTpqdHQ0FRUVPmViYmLcgZhMhIaG0tDQwLBhw3jyySd56qmneOqpp7TyF1xwAZs2bdLKlJeXk5CQALjPd4+Kcl8nEBUVRX19/RljbG1txW63B9S+lpaWgNfVm8QWGIktMBJbYL4LsemWYLpf+AW+w1X0VObxxx9n4cKFjBjhfS3H+eefz5IlS7jlllsYPnw4F154IUZj4ON7WSyWgM9DH8rnsEtsgZHYAiOxBebbHFtfk49uCcZqtVJbW6tNOxwOrYfRtUxNTQ1Wq5WOjg4aGxuJiIjggw8+oLS0lLy8PE6cOIHBYMBisbBgwQKysrLIysoCYOPGjURHRwMwatQo6urqiIqKoq6ujpEjzzyacX96MND3J3kwSGyBkdgCI7EF5tsaW2tra5/q0C3BTJ48mcrKSqqqqoiOjqakpITHHnvMq0xycjJbt25l2rRplJaWkpiYiKIo/OlPf9LKPP744wwfPpwFCxYAp4d++Oqrr9ixYwd/+ctftLq2bdvG0qVL2bZtGykpKWeMcerUqQPYYiGEEF3plmBMJhM5OTksWbIEp9PJ/PnzmTRpEgUFBSQkJJCSkkJmZiZr1qzBZrMRHh5Ofn7+Getdvnw5x44dw2QysW7dOm0sn6VLl5Kdnc2LL75ITEwMBQUFejVNCCFEHyiqvwMhQgghRD/JlfxCCCF0IQlGCCGELiTBCCGE0IUkGCGEELqQG44FoLy8nPXr1+NyucjKymLp0qWDFsu9997LG2+8wahRo9i+fTvgHvjzrrvu4vDhw4wZM4ZNmzZ53znvG1JTU8PatWs5cuQIBoOB6667joULFw6J+FpbW7nxxhtpa2vD6XSSmprKihUrqKqqYtWqVRw/fpyLLrqIRx55BLPZ/I3G5uE5+zI6OponnnhiyMSWnJzMiBEjMBgMGI1GXn755SHxmgKcOHGC//mf/+GTTz5BURQeeughxo8fP+ixff7559x1113adFVVFStWrCAjI2PQYwN45plnKCoqQlEULrjgAjZs2EBdXV3/32+qOCsdHR1qSkqK+uWXX6qtra3q3Llz1U8//XTQ4vnnP/+pHjhwQE1LS9Pm/fa3v1WfeOIJVVVV9YknnlAfeeSRQYnN4XCoBw4cUFVVVRsbG9Wrr75a/fTTT4dEfC6XS21qalJVVVXb2trUzMxM9b333lNXrFihbt++XVVVVb3//vvV559//huPzeOpp55SV61apS5dulRVVXXIxDZ79mz16NGjXvOGwmuqqqq6du1a9a9//auqqqra2tqqHj9+fMjE5tHR0aFeccUVanV19ZCIrba2Vp09e7ba3Nysqqr7ffbSSy8NyPtNdpGdpb4M4vlNuuyyy3x+8QyVgT+joqK4+OKLAQgJCWHChAk4HI4hEZ+iKNpQRB0dHXR0dKAoCm+//TapqakAzJs3b9Be29raWt544w0yMzMB97BKQyU2f4bCa9rU1MT+/fu158xsNhMWFjYkYutq3759xMbGMmbMmCETm9PppKWlhY6ODlpaWhg9evSAvN8kwZwlf4N4OhyOQYzIVyADf+qturoau93OJZdcMmTiczqdpKenc8UVV3DFFVcQGxtLWFgYJpN7z7HVah201/ahhx5izZo1GAzuj2hDQ8OQiQ1g8eLF/PSnP9VG0hgKr2lVVRUjR47k3nvvJSMjg1/96lecOnVqSMTWVUlJCXPmzAGGxvMWHR3NLbfcwuzZs7nqqqsICQnh4osvHpD3mySYs6T2YRBP4e3kyZOsWLGC++67j5CQkMEOR2M0GikuLmbv3r1UVFTw+eef+5QZjNd2z549jBw5UhspvCeD9b574YUX2Lp1K08++STPP/88+/fvH5Q4uuvo6ODDDz/khhtuYNu2bQQHBw+5+0K1tbVRVlbGNddcM9ihaI4fP87u3bvZvXs3f//732lubqa8vNynXCDvN0kwZ6kvg3gONs/An0CfB/7US3t7OytWrGDu3LlcffXVQy4+gLCwMC6//HLef/99Tpw4QUdHB+DeTTUYr+2///1vysrKSE5OZtWqVbz99tusX79+SMQGeA0wa7PZqKioGBKvqdVqxWq1cskllwBwzTXX8OGHHw6J2DzKy8u5+OKLOeecc4Ch8Vl46623GDt2LCNHjiQoKIirr76a9957b0Deb5JgzlLXQTzb2tooKSkhOTl5sMPy4hn4E+jzwJ96UFWVX/3qV0yYMIGbb755SMVXX1/PiRMnAPe9L9566y3OP/98Lr/8ckpLSwHYunXroLy2q1evpry8nLKyMjZu3EhiYiKPPfbYkIjt1KlTNDU1aY/ffPNNJk2aNCRe09GjR2O1WrWe6L59+zj//POHRGweJSUlpKWladNDIbZzzz2XDz74gObmZlRVZd++fUycOHFA3m8yFlkA9u7dy0MPPaSdRnr77bcPWiyrVq3in//8Jw0NDYwaNYrly5fzox/9iOzsbGpqarSBPyMiIr7x2N59911uvPFGLrjgAu1YwqpVq5gyZcqgx/fRRx/xy1/+EqfTiaqqXHPNNdx5551UVVVx1113cfz4ceLj48nLyxu005QB3nnnHZ566intNOXBjq2qqoply5YB7mNYc+bM4fbbb6ehoWHQX1NwDzH/q1/9ivb2dmJjY9mwYQMul2tIxNbc3MwPf/hDdu3aRWhoKMCQed4KCwt57bXXMJlMxMfHs379ehwOR7/fb5JghBBC6EJ2kQkhhNCFJBghhBC6kAQjhBBCF5JghBBC6EISjBBCCF1IghHiW+qdd97htttuG+wwhOiRJBghhBC6kPvBCKGz4uJinnvuOdrb27nkkktYt24d06dP5/rrr+edd94hLCyM/Px8Ro4cid1uZ926dTQ3NxMXF8dDDz1EeHg4X3zxBevWraO+vh6j0UhBQQHgvpp+xYoVfPLJJ1x88cXk5eXJ2HhiyJAejBA6+uyzz3j99dd54YUXKC4uxmAw8Oqrr3Lq1Ckuuugitm7dymWXXcb//u//ArB27VruvvtuXn31VS644AJt/t13382NN97IK6+8wp///GdGjx4NwIcffsh9993Ha6+9RnV1Nf/6178Gra1CdCcJRggd7du3jwMHDpCZmUl6ejr79u2jqqoKg8HAtddeC0B6ejr/+te/aGxspLGxkR/84AeA+x4c7777Lk1NTTgcDmw2GwAWi4Xg4GAApkyZgtVqxWAw8L3vfY/Dhw8PTkOF8EN2kQmhI1VVmTdvHqtXr/aa/7vf/c5rOtDdWl3HhjIajTidzoDqEUIP0oMRQkczZsygtLSUo0ePAnDs2DEOHz6My+XSRqp99dVXufTSSwkNDSUsLIx3330XcB+7ueyyywgJCcFqtWp3O2xra6O5uXlwGiTEWZAejBA6mjhxItnZ2dxyyy24XC6CgoLIyclh+PDhfPrpp/z0pz8lJCSETZs2AfDb3/5WO8jvGQ0Y4JFHHiEnJ4eCggKCgoK0g/xCDGUymrIQg2DatGm89957gx2GELqSXWRCCCF0IT0YIYQQupAejBBCCF1IghFCCKELSTBCCCF0IQlGCCGELiTBCCGE0MX/A1S6E/vWLtmBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_ae_sigmoid_adam_mse  = plot_hist_auto(hist_ae_sigmoid_adam_mse, './Figures/hist_ae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- SPAE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 140)               29540     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 210)               29610     \n",
      "=================================================================\n",
      "Total params: 59,150\n",
      "Trainable params: 59,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "spae_sigmoid_adam_mse,enc_train_x_spsam,enc_test_x_spsam = spae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spae_sigmoid_adam_mse = load_model('spae_sigmoid_adam_mse_redds20bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 18:09:28 2019\n",
      "Train on 1330452 samples, validate on 332613 samples\n",
      "Epoch 1/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.1342 - acc: 7.9672e-05 - val_loss: 0.0994 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09935, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0962 - acc: 3.5326e-05 - val_loss: 0.0976 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09935 to 0.09765, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0954 - acc: 3.0817e-05 - val_loss: 0.0950 - val_acc: 2.7058e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09765 to 0.09505, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0899 - acc: 1.5784e-05 - val_loss: 0.0902 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09505 to 0.09016, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0878 - acc: 1.9542e-05 - val_loss: 0.0891 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09016 to 0.08911, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0870 - acc: 2.2549e-05 - val_loss: 0.0885 - val_acc: 3.3071e-05\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08911 to 0.08855, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0866 - acc: 2.5555e-05 - val_loss: 0.0882 - val_acc: 3.3071e-05\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08855 to 0.08820, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0863 - acc: 3.0065e-05 - val_loss: 0.0880 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.08820 to 0.08797, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0861 - acc: 3.3071e-05 - val_loss: 0.0878 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08797 to 0.08780, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0859 - acc: 3.4575e-05 - val_loss: 0.0877 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08780 to 0.08767, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0858 - acc: 3.6830e-05 - val_loss: 0.0876 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08767 to 0.08757, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0857 - acc: 3.7581e-05 - val_loss: 0.0875 - val_acc: 4.5097e-05\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08757 to 0.08749, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0857 - acc: 4.5849e-05 - val_loss: 0.0874 - val_acc: 4.5097e-05\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08749 to 0.08742, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0856 - acc: 1.2778e-04 - val_loss: 0.0874 - val_acc: 3.6379e-04\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08742 to 0.08737, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0855 - acc: 0.0015 - val_loss: 0.0873 - val_acc: 0.0038\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08737 to 0.08732, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 16/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0855 - acc: 0.0113 - val_loss: 0.0873 - val_acc: 0.0307\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08732 to 0.08728, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0855 - acc: 0.0197 - val_loss: 0.0872 - val_acc: 0.0345\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.08728 to 0.08724, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0854 - acc: 0.0234 - val_loss: 0.0872 - val_acc: 0.0382\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08724 to 0.08721, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 19/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0854 - acc: 0.0268 - val_loss: 0.0872 - val_acc: 0.0414\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08721 to 0.08719, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0854 - acc: 0.0329 - val_loss: 0.0872 - val_acc: 0.0430\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08719 to 0.08716, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0854 - acc: 0.0368 - val_loss: 0.0871 - val_acc: 0.0474\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08716 to 0.08713, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 22/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0853 - acc: 0.0389 - val_loss: 0.0871 - val_acc: 0.0520\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08713 to 0.08711, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 23/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0853 - acc: 0.0426 - val_loss: 0.0871 - val_acc: 0.0581\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08711 to 0.08710, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 24/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0853 - acc: 0.0424 - val_loss: 0.0871 - val_acc: 0.0535\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.08710 to 0.08709, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 25/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0853 - acc: 0.0441 - val_loss: 0.0871 - val_acc: 0.0547\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.08709 to 0.08707, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 26/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0853 - acc: 0.0454 - val_loss: 0.0871 - val_acc: 0.0570\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.08707 to 0.08706, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 27/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0853 - acc: 0.0479 - val_loss: 0.0871 - val_acc: 0.0702\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08706 to 0.08705, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 28/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0498 - val_loss: 0.0870 - val_acc: 0.0590\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.08705 to 0.08703, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 29/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0510 - val_loss: 0.0870 - val_acc: 0.0606\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08703 to 0.08702, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 30/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0525 - val_loss: 0.0870 - val_acc: 0.0620\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.08702 to 0.08701, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 31/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0536 - val_loss: 0.0870 - val_acc: 0.0655\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.08701 to 0.08700, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 32/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0544 - val_loss: 0.0870 - val_acc: 0.0633\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.08700 to 0.08699, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 33/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0560 - val_loss: 0.0870 - val_acc: 0.0635\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.08699 to 0.08698, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 34/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0570 - val_loss: 0.0870 - val_acc: 0.0660\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.08698 to 0.08698, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 35/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0575 - val_loss: 0.0870 - val_acc: 0.0687\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.08698 to 0.08697, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 36/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0582 - val_loss: 0.0870 - val_acc: 0.0686\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.08697 to 0.08696, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 37/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0584 - val_loss: 0.0870 - val_acc: 0.0754\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.08696 to 0.08696, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 38/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0592 - val_loss: 0.0870 - val_acc: 0.0692\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.08696 to 0.08695, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 39/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0593 - val_loss: 0.0869 - val_acc: 0.0731\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08695 to 0.08695, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 40/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0852 - acc: 0.0582 - val_loss: 0.0869 - val_acc: 0.0685\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.08695 to 0.08694, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 41/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0608 - val_loss: 0.0869 - val_acc: 0.0681\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.08694 to 0.08693, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 42/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0611 - val_loss: 0.0869 - val_acc: 0.0677\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.08693 to 0.08693, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 43/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0614 - val_loss: 0.0869 - val_acc: 0.0732\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.08693 to 0.08692, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 44/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0617 - val_loss: 0.0869 - val_acc: 0.0779\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.08692\n",
      "Epoch 45/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0606 - val_loss: 0.0869 - val_acc: 0.0661\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.08692 to 0.08692, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 46/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0616 - val_loss: 0.0869 - val_acc: 0.0726\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.08692 to 0.08691, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 47/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0618 - val_loss: 0.0869 - val_acc: 0.0770\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.08691 to 0.08691, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 48/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0621 - val_loss: 0.0869 - val_acc: 0.0727\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.08691 to 0.08690, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 49/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0635 - val_loss: 0.0869 - val_acc: 0.0691\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.08690 to 0.08690, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 50/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0630 - val_loss: 0.0869 - val_acc: 0.0699\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.08690 to 0.08690, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 51/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0634 - val_loss: 0.0869 - val_acc: 0.0656\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.08690\n",
      "Epoch 52/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0638 - val_loss: 0.0869 - val_acc: 0.0714\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.08690 to 0.08689, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 53/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0623 - val_loss: 0.0869 - val_acc: 0.0716\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.08689 to 0.08689, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 54/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0618 - val_loss: 0.0869 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.08689 to 0.08688, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 55/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0639 - val_loss: 0.0869 - val_acc: 0.0730\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.08688 to 0.08688, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 56/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0629 - val_loss: 0.0869 - val_acc: 0.0765\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.08688 to 0.08688, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 57/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0618 - val_loss: 0.0869 - val_acc: 0.0700\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.08688 to 0.08688, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 58/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0610 - val_loss: 0.0869 - val_acc: 0.0739\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.08688 to 0.08688, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 59/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0624 - val_loss: 0.0869 - val_acc: 0.0642\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.08688 to 0.08687, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 60/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0851 - acc: 0.0607 - val_loss: 0.0869 - val_acc: 0.0765\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.08687 to 0.08687, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 61/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0851 - acc: 0.0618 - val_loss: 0.0869 - val_acc: 0.0813\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.08687\n",
      "Epoch 62/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0851 - acc: 0.0614 - val_loss: 0.0869 - val_acc: 0.0709\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.08687 to 0.08687, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 63/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0621 - val_loss: 0.0869 - val_acc: 0.0569\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.08687\n",
      "Epoch 64/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0605 - val_loss: 0.0869 - val_acc: 0.0734\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.08687 to 0.08686, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 65/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0610 - val_loss: 0.0869 - val_acc: 0.0769\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.08686 to 0.08686, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 66/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0604 - val_loss: 0.0869 - val_acc: 0.0521\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.08686\n",
      "Epoch 67/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0581 - val_loss: 0.0869 - val_acc: 0.0806\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.08686\n",
      "Epoch 68/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0605 - val_loss: 0.0869 - val_acc: 0.0808\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.08686\n",
      "Epoch 69/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0592 - val_loss: 0.0869 - val_acc: 0.0696\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.08686 to 0.08685, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 70/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0851 - acc: 0.0616 - val_loss: 0.0869 - val_acc: 0.0523\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.08685\n",
      "Epoch 71/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0603 - val_loss: 0.0869 - val_acc: 0.0710\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.08685\n",
      "Epoch 72/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0613 - val_loss: 0.0869 - val_acc: 0.0785\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.08685\n",
      "Epoch 73/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0607 - val_loss: 0.0868 - val_acc: 0.0655\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.08685 to 0.08685, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 74/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0586 - val_loss: 0.0868 - val_acc: 0.0526\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.08685 to 0.08684, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 75/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0851 - acc: 0.0592 - val_loss: 0.0868 - val_acc: 0.0519\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.08684 to 0.08684, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 76/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0589 - val_loss: 0.0868 - val_acc: 0.0731\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.08684\n",
      "Epoch 77/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0576 - val_loss: 0.0868 - val_acc: 0.0546\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.08684 to 0.08684, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 78/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0851 - acc: 0.0569 - val_loss: 0.0868 - val_acc: 0.0784\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.08684\n",
      "Epoch 79/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0851 - acc: 0.0559 - val_loss: 0.0868 - val_acc: 0.0774\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.08684\n",
      "Epoch 80/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0553 - val_loss: 0.0868 - val_acc: 0.0812\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.08684\n",
      "Epoch 81/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0550 - val_loss: 0.0868 - val_acc: 0.0814\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.08684\n",
      "Epoch 82/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0851 - acc: 0.0556 - val_loss: 0.0868 - val_acc: 0.0673\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.08684\n",
      "Time elapsed (hh:mm:ss.ms) 0:20:45.746159\n"
     ]
    }
   ],
   "source": [
    "hist_spae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/spae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = spae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.08505774986883327\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8U3W+//FX1q6U2tINWlkEZRd3VJSxDiIULAjobK6gXEcG3AaveAfuw7mAMm7MzHVG9OrMz+u4CwhFmBGVgiKC4vQyxhFhgBZoEApdoE2b5Pz+SJomXegCaaJ9Px+PPJKcLZ8s7Tvf8/2eE5NhGAYiIiInYY50ASIiEv0UFiIi0iqFhYiItEphISIirVJYiIhIqxQWIiLSKoWFyCkoKSnhnHPOwe12t7rs22+/zY9//ONT3o5IJCgspMvIzc1l6NChlJWVhUzPz8/nnHPOoaSkJEKViUQ/hYV0Kb169aKgoCBw/5///Cc1NTURrEjku0FhIV1Kfn4+K1asCNxfsWIFkyZNClmmsrKSuXPnMnLkSK666iqeeeYZvF4vAB6Ph8cee4xLLrmEq6++mg0bNjRZd968eYwaNYorrriCp556Co/H0+46nU4n//Zv/8bFF1/MmDFjeP311wPzioqKuP766zn//PO57LLLWLx4MQAul4sHHniASy65hAsvvJApU6Zw+PDhdj+2SHOskS5ApDONGDGClStXsmvXLvr06cOaNWv4y1/+wtNPPx1Y5te//jWVlZW89957HDt2jOnTp5OWlsa0adN4/fXX+eCDD1ixYgVxcXH84he/CNn+gw8+SI8ePfjrX/9KdXU1M2fOJCsrix/96EftqvP++++nf//+bNy4kd27d3PbbbeRk5PDpZdeysKFC7n55puZNGkSx48fZ+fOnQAsX76cqqoqPvzwQ+x2Ow6Hg9jY2FN/0URQy0K6oPrWxUcffUS/fv3IyMgIzPN4PKxZs4b777+fxMREsrOzue2223jnnXcAePfdd7nlllvIysoiOTmZmTNnBtY9fPgwhYWFzJs3j/j4eFJTU7n11ltDdnu1xcGDB/nss8944IEHiImJYdCgQUybNo2VK1cCYLVa2bdvH2VlZSQkJDBixIjA9GPHjrF3714sFgtDhw4lMTHxVF8uEUAtC+mC8vPz+dnPfkZJSQn5+fkh844ePUpdXR09e/YMTOvZsydOpxOAQ4cOkZWVFTKv3oEDB3C73YwaNSowzev1hizfFocOHaJ79+4h/+h79uzJjh07AFi4cCG//e1vGTduHNnZ2cyaNYurrrqK/Px8SktLue+++6ioqOC6667j3nvvxWaztevxRZqjsJAup1evXmRnZ7NhwwYWLlwYMu+MM87AZrNx4MAB+vfvD/i+6de3PtLS0jh48GBg+eDbmZmZ2O12PvnkE6zWjv9ppaenU15eTlVVVSAwgmvo06cPTz75JF6vl7/+9a/Mnj2bLVu2EB8fz6xZs5g1axYlJSXceeed9O3bl2nTpnW4FpF62g0lXdLChQv585//THx8fMh0i8XCtddey1NPPUVVVRX79+/nxRdf5LrrrgNg3LhxvPTSS5SWllJeXs6yZcsC66anp3P55Zfz6KOPUlVVhdfrZd++fXz66aftqi0rK4vzzjuPJ598EpfLxVdffcWbb77JxIkTAVi5ciVlZWWYzWaSkpICdX/yySf885//xOPxkJiYiNVqxWKxnMrLJBKgloV0SWeeeWaL8371q1/x61//mh/+8IfExMQwbdo0pkyZAsANN9zAnj17yM/PJyEhgenTp/PJJ58E1l2yZAmPP/4448eP5/jx4+Tk5HDHHXe0u74nn3ySBQsWcMUVV5CUlMQvfvELLr/8cgA2btzIo48+Sk1NDT179uSpp54iJiaGw4cPs2DBApxOJ/Hx8YwfPz4QciKnyqQfPxIRkdZoN5SIiLRKYSEiIq1SWIiISKsUFiIi0qrvzWioL774gpiYmA6v73K5Tmn9cIjGmiA664rGmkB1tUc01gTRWdfprMnlcgXOAnAy35uwqD8tQkc5HI5TWj8corEmiM66orEmUF3tEY01QXTWdTprcjgcbVpOu6FERKRVCgsREWmVwkJERFr1vemzEBHpiLq6OkpKSlr8xcS6uro279fvLB2pKTY2luzs7A6fhVhhISJdWklJCd26daNPnz6YTKYm86urq4mLi4tAZS1rb02GYXDkyBFKSkro27dvhx5Tu6FEpEurqakhNTW12aD4vjCZTKSmpp7S780rLESky/s+B0W9U32OCgtgddEBKl2eSJchIhK1unxYlFfXMesv29nwr+ORLkVEuqCKigpefvnldq93xx13UFFREYaKmtflw8Lsb5m5PN7IFiIiXVJFRQWvvPJKk+kez8n3djz33HOBX0rsDF1+NJTN4stLt1e/ASUine+JJ55g37595OfnY7VaiY+PJz09HYfDwZo1a/j5z39OaWkpLpeLm2++mRtvvBGA3Nxc3nzzTU6cOMEdd9zBBRdcwPbt28nIyOCZZ54hNjb2tNbZ5cPCXh8W6rIQ6fLe+qyE17cVh0zzer2YzR3fCXPDhTlMuSC7xfn3338/O3fuZOXKlWzZsoWZM2eyatUqcnJyAFi0aBHJycnU1NQwdepUrrnmmiZBsHfvXp588kn+67/+izlz5rBu3Try8/M7XHNzunxYmM0mrGYTdWpZiEgUGDZsWCAoAF566SX+9re/AXDw4EH27t3LOeecE7JOdnZ24MSCQ4YMYf/+/ae9ri4fFuDbFVXnUViIdHVTLshu0gro7IPy4uPjA7e3bNnCxx9/zGuvvUZcXBw33XQTLperyTp2uz1w22KxNLvMqeryHdwAdqtZfRYiEhEJCQkcP978aMzKykq6d+9OXFwcu3bt4osvvujk6hqoZYGvZaGwEJFIOOOMMzj//POZMGECMTEx9OjRIzDvyiuv5NVXX2XixIn07du3TT9SFC4KC8BuMWk3lIhEzBNPPNHsdLvdzvPPP99kenV1Ne+//z4AKSkprF69OjBv+vTpYalRu6Hw7Yaq02EWIiItUlig3VAiIq1RWKDRUCIirVFYADaNhhIROSmFBRCj3VAiIicV1rAoLCxk7NixjBkzhmXLljWZv3XrViZPnszgwYNZu3ZtYPr+/fu5/vrryc/PJy8vr9mTbJ1ONqtGQ4mInEzYwsLj8fDII4/w/PPPU1BQwOrVq/nmm29ClsnKymLx4sVMmDAhZHpaWhqvvvoqK1eu5PXXX+e5557D6XSGq1Rfn4VGQ4lIBHT0FOUAf/rTn6iurj7NFTUvbGFRVFRE7969ycnJwW63k5eXx/r160OWyc7OZuDAgU1O0mW32wOHr9fW1uL1hvc/uV27oUQkQlo6RXlb/L//9/86LSzCdlCe0+kkMzMzcD8jI4OioqI2r3/w4EHuvPNO9u3bx9y5c8nIyDjp8i6XC4fD0aFaa05UUev2dnj9cKmpqYm6miA664rGmkB1tUekaqqrqzvpP1zDMML6D3nJkiXs27ePiRMnMnLkSFJSUvjrX/9KXV0dV111FT//+c+prq7ml7/8JYcOHcLj8XDHHXdQVlaG0+nkpptuIjk5udmD9xqrq6vr8GsctrAwjKbf1NvzG7BZWVmsWrUKp9PJ3XffzdixY0MOg28sJiYmcNbF9kotcvH1YWeH1w8Xh8MRdTVBdNYVjTWB6mqPSNXkcDgaThT4xSuw/X9D5nu8HixmS8cf4LyfwYgftzh77ty57N69m1WrVrFp0ybWrVvH22+/jWEY3HXXXezYsYOysjKysrL4n//5HwAOHTpEeno6L7/8Mi+99BIpKSltKsVmszV5jdsaHmHbDZWZmUlpaWngvtPpJD09vd3bycjIYMCAAWzbtu10lhfCbjHrFOUiEnEfffQRH330EZMmTWLy5Mns3r2bPXv2cPbZZ/Pxxx/zm9/8hm3bttGtW7dOry1sLYthw4axZ88eiouLycjIoKCgoMXznzRWWlpKcnIysbGxlJeX8/nnn3PrrbeGq1RsVpP6LETE1wJo1Aqo7cRTlBuGwZ133smPfvSjJvPefvttNmzYwBNPPMEll1zCPffc0yk11QtbWFitVubPn8+MGTPweDxMmTKFAQMGsHTpUoYOHcrVV19NUVERs2bNoqKigg8++IDf/e53FBQUsGvXLh599FFMJhOGYXD77bc3+bGP00lHcItIpASfonzUqFEsXbqUiRMnkpCQgNPpxGq14na7SU5OJj8/n4SEBN54442Qddu6G+pUhPWss6NHj2b06NEh0+bMmRO4PXz4cAoLC5usd/nll7Nq1apwlhbC93sWnfZwIiIBwacov+KKK5gwYUKgZREfH89vfvMb9u7dy5IlSzCbzVitVh566CEAbrjhBu644w7S0tJ46aWXwlqnTlGOhs6KSGQ13kV/yy23hNw/88wzueKKKwL360dn3XTTTdx0003hLxCd7gPw7YbyGuD2qHkhItIchQW+sADUbyEi0gKFBb4+C4BatSxEuqTmjgv7vjnV56iwwPezqgC16uUW6XJiY2M5cuTI9zowDMPgyJEjxMbGdngb6uAmeDeUwkKkq8nOzqakpIRvv/222fl1dXXYbLZOrurkOlJTbGws2dnZHX5MhQUNu6EUFiJdj81mo2/fvi3O16lRfLQbCrUsRERao7CgISxc6rMQEWmWwgKIsWrorIjIySgs0G4oEZHWKCwAm4bOioiclMICsOmgPBGRk1JY4DuRIECdWhYiIs1SWKDTfYiItEZhgTq4RURao7Ag6Ahut4bOiog0R2FB0GgotSxERJqlsKChg1tDZ0VEmqewQCcSFBFpjcICdXCLiLRGYQFYzTqCW0TkZBQWgMlkwmqGWp1IUESkWQoLP5vFpN1QIiItUFj42cwm7YYSEWmBwsLPalbLQkSkJQoLP5vFpIPyRERaoLDw87Us1MEtItIchYWfzQy1bk+kyxARiUoKCz/faCi1LEREmqOw8FMHt4hIy8IaFoWFhYwdO5YxY8awbNmyJvO3bt3K5MmTGTx4MGvXrg1Mdzgc3HjjjeTl5TFx4kTWrFkTzjIBX1i4NHRWRKRZ1nBt2OPx8Mgjj/Diiy+SkZHB1KlTyc3NpX///oFlsrKyWLx4MS+88ELIurGxsTz22GP06dMHp9PJlClTGDVqFElJSeEqVy0LEZGTCFtYFBUV0bt3b3JycgDIy8tj/fr1IWGRnZ0NgNkc2sDp27dv4HZGRgYpKSmUlZWFNSxsFhMnFBYiIs0KW1g4nU4yMzMD9zMyMigqKmr3doqKiqirq+PMM8886XIulwuHw9Hu7dcz46XyePUpbeN0q6mpiap66kVjXdFYE6iu9ojGmiA664pETWELC8NoOrLIZDK1axuHDh3il7/8JY899liT1kdjMTExDBo0qF3bD1l/gxOz1XJK2zjdHA5HVNVTLxrrisaaQHW1RzTWBNFZ1+msqa2hE7YO7szMTEpLSwP3nU4n6enpbV6/qqqKmTNncs899zBixIhwlBjCZtG5oUREWhK2sBg2bBh79uyhuLiY2tpaCgoKyM3NbdO6tbW13H333eTn5zNu3LhwlRhCHdwiIi0L224oq9XK/PnzmTFjBh6PhylTpjBgwACWLl3K0KFDufrqqykqKmLWrFlUVFTwwQcf8Lvf/Y6CggLeffddtm3bxrFjx1i+fDkAjz76aFibgjazzg0lItKSsIUFwOjRoxk9enTItDlz5gRuDx8+nMLCwibr5efnk5+fH87SmrBZTNRpN5SISLN0BLefTiQoItIyhYWf1QK1Hm+zo7hERLo6hYWfzewb1qvWhYhIUwoLP2sgLNRvISLSmMLCz2bxhYWOtRARaUph4aeWhYhIyxQWfvV9FjrWQkSkKYWFn9Xiu9ZuKBGRphQWfhoNJSLSMoWFn019FiIiLVJY+Fkt6rMQEWmJwsKvfjSU+ixERJpSWPjVH2eh3VAiIk0pLPys/ldCYSEi0pTCws+m3VAiIi1SWPgF+iw0dFZEpAmFhV+gz0ItCxGRJhQWflad7kNEpEUKCz8dlCci0jKFhZ9OUS4i0jKFhV/D0Fl1cIuINKaw8NMR3CIiLVNY+FnMJixmk/osRESaobAIYrMoLEREmqOwCGKzmHFpN5SISBMKiyB2i1ktCxGRZigsgtitCgsRkeYoLILYLGaNhhIRaYbCIoivg1vHWYiINKawCGK3WnRuKBGRZrQpLP785z9TVVWFYRjMmzePyZMns2nTpnDX1unsGjorItKsNoXFW2+9RWJiIps2baKsrIzFixfzxBNPtLpeYWEhY8eOZcyYMSxbtqzJ/K1btzJ58mQGDx7M2rVrQ+ZNnz6dCy+8kJkzZ7bxqZw69VmIiDSvTWFhGL79+Bs2bGDKlCkMHDgwMK0lHo+HRx55hOeff56CggJWr17NN998E7JMVlYWixcvZsKECU3WnzFjBkuWLGnr8zgtNBpKRKR5bQqLoUOHcvvtt1NYWMioUaOoqqrCbD75qkVFRfTu3ZucnBzsdjt5eXmsX78+ZJns7GwGDhzY7LYuvfRSEhIS2vFUTp3NYtYv5YmINMPaloUWLlyIw+EgJyeHuLg4jh07xqJFi066jtPpJDMzM3A/IyODoqKiU6s2zLQbSkSkeW0Ki+3btzNo0CDi4+NZuXIlX375JTfffPNJ12luN5XJZOpYlW3gcrlwOBwdXr+mpoaaE1VUnag9pe2cTjU1NVFTS7BorCsaawLV1R7RWBNEZ12RqKlNYfGf//mfvPPOO3z11Vc8//zzTJ06lQcffJD//d//bXGdzMxMSktLA/edTifp6emnXnELYmJiGDRoUIfXdzgc9DgDiquOndJ2TieHwxE1tQSLxrqisSZQXe0RjTVBdNZ1Omtqa+i0qc/CarViMpl47733uPnmm7nllls4fvz4SdcZNmwYe/bsobi4mNraWgoKCsjNzW1TUZGi3VAiIs1rU1gkJCTw7LPP8s477/CDH/wAj8eD2+0+6TpWq5X58+czY8YMxo8fz7hx4xgwYABLly4NdHQXFRVx5ZVXsnbtWhYsWEBeXl5g/Z/85CfMmTOHzZs3c+WVV7Jx48ZTeJptY9NoKBGRZrVpN9RTTz3F6tWrWbRoEWlpaRw4cIDp06e3ut7o0aMZPXp0yLQ5c+YEbg8fPpzCwsJm1/3LX/7SltJOK7taFiIizWpTyyItLY2JEydSWVnJBx98QExMDJMmTQp3bZ3Od5yFhs6KiDTWprBYs2YN06ZNY+3atbz77ruB2983NotJ54YSEWlGm3ZD/fGPf+TNN98kNTUVgLKyMm699VauvfbasBbX2ewWCx6vgcdrYDGHb5iviMh3TZtP91EfFADJycmtnu7ju8hm9QWEOrlFREK1qWUxatQopk+fHhittGbNGq688sqwFhYJdosvO2s9XmJtlghXIyISPdoUFg8++CDr1q3j888/xzAMbrzxRsaMGRPu2jqd3eoLizqNiBIRCdGmsAAYO3YsY8eODWctEWfztyw0IkpEJNRJw+K8885r9nxOhmFgMpn4/PPPw1ZYJNSHhY61EBEJddKw2L59e2fVERVsFl8wavisiEgo/QZ3kJj6PguFhYhICIVFEO2GEhFpnsIiSEMHt8JCRCSYwiJI/dBZ9VmIiIRSWATR0FkRkeYpLILY1WchItIshUUQu0ZDiYg0S2ERpP44C4WFiEgohUWQ+j4Ll3ZDiYiEUFgE0W4oEZHmKSyC1Hdw66yzIiKhFBZBbDrOQkSkWQqLIA0d3DrOQkQkmMIiiI6zEBFpnsIiiMlkwmYxqYNbRKQRhUUjNotZLQsRkUYUFo3YrWa1LEREGlFYNGKzmKlVB7eISAiFRSN27YYSEWlCYdGIOrhFRJpSWDSiPgsRkaYUFo1oNJSISFMKi0Z8HdwKCxGRYGENi8LCQsaOHcuYMWNYtmxZk/lbt25l8uTJDB48mLVr14bMW758Oddccw3XXHMNy5cvD2eZIbQbSkSkKWu4NuzxeHjkkUd48cUXycjIYOrUqeTm5tK/f//AMllZWSxevJgXXnghZN1jx47x+9//nrfeeguTycT1119Pbm4u3bt3D1e5AXaLmeo6T9gfR0TkuyRsLYuioiJ69+5NTk4OdrudvLw81q9fH7JMdnY2AwcOxGwOLWPTpk1cfvnlJCcn0717dy6//HI2btwYrlJD2Cwm9VmIiDQStpaF0+kkMzMzcD8jI4OioqIOr+t0Ok+6jsvlwuFwdKxYoKamBofDgav6OJUn3Ke0rdOlvqZoE411RWNNoLraIxprguisKxI1hS0sDKPpUdAmkyls68bExDBo0KC2FdfY/s/4ymNl4KBBpGyvprS6ouPbOo0cDkdU1NFYNNYVjTWB6mqPaKwJorOu01lTW0MnbLuhMjMzKS0tDdx3Op2kp6eHfd0OeWEcGdufBnQEt4hIc8IWFsOGDWPPnj0UFxdTW1tLQUEBubm5bVp31KhRbNq0ifLycsrLy9m0aROjRo0KV6lw/s0k714Jh3dis2g0lIhIY2ELC6vVyvz585kxYwbjx49n3LhxDBgwgKVLlwY6uouKirjyyitZu3YtCxYsIC8vD4Dk5GR+/vOfM3XqVKZOncrdd99NcnJyuEqF0Q/itcTAe//pHzqrEwmKiAQLW58FwOjRoxk9enTItDlz5gRuDx8+nMLCwmbXrQ+KTpGYxpGBPyN9xzL6nTOBWndK5zyuiMh3hI7g9is758eQmMmY/f9NrUfHWYiIBFNY+BnWOMh9mOyq/yPX2NLsiCwRka5KYRHs3J9wJL4fcy2v4qmrjXQ1IiJRQ2ERzGJla7+76WcuxfPPta0vLyLSRSgsGjmYPprDRhLm/3sj0qWIiEQNhUUjVrudVZ5Lse5aB9XHIl2OiEhUUFg0YreYWO4ZhclTC1+ujHQ5IiJRQWHRiN1qpsjoR11yPyh6PdLliIhEBYVFIzaLGTBx4MzrYO8mOFYc6ZJERCJOYdHIyH6p9EqO49+KzvJNUEe3iIjCorEeiTG8NnMk1Qk5fG6czYltfwEdoCciXZzCohnZZ8Tz+sxL2RiXS3z5TrZ+siHSJYmIRJTCogXpSbHcPONe3Fj4ct3zOv2HiHRpCouTOKNHJgfTr2CM8REHjlVHuhwRkYhRWLTC6HMlPU1l7NmzK9KliIhEjMKiFSlnXQhA+b8+j3AlIiKRo7BoRWLv83w3DhZFthARkQhSWLQmNgmntSdJx76MdCUiIhGjsGiDsm7ncGbtN3i8GhElIl2TwqIN3OnDONN0iH0HDka6FBGRiFBYtEFin/MBOLRza4QrERGJDIVFG2SccxEANfu+iHAlIiKRobBog/iUbI6Ykok5vCPSpYiIRITCoo0OxA4g/fg/I12GiEhEKCza6HjKEHI8Jbhqjke6FBGRTqewaCNLr3OxmTwc+Hp7pEsREel0Cos2SvWf9uPY7s8iXImISOdTWLRRdr/BVBlxGAf/HulSREQ6ncKijew2K/+y9iXpmCPSpYiIdDqFRTsc7jaQXq5d4PVEuhQRkU6lsGgHT9pQ4nBxolRDaEWkawlrWBQWFjJ27FjGjBnDsmXLmsyvra3lnnvuYcyYMUybNo2SkpLA9IceeoiJEydy3XXXsWXLlnCW2WYJ9af9+Fqn/RCRriVsYeHxeHjkkUd4/vnnKSgoYPXq1XzzzTchy7zxxhskJSXxt7/9jVtvvZXHH388MB1g1apVvPjiizz22GN4vd5wldpmPQeMoNawULNPw2dFpGsJW1gUFRXRu3dvcnJysNvt5OXlsX79+pBl3n//fSZPngzA2LFj2bx5M4Zh8M033zBy5EgAUlNT6datGzt2RP5UGzk9kvk/+pOzbwUc0c+sikjXYQ3Xhp1OJ5mZmYH7GRkZFBUVNVkmKyvLV4jVSrdu3Th69CgDBw5k/fr15OXlcfDgQf7xj39w8OBBhg8f3uLjuVwuHI6Oj1Sqqalp0/p/TLybx48/jOvF69h79XN4Ys/o8GOerpo6WzTWFY01gepqj2isCaKzrkjUFLawMIymPxRkMpnatMyUKVPYtWsXU6ZMoWfPnpx33nlYLJaTPl5MTAyDBg3qcL0Oh6NN64+pSuS2t+/nDctizt72H3DLKrAndPhxT0dNnS0a64rGmkB1tUc01gTRWdfprKmtoRO23VCZmZmUlpYG7judTtLT05ssc/Cg7weF3G43lZWVJCcnY7VamTdvHitXruQPf/gDlZWV9OnTJ1yltst1I3qyJ34of+wxDw5shzdvB4870mWJiIRV2MJi2LBh7Nmzh+LiYmpraykoKCA3NzdkmdzcXJYvXw7AunXrGDlyJCaTierqak6cOAHARx99hMVioX///uEqtV1ibRZ+cvGZPL5vAEdHL4Sv18KL40BHdovI91jYwsJqtTJ//nxmzJjB+PHjGTduHAMGDGDp0qWBju6pU6dy7NgxxowZw4svvsgDDzwAwJEjR5g8eTLjxo3jueeeY8mSJeEqs0N+NrI3FpOJ/676AUx+Fsp2w7IfQMH9UH000uWJiJx2YeuzABg9ejSjR48OmTZnzpzA7ZiYGH772982WS87O5t169aFs7RTktk9lnHDsnhtWzH3jplKwtnXwgeLYOtzsONtGPETOPfHkDk00qWKiJwWOoK7g267vA+VNW7e+rwE4pJh/BKYWQi9L4Mtz8IfL4c/jIJNT/v6NnSKEBH5Dgtry+L77Pwzz+DcnGT+9PEefnZJb8xmE2QOgx+9DMePwD/ehi/+Au8t8K0Q090XJDkXQcYwyBgCST2h0QgxEZFopLA4Bbdd1od7XvuCVUUHyB/Rq2FGQipcfIfvUnEQ9n4EezbCvzbC1+82LBebDD0GwBl9IaWv77p7L+jWE5KyOv8JiYi0QGFxCsYPy+L5Tbu597UvcFbUcMcV/ZocS0JSFgyb6rsA1JSD80tw7gDnP6BsF+z7BP7vDSD0uJOzbYmQlAkJaZDQA+JTIS4F4s7wXWK7Q2wSxCT5btsTISYRbPFqsYjIaaWwOAV2q5nXZ17KA2/8nUVrvuKrg5Usun4YsbaTHEAY2x16X+q7BHO74FgxVB6ACt+lfO8/SIk07DrhAAARTUlEQVTxwvFv4duvobrMN9rK28pxHSazLzhs8WCPB1uC/zoOrHG+a1scWGN9F1ssWGLAGnyJDbrEgMXuv9iIOVYC35rBbPVNM1vBYvNdm61gtoDJ4r82K7hEvgcUFqco3m7lv39yPr97/xue/NvX7Pq2ivkTh3BB73aeBsQaAz36+y5+zh4OUhofpWkYUFvlC42acqipAFeF77q2ElxVvvmuKqg7DrUnoO4E1B6Humo4ccQ3ze0Cd7Xvuq4ajLZ3wPcDaM9gNZMlKEzqg8QfLCYTYPJdm0z+8LGFLm+2+tcx+8MnOIh8l15VVfB/yc0HlSl02dCLqcm2MJmDamp0O7DNZqYHPw//tOSDpXDi06Dn2Wh9w+MLf68HMBo9d2vT5aEhfAOPafbPMoW+nsHXGL7Pjv863lkCcUcaHiNkWZpuL/C4JpqG/8mWO8m6hgGG11+TF3v5bvjW/96Z/V+4DG/Qcib/ZyD4S0ijx/GtFFqeYfhfZ0/D9izWhi9AhuH/2/FfDK//S1UsWOOwVZXA0biG19pi879P/veo8XNsfLuxkHlB70vTBZtZz8/rBk9dw/3A31L4KCxOA5PJxOyrB3B2Rjf+/e0ipvzhYy7um8Jdo8/iB+ekNd01dWoPBjHdfJfTyeMGj8sfIi5w1wRdXL4PpscF7lpK9v2L7KwM/we21n/tBm+dbznD6/vjNAz/P0J3wzKBf47+iwGBPxjD699GbcP2vP7lDS+4axu2Xf/P1f/Hb68+ATUH/ct6wOv/J+T1NNRieINqo4Vt+S+nSbT2PPWOdAHNOCvSBbQgOg4HDtXkRB8Xz/SNyAwjhcVpdO3QTK4Y0IPXthbz/Mbd3PanrZyVlsDYIZlcPSiDETnJWMxRukvGYvVd2nCeq0rDAVF2rpx/ne7z9xhBARYcZvXBExRUofODpnk97PzmGwb0P6vpcvW3g1ta0BC4nrqmIecrrKG+wPYabbO565CWEuzd8y9652SHBmvgXG2N1280rfHr1Oq6NL8uhLQQSvbvJ7tnVkNLq3GrLRD+jd+HoOcY2G7jb+WWhtfaZPK/xrUN387rv4DFJPoer77F7Xaxf38xvbIyg1oo7ob3KLBLuKXXqvHfuxH0MtS/LzS8PyGvV6P1ghz69lvS09IaJpyVS7gpLE6zhBgrt4/qy89G9uadvx/gzc+KebZwN898uIvUBDuX9e/BudndGZGTzJCe3Ymzn/wEiRIh9buTTvFQJHd8BXTPPj01nUYnTpwB/aIr8Cst0fclBKDC5qBXlNV1xOEgvZNrUliEid1qZuoF2Uy9IJvyE3Vs2Pkt6x1Otu05yqq/HwDAYjbROzWes9ISOSstkX5pCWSfEUd2cjyZ3WMj/AxERBooLDpB93gb153bk+vO7QnAocoaiorL+XvJMXY6q9j1bRUf/vMQdZ6GpqbJBMmxFnqecZi0bjGkd4shNTGGlHg7yfE2UhJ810mxNrrH2UiKsxFjNZ/e/hERET+FRQSkd4vlh4Nj+eHgjMA0t8fL/mPV7D9aTcmxag4cq8axt5Q6SyyHKmv48kAFZcdrcXubGzXhY7OYSIyxkhhrJcFuJSHGSrzdQoLddx1rtxBvsxBntxBrsxBjNRNrs/gvZuwWMzH+6fXzYqy+aXaLb77dasbtNTAMQ8Ek0oUoLKKE1WKmd2oCvVMbOpgdDk9Ip61hGFS53Bw9XkfZiVrKq+sCl4rqOqpcbqpq3Bx3ual0uTlR66ayxo2zooYTtR5q6jycqPVQXedp2n/Wbv/CZjFhNZuxWkzYLGasZhNWswmz2YTFbMJi8l+bffMt/vnB9y1mE2aTCYsZrObQZSz12zKFLlc/zWxqmH/k8FEySr/xdTVgwmwisI36dX0XMJtM/i6J0PvByzTMN2HCP2Iz+H79NsDfR9lw32xu2M7ewzXUlRzD7A/WJv2uQR2g9TVYzA21mIK2HbycyV+HqdFj19fSWMPz8617vNZLlcvtu99o+80xB96Dhr7Xlj5CzW2qvmb57lJYfIeYTCa6xdroFmvjzNT4Dm/HMAxcbi+uOi81bl+I1Lq9vmluD6463+2aOk9gWq3bS63HoNbt5WCpk+SUVOq8Bm6PlzqPgdvrxe0xqPP4Wh0ew8Dj9V3c/us6jxevYeD2GNTUeajzGni9wct58Rrg9nrxeHzreYO24zXw3TZ863kN37QG0Xp6+AORLqAFeyLyqOZmws432MnAbN4TCGQD/wAkfyyZaAgss9nkHwjmmxcYge1/jPrQrv+iAA2fH28zaWcO+VIRWq/b7cZuOxAIZFPgy4avJq9h+EaJ+7cb+uUj9MtJ/efW418HmgZ+YPuNXrfgQVSu2lpiCpyBeTdelMPM0eEdfKyw6IJMJlNg91N3bO1e3+GoZdCgc8JQWcd4vQb/cDg455yBgT/Y4FCpD536P2iP12gYcYr/Pr5/PPX/UOrneb2+6/p167dj+B+38T+0huV88/fu20d2do5v+UbNucatu8DjB4Vh/WM1/acYPC/0fmDjjYZieoOev9PpJC093V/vyV9f3+vQ8NoE/sHTtKXUXIs1pDbDCLwWjes/cvgIKampgdc1ECb+8PDV3vA+1P9jrRf8T9frfw09/nXMptCWVeC4RkyB51e/bOM209Gjx+jePdlXc329RsNnpr4lGXSYXeDxfbfrn68R1ELz1xD0/ja8Fk0/KyGjbIHKigqSuncPzM5J6fiXx7ZSWMh3ntm/68pujb4z7juMIwwalNH6gp3M4XAxaFB0HQYXjb91DdFZVyRqir6/LhERiToKCxERaZXCQkREWqWwEBGRViksRESkVQoLERFplcJCRERapbAQEZFWmQyjuWMuv3u++OILYmJiIl2GiMh3isvlYsSIEa0u970JCxERCR/thhIRkVYpLEREpFUKCxERaZXCQkREWqWwEBGRViksRESkVV3+x48KCwtZuHAhXq+XadOmceedd0akjoceeogPP/yQ1NRUVq9eDcCxY8e499572b9/P7169eLpp5+me9CvY4XbwYMHmTt3LocPH8ZsNnPDDTdwyy23RLwul8vFT3/6U2pra/F4PIwdO5bZs2dTXFzMfffdR3l5OYMHD2bJkiXY7fZOqwvA4/EwZcoUMjIyePbZZ6OiptzcXBISEjCbzVgsFt5+++2Iv4cAFRUV/Md//Adff/01JpOJRYsW0bdv34jVtXv3bu69997A/eLiYmbPns2kSZMi/lr96U9/4o033sBkMnH22WezePFiDh061LmfLaMLc7vdxtVXX23s27fPcLlcxsSJE42dO3dGpJZPP/3U2LFjh5GXlxeY9thjjxnPPvusYRiG8eyzzxpLlizp1JqcTqexY8cOwzAMo7Ky0rjmmmuMnTt3Rrwur9drVFVVGYZhGLW1tcbUqVON7du3G7NnzzZWr15tGIZh/OpXvzJefvnlTq3LMAzjhRdeMO677z7jzjvvNAzDiIqarrrqKuPIkSMh0yL9HhqGYcydO9d4/fXXDcMwDJfLZZSXl0dFXYbh+99w2WWXGSUlJRGvqbS01LjqqquM6upqwzB8n6m33nqr0z9bXXo3VFFREb179yYnJwe73U5eXh7r16+PSC0XXXRRk28r69evZ9KkSQBMmjSJ9957r1NrSk9PZ8iQIQAkJibSr18/nE5nxOsymUwkJCQA4Ha7cbvdmEwmPvnkE8aOHQvA5MmTO/29LC0t5cMPP2Tq1KmA73eVI11TSyL9HlZVVbF169bAa2W320lKSop4XfU2b95MTk4OvXr1ioqaPB4PNTU1uN1uampqSEtL6/TPVpcOC6fTSWZmZuB+RkYGTqczghWFOnLkCOnp6YDvH3dZWVnEaikpKcHhcHDuuedGRV0ej4f8/Hwuu+wyLrvsMnJyckhKSsJq9e1ZzczM7PT3ctGiRfzyl7/EbPb9WR09ejTiNdWbPn06119/Pa+99hoQ+c9WcXExKSkpPPTQQ0yaNImHH36YEydORLyuegUFBUyYMAGI/GuVkZHB7bffzlVXXcWoUaNITExkyJAhnf7Z6tJhYTRzphOTyRSBSqLb8ePHmT17NvPmzSMxMTHS5QBgsVhYuXIlGzZsoKioiN27dzdZpjPfyw8++ICUlBSGDh160uUi8fl65ZVXWL58Oc899xwvv/wyW7du7fQaGnO73Xz55Zf8+Mc/ZsWKFcTFxbFs2bJIlwVAbW0t77//Ptdee22kSwGgvLyc9evXs379ejZu3Eh1dTWFhYVNlgv3Z6tLh0VmZialpaWB+06nM/ANIhqkpqZy6NAhAA4dOkRKSkqn11BXV8fs2bOZOHEi11xzTdTUVS8pKYlLLrmEL774goqKCtxuN+DbJdSZ7+Xnn3/O+++/T25uLvfddx+ffPIJCxcujGhN9TIyMgDf+zZmzBiKiooi/h5mZmaSmZnJueeeC8C1117Ll19+GfG6wDfoZciQIfTo0QOI/Of9448/Jjs7m5SUFGw2G9dccw3bt2/v9M9Wlw6LYcOGsWfPHoqLi6mtraWgoIDc3NxIlxWQm5vLihUrAFixYgVXX311pz6+YRg8/PDD9OvXj9tuuy1q6iorK6OiogKAmpoaPv74Y8466ywuueQS1q1bB8Dy5cs79b28//77KSws5P333+fJJ59k5MiRPPHEExGtCeDEiRNUVVUFbn/00UcMGDAg4u9hWloamZmZgRbh5s2bOeussyJeF/h2QeXl5QXuR7qmnj178ve//53q6moMw2Dz5s3079+/0z9bXf6ssxs2bGDRokWBIY933XVXROq47777+PTTTzl69Cipqan84he/4Ic//CH33HMPBw8eJCsri6VLl5KcnNxpNW3bto2f/vSnnH322YH98Pfddx/Dhw+PaF1fffUV//7v/47H48EwDK699lpmzZpFcXEx9957L+Xl5QwaNIjHH3+804epAmzZsoUXXnghMHQ2kjUVFxdz9913A75+ngkTJnDXXXdx9OjRiL6HAA6Hg4cffpi6ujpycnJYvHgxXq83onVVV1fzgx/8gPfee49u3boBRMVr9dvf/pY1a9ZgtVoZNGgQCxcuxOl0dupnq8uHhYiItK5L74YSEZG2UViIiEirFBYiItIqhYWIiLRKYSEiIq1SWIhEgS1btjBz5sxIlyHSIoWFiIi0qsv/noVIe6xcuZKXXnqJuro6zj33XBYsWMCFF17IjTfeyJYtW0hKSuKpp54iJSUFh8PBggULqK6u5swzz2TRokV0796dvXv3smDBAsrKyrBYLCxduhTwHWE9e/Zsvv76a4YMGcLjjz+uc5VJ1FDLQqSNdu3axbvvvssrr7zCypUrMZvNrFq1ihMnTjB48GCWL1/ORRddxO9//3sA5s6dywMPPMCqVas4++yzA9MfeOABfvrTn/LOO+/w6quvkpaWBsCXX37JvHnzWLNmDSUlJXz22WcRe64ijSksRNpo8+bN7Nixg6lTp5Kfn8/mzZspLi7GbDYzfvx4APLz8/nss8+orKyksrKSiy++GPD93sC2bduoqqrC6XQyZswYAGJiYoiLiwNg+PDhZGZmYjabGThwIPv374/MExVphnZDibSRYRhMnjyZ+++/P2T6M888E3K/o7uOgs/rY7FY8Hg8HdqOSDioZSHSRpdeeinr1q3jyJEjgO830vfv34/X6w2c/XPVqlVccMEFdOvWjaSkJLZt2wb4+jouuugiEhMTyczMDPzaWm1tLdXV1ZF5QiLtoJaFSBv179+fe+65h9tvvx2v14vNZmP+/PnEx8ezc+dOrr/+ehITE3n66acBeOyxxwId3PVnVQVYsmQJ8+fPZ+nSpdhstkAHt0g001lnRU7Reeedx/bt2yNdhkhYaTeUiIi0Si0LERFplVoWIiLSKoWFiIi0SmEhIiKtUliIiEirFBYiItKq/w84D6UHGt324wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_spae_sigmoid_adam_mse  = plot_hist_auto(hist_spae_sigmoid_adam_mse, './Figures/hist_spae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valueDict = {\n",
    "    'loss_value_ae_sigmoid_adam_mse': best_loss_value_ae_sigmoid_adam_mse,\n",
    "    'loss_value_spae_sigmoid_adam_mse': best_loss_value_spae_sigmoid_adam_mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_value_ae_sigmoid_adam_mse': 0.04916991403010735,\n",
       " 'loss_value_spae_sigmoid_adam_mse': 0.08505774986883327}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330452, 140)\n",
      "(415767, 140)\n",
      "(1330452, 140)\n",
      "(415767, 140)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train_x_asam.shape)\n",
    "print(enc_test_x_asam.shape)\n",
    "\n",
    "print(enc_train_x_spsam.shape)\n",
    "print(enc_test_x_spsam.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_asam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 18:30:14 2019\n",
      "Train on 1064361 samples, validate on 266091 samples\n",
      "Epoch 1/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.2220 - acc: 0.8872 - val_loss: 0.1353 - val_acc: 0.9290\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13531, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.1287 - acc: 0.9322 - val_loss: 0.1107 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13531 to 0.11066, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.1109 - acc: 0.9419 - val_loss: 0.1033 - val_acc: 0.9448\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11066 to 0.10327, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0973 - acc: 0.9497 - val_loss: 0.0901 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10327 to 0.09008, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0854 - acc: 0.9568 - val_loss: 0.0852 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09008 to 0.08521, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0754 - acc: 0.9627 - val_loss: 0.0658 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08521 to 0.06580, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0677 - acc: 0.9667 - val_loss: 0.0643 - val_acc: 0.9685\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06580 to 0.06431, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0617 - acc: 0.9700 - val_loss: 0.0634 - val_acc: 0.9703\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06431 to 0.06336, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0569 - acc: 0.9724 - val_loss: 0.0496 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06336 to 0.04958, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0529 - acc: 0.9744 - val_loss: 0.0505 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04958\n",
      "Epoch 11/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0495 - acc: 0.9762 - val_loss: 0.0455 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04958 to 0.04547, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0473 - acc: 0.9771 - val_loss: 0.0473 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04547\n",
      "Epoch 13/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0445 - acc: 0.9785 - val_loss: 0.0425 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.04547 to 0.04253, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0426 - acc: 0.9795 - val_loss: 0.0379 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04253 to 0.03788, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0410 - acc: 0.9802 - val_loss: 0.0385 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03788\n",
      "Epoch 16/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0395 - acc: 0.9809 - val_loss: 0.0419 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03788\n",
      "Epoch 17/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0378 - acc: 0.9818 - val_loss: 0.0358 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03788 to 0.03578, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0368 - acc: 0.9823 - val_loss: 0.0315 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03578 to 0.03153, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 19/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0358 - acc: 0.9829 - val_loss: 0.0306 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.03153 to 0.03060, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0350 - acc: 0.9833 - val_loss: 0.0323 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03060\n",
      "Epoch 21/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0337 - acc: 0.9837 - val_loss: 0.0347 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03060\n",
      "Epoch 22/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0331 - acc: 0.9841 - val_loss: 0.0296 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.03060 to 0.02960, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 23/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0327 - acc: 0.9842 - val_loss: 0.0292 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02960 to 0.02925, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 24/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0320 - acc: 0.9846 - val_loss: 0.0331 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02925\n",
      "Epoch 25/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0315 - acc: 0.9848 - val_loss: 0.0366 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02925\n",
      "Epoch 26/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0310 - acc: 0.9852 - val_loss: 0.0311 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02925\n",
      "Epoch 27/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0306 - acc: 0.9855 - val_loss: 0.0281 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02925 to 0.02810, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 28/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0303 - acc: 0.9856 - val_loss: 0.0269 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02810 to 0.02685, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 29/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0299 - acc: 0.9857 - val_loss: 0.0280 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02685\n",
      "Epoch 30/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0295 - acc: 0.9858 - val_loss: 0.0284 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02685\n",
      "Epoch 31/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0293 - acc: 0.9861 - val_loss: 0.0335 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02685\n",
      "Epoch 32/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0289 - acc: 0.9862 - val_loss: 0.0276 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02685\n",
      "Epoch 33/200\n",
      "1064361/1064361 [==============================] - 17s 16us/step - loss: 0.0285 - acc: 0.9865 - val_loss: 0.0290 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02685\n",
      "Time elapsed (hh:mm:ss.ms) 0:09:52.569586\n"
     ]
    }
   ],
   "source": [
    "hist_ae_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ae_ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = ae_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_asam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_ae_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_ae_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_ae_ann_2h_unisoftsigbinlosadam, './Figures/ae_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_ae_ann_2h_prob_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict(ae_ann_2h_unisoftsigbinlosadam,enc_test_x_asam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_asam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 18:40:07 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.3661 - acc: 0.8165\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1741 - acc: 0.9127\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1464 - acc: 0.9240\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1369 - acc: 0.9275\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1294 - acc: 0.9317\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1235 - acc: 0.9350\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.1186 - acc: 0.9372\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1123 - acc: 0.9412\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1073 - acc: 0.9439\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1016 - acc: 0.9474\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0976 - acc: 0.9501\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0924 - acc: 0.9527\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0896 - acc: 0.9553\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0846 - acc: 0.9578\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 5s 17us/step - loss: 0.0805 - acc: 0.9598\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0769 - acc: 0.9618\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0749 - acc: 0.9634\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0712 - acc: 0.9649\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0694 - acc: 0.9663\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0665 - acc: 0.9679\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0644 - acc: 0.9687\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0621 - acc: 0.9702\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0597 - acc: 0.9711\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0582 - acc: 0.9723\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0572 - acc: 0.9728\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0550 - acc: 0.9734\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0553 - acc: 0.9736\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0523 - acc: 0.9747\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0516 - acc: 0.9753\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0500 - acc: 0.9762\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0500 - acc: 0.9760\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0482 - acc: 0.9767\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0479 - acc: 0.9768\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0471 - acc: 0.9775\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0447 - acc: 0.9782\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0451 - acc: 0.9782\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0437 - acc: 0.9790\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0438 - acc: 0.9790\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0421 - acc: 0.9797\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0421 - acc: 0.9797\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0416 - acc: 0.9799\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0403 - acc: 0.9807\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0393 - acc: 0.9809\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0406 - acc: 0.9805\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0388 - acc: 0.9812\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0386 - acc: 0.9814\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0380 - acc: 0.9817\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0376 - acc: 0.9819\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0374 - acc: 0.9821\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0368 - acc: 0.9823\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0359 - acc: 0.9827\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0364 - acc: 0.9824\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0358 - acc: 0.9830\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0345 - acc: 0.9834\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0355 - acc: 0.9830\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0347 - acc: 0.9835\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0345 - acc: 0.9834\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0347 - acc: 0.9835\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0332 - acc: 0.9841\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0336 - acc: 0.9840\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0333 - acc: 0.9843\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0335 - acc: 0.9841\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0322 - acc: 0.9848\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0317 - acc: 0.9848\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0324 - acc: 0.9845\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0319 - acc: 0.9847\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0318 - acc: 0.9848\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0307 - acc: 0.9853\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0310 - acc: 0.9851\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0315 - acc: 0.9850\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0303 - acc: 0.9854\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0307 - acc: 0.9856\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0294 - acc: 0.9858\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0302 - acc: 0.9856\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0296 - acc: 0.9858\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0303 - acc: 0.9857\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0287 - acc: 0.9863\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0297 - acc: 0.9860\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0295 - acc: 0.9861\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0295 - acc: 0.9861\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0288 - acc: 0.9863\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0290 - acc: 0.9864\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0293 - acc: 0.9859\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0281 - acc: 0.9866\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0283 - acc: 0.9866\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0286 - acc: 0.9863\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0277 - acc: 0.9869\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0297 - acc: 0.9860\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0278 - acc: 0.9868\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0274 - acc: 0.9871\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0277 - acc: 0.9870\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0279 - acc: 0.9869\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0280 - acc: 0.9867\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0272 - acc: 0.9872\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0282 - acc: 0.9870\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0265 - acc: 0.9876\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0271 - acc: 0.9873\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0271 - acc: 0.9871\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0270 - acc: 0.9873\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0263 - acc: 0.9877\n",
      "83154/83154 [==============================] - 0s 4us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.3695 - acc: 0.8140\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.1745 - acc: 0.9123\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.1464 - acc: 0.9233\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.1366 - acc: 0.9281\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.1287 - acc: 0.9320\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.1207 - acc: 0.9363\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.1159 - acc: 0.9394\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.1099 - acc: 0.9425\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.1054 - acc: 0.9451\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.1004 - acc: 0.9482\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0966 - acc: 0.9507\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0925 - acc: 0.9529\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0877 - acc: 0.9558\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0846 - acc: 0.9579\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0804 - acc: 0.9597\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0765 - acc: 0.9620\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0742 - acc: 0.9637\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0714 - acc: 0.9652\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0688 - acc: 0.9664\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0668 - acc: 0.9674\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0649 - acc: 0.9686\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0631 - acc: 0.9695\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0614 - acc: 0.9703\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0594 - acc: 0.9714\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0578 - acc: 0.9722\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0565 - acc: 0.9727\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0557 - acc: 0.9732\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0548 - acc: 0.9735\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0534 - acc: 0.9743\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0524 - acc: 0.9749\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0514 - acc: 0.9753\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0502 - acc: 0.9759\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0500 - acc: 0.9758\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0493 - acc: 0.9762\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0484 - acc: 0.9766\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0471 - acc: 0.9774\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0469 - acc: 0.9777\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 5s 16us/step - loss: 0.0471 - acc: 0.9776\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0447 - acc: 0.9785\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0457 - acc: 0.9780\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0438 - acc: 0.9789\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0433 - acc: 0.9791\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0429 - acc: 0.9788\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0437 - acc: 0.9789\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0426 - acc: 0.9797\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0413 - acc: 0.9797\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0417 - acc: 0.9800\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0408 - acc: 0.9802\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0402 - acc: 0.9805\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0403 - acc: 0.9804\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0396 - acc: 0.9807\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0385 - acc: 0.9812\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0391 - acc: 0.9812\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0393 - acc: 0.9809\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0385 - acc: 0.9815\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0379 - acc: 0.9818\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0378 - acc: 0.9818\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0373 - acc: 0.9821\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0369 - acc: 0.9823\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0365 - acc: 0.9821\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0366 - acc: 0.9824\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0364 - acc: 0.9826\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0363 - acc: 0.9825\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0353 - acc: 0.9828\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0362 - acc: 0.9825\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0346 - acc: 0.9831\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0350 - acc: 0.9831\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0349 - acc: 0.9829\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0342 - acc: 0.9835\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0337 - acc: 0.9837\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0344 - acc: 0.9835\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0335 - acc: 0.9836\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0336 - acc: 0.9836\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0339 - acc: 0.9834\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0329 - acc: 0.9842\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0332 - acc: 0.9838\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0327 - acc: 0.9841\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0328 - acc: 0.9839\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0323 - acc: 0.9842\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0321 - acc: 0.9842\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0328 - acc: 0.9844\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0328 - acc: 0.9842\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0315 - acc: 0.9845\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0322 - acc: 0.9841\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0310 - acc: 0.9850\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0319 - acc: 0.9847\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0318 - acc: 0.9845\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0321 - acc: 0.9844\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0306 - acc: 0.9850\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0309 - acc: 0.9850\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0315 - acc: 0.9848\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0310 - acc: 0.9850\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0310 - acc: 0.9849\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0305 - acc: 0.9851\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0307 - acc: 0.9851\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0301 - acc: 0.9853\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0300 - acc: 0.9853\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0317 - acc: 0.9847\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0296 - acc: 0.9854\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 5s 15us/step - loss: 0.0298 - acc: 0.9856\n",
      "83154/83154 [==============================] - 0s 4us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 6s 17us/step - loss: 0.3641 - acc: 0.8166\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1716 - acc: 0.9136\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1476 - acc: 0.9223\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1372 - acc: 0.9278\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1305 - acc: 0.9311\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1229 - acc: 0.9357\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1167 - acc: 0.9393\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1112 - acc: 0.9426\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1066 - acc: 0.9455\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1019 - acc: 0.9478\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0987 - acc: 0.9497\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0947 - acc: 0.9519\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0898 - acc: 0.9547\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0892 - acc: 0.9552\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0843 - acc: 0.9578\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0810 - acc: 0.9597\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0798 - acc: 0.9603\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0758 - acc: 0.9626\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0742 - acc: 0.9635\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0710 - acc: 0.9651\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0693 - acc: 0.9661\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0677 - acc: 0.9671\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0645 - acc: 0.9684\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0638 - acc: 0.9691\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0614 - acc: 0.9703\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0606 - acc: 0.9708\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0588 - acc: 0.9715\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0574 - acc: 0.9723\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0561 - acc: 0.9731\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 5s 14us/step - loss: 0.0554 - acc: 0.9733\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0536 - acc: 0.9744\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0511 - acc: 0.9755\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0523 - acc: 0.9751\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0502 - acc: 0.9760\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0495 - acc: 0.9764\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0481 - acc: 0.9771\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0477 - acc: 0.9774\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0473 - acc: 0.9776\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0467 - acc: 0.9775\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0458 - acc: 0.9782\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0457 - acc: 0.9783\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0437 - acc: 0.9791\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0440 - acc: 0.9794\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0429 - acc: 0.9795\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0432 - acc: 0.9797\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0415 - acc: 0.9804\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0409 - acc: 0.9804\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0417 - acc: 0.9804\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0402 - acc: 0.9810\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0395 - acc: 0.9814\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0399 - acc: 0.9812\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0387 - acc: 0.9816\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0384 - acc: 0.9817\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0382 - acc: 0.9820\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0372 - acc: 0.9822\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0374 - acc: 0.9825\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0363 - acc: 0.9827\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0367 - acc: 0.9828\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0363 - acc: 0.9826\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0355 - acc: 0.9832\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0353 - acc: 0.9836\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0354 - acc: 0.9833\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0350 - acc: 0.9835\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0344 - acc: 0.9837\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0347 - acc: 0.9837\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0342 - acc: 0.9838\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0341 - acc: 0.9838\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0339 - acc: 0.9840\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0331 - acc: 0.9844\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0327 - acc: 0.9846\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0333 - acc: 0.9843\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0325 - acc: 0.9848\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0327 - acc: 0.9849\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0329 - acc: 0.9845\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0327 - acc: 0.9844\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0316 - acc: 0.9852\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0325 - acc: 0.9849\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0309 - acc: 0.9853\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0314 - acc: 0.9852\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0312 - acc: 0.9853\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0314 - acc: 0.9854\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0312 - acc: 0.9854\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0316 - acc: 0.9853\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0307 - acc: 0.9854\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0309 - acc: 0.9856\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0306 - acc: 0.9855\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0311 - acc: 0.9854\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0306 - acc: 0.9856\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0299 - acc: 0.9861\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0303 - acc: 0.9855\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0305 - acc: 0.9856\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0300 - acc: 0.9859\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0301 - acc: 0.9862\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0298 - acc: 0.9861\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0298 - acc: 0.9860\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0293 - acc: 0.9863\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0298 - acc: 0.9861\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0297 - acc: 0.9859\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0291 - acc: 0.9867\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0300 - acc: 0.9861\n",
      "83153/83153 [==============================] - 0s 5us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 5s 17us/step - loss: 0.3711 - acc: 0.8128\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1742 - acc: 0.9124\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1485 - acc: 0.9224\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1380 - acc: 0.9280\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1280 - acc: 0.9326\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1213 - acc: 0.9365\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1161 - acc: 0.9395\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1094 - acc: 0.9435\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1038 - acc: 0.9465\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0994 - acc: 0.9489\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0958 - acc: 0.9514\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0888 - acc: 0.9551\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0879 - acc: 0.9561\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0829 - acc: 0.9591\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0809 - acc: 0.9597\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0765 - acc: 0.9623\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0747 - acc: 0.9630\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0724 - acc: 0.9642\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0698 - acc: 0.9658\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0674 - acc: 0.9673\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0655 - acc: 0.9682\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0640 - acc: 0.9690\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0624 - acc: 0.9699\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0603 - acc: 0.9708\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0589 - acc: 0.9716\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0580 - acc: 0.9720\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0563 - acc: 0.9730\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0553 - acc: 0.9734\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0539 - acc: 0.9740\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0528 - acc: 0.9749\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0523 - acc: 0.9750\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0504 - acc: 0.9757\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0504 - acc: 0.9760\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0497 - acc: 0.9761\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0488 - acc: 0.9765\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0469 - acc: 0.9774\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0465 - acc: 0.9772\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0464 - acc: 0.9778\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0448 - acc: 0.9786\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0446 - acc: 0.9785\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0444 - acc: 0.9786\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0424 - acc: 0.9797\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0419 - acc: 0.9799\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0420 - acc: 0.9800\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0406 - acc: 0.9806\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0409 - acc: 0.9803\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0412 - acc: 0.9803\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0398 - acc: 0.9808\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0389 - acc: 0.9810\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0390 - acc: 0.9815\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0391 - acc: 0.9814\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0382 - acc: 0.9818\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0377 - acc: 0.9818\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0373 - acc: 0.9822\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0373 - acc: 0.9822\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0370 - acc: 0.9823\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0357 - acc: 0.9830\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0363 - acc: 0.9825\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0353 - acc: 0.9833\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0357 - acc: 0.9828\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0348 - acc: 0.9832\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0352 - acc: 0.9833\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0338 - acc: 0.9838\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0338 - acc: 0.9837\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0344 - acc: 0.9838\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0331 - acc: 0.9842\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0331 - acc: 0.9842\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0335 - acc: 0.9843\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0330 - acc: 0.9840\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0324 - acc: 0.9844\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0325 - acc: 0.9844\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0320 - acc: 0.9848\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0326 - acc: 0.9846\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0316 - acc: 0.9848\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0319 - acc: 0.9848\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0312 - acc: 0.9852\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0313 - acc: 0.9851\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0306 - acc: 0.9854\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0316 - acc: 0.9850\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0309 - acc: 0.9854\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0307 - acc: 0.9855\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0301 - acc: 0.9857\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0297 - acc: 0.9857\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0310 - acc: 0.9856\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0301 - acc: 0.9858\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0308 - acc: 0.9854\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0294 - acc: 0.9860\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0295 - acc: 0.9859\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0294 - acc: 0.9862\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0297 - acc: 0.9860\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0294 - acc: 0.9861\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0296 - acc: 0.9860\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0293 - acc: 0.9862\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0282 - acc: 0.9865\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0287 - acc: 0.9864\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0285 - acc: 0.9865\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0284 - acc: 0.9864\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0288 - acc: 0.9866\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0279 - acc: 0.9869\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0282 - acc: 0.9865\n",
      "83153/83153 [==============================] - 0s 5us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 6s 17us/step - loss: 0.3581 - acc: 0.8208\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1702 - acc: 0.9144\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1464 - acc: 0.9240\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1354 - acc: 0.9287\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1274 - acc: 0.9333\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1213 - acc: 0.9360\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1166 - acc: 0.9390\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1102 - acc: 0.9424\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1064 - acc: 0.9445\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1007 - acc: 0.9481\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0968 - acc: 0.9502\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0927 - acc: 0.9525\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0877 - acc: 0.9551\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0854 - acc: 0.9566\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0812 - acc: 0.9589\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0797 - acc: 0.9600\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0759 - acc: 0.9623\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0735 - acc: 0.9634\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0710 - acc: 0.9646\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0681 - acc: 0.9664\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0669 - acc: 0.9671\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0640 - acc: 0.9683\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0636 - acc: 0.9690\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0614 - acc: 0.9702\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0595 - acc: 0.9710\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0575 - acc: 0.9719\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0569 - acc: 0.9725\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0558 - acc: 0.9728\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0549 - acc: 0.9734\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0529 - acc: 0.9743\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0522 - acc: 0.9746\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0516 - acc: 0.9752\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0497 - acc: 0.9759\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0492 - acc: 0.9760\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0483 - acc: 0.9766\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0471 - acc: 0.9770\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0468 - acc: 0.9774\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0451 - acc: 0.9783\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0456 - acc: 0.9781\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0446 - acc: 0.9783\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0436 - acc: 0.9788\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0433 - acc: 0.9791\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0427 - acc: 0.9796\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0422 - acc: 0.9797\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0418 - acc: 0.9799\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0413 - acc: 0.9799\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0397 - acc: 0.9807\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0404 - acc: 0.9804\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0394 - acc: 0.9808\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0387 - acc: 0.9812\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0385 - acc: 0.9813\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0381 - acc: 0.9814\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0380 - acc: 0.9814\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0370 - acc: 0.9821\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0382 - acc: 0.9815\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0365 - acc: 0.9824\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0363 - acc: 0.9826\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0362 - acc: 0.9826\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0354 - acc: 0.9829\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0357 - acc: 0.9827\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0353 - acc: 0.9830\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0354 - acc: 0.9828\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0348 - acc: 0.9832\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0336 - acc: 0.9838\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0337 - acc: 0.9839\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0337 - acc: 0.9835\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0332 - acc: 0.9840\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0335 - acc: 0.9838\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0337 - acc: 0.9838\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0337 - acc: 0.9838\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0327 - acc: 0.9842\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0325 - acc: 0.9844\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0329 - acc: 0.9842\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0319 - acc: 0.9847\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0321 - acc: 0.9846\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0315 - acc: 0.9849\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0309 - acc: 0.9853\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0317 - acc: 0.9849\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0313 - acc: 0.9849\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0313 - acc: 0.9849\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0314 - acc: 0.9851\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0304 - acc: 0.9854\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0302 - acc: 0.9856\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0316 - acc: 0.9846\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0300 - acc: 0.9856\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0312 - acc: 0.9851\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0299 - acc: 0.9857\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0301 - acc: 0.9856\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0293 - acc: 0.9860\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0304 - acc: 0.9856\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0296 - acc: 0.9858\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0292 - acc: 0.9861\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0287 - acc: 0.9861\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0294 - acc: 0.9859\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0299 - acc: 0.9857\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0282 - acc: 0.9867\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0295 - acc: 0.9861\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0292 - acc: 0.9862\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0295 - acc: 0.9860\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0283 - acc: 0.9864\n",
      "83153/83153 [==============================] - 0s 6us/step\n",
      "Time elapsed (hh:mm:ss.ms) 0:41:54.129137\n",
      "Overall accuracy of Neural Network model: 0.9860715256381579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 41.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9866    0.9856    0.9861    207840\n",
      "           1     0.9856    0.9866    0.9861    207927\n",
      "\n",
      "   micro avg     0.9861    0.9861    0.9861    415767\n",
      "   macro avg     0.9861    0.9861    0.9861    415767\n",
      "weighted avg     0.9861    0.9861    0.9861    415767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_ae_ann_2h_prob_unisoftsigbinlosadam,pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XlYVGX7wPHvACK44JYCbxBpmqKioGLuJIgoiIiKqeVuKuVSuZdZkUu2+dqGub+p6U/NJcU1SnHFTBRNrDRRUcEFUUTZhvP74+QoyTIow8zA/bkur2Fmzpy555lx7nme5zz30SiKoiCEEELkw8LYAQghhDBtkiiEEEIUSBKFEEKIAkmiEEIIUSBJFEIIIQokiUIIIUSBJFGUcj/++CNDhw41dhgmxcPDg4sXL5b48yYkJFC/fn2ys7NL/LkNISAggOjo6CI/7kk+k/v27eO11157rMc+rszMTLp06cKNGzdK9HlNiUbWUZQcb29vrl+/jqWlJRUqVKB9+/a8++67VKxY0dihFZujR4/y3//+lxMnTmBhYYGnpycTJkygbt26RolnwIABdO/enZCQkBJ5vnPnzjF37lyio6PJzs7mP//5Dz179mTgwIFcuXIFHx8ffv/9d6ysrEoknvzUr1+fnTt34uLiYtDnSUhIKNbX3LNnT6ZPn467uzugvg5bW1s0Gg2VKlXC39+fSZMmYWlpqXvML7/8wtdff82ZM2coX7487dq1Y+LEiTg4OOi2uXr1Kv/973+JiooiLS0Ne3t7/P39GT58OBUqVGDhwoXcuHGDKVOmPPFrMEfSoyhh8+fPJyYmho0bN3Lq1CkWLFhg7JAeS16/imNiYhg2bBg+Pj7s3buXyMhI6tevT79+/QzyC97UfplfuHCBPn364OjoyObNm/ntt9+YN28eJ0+eJC0trVify5iv3VjPHRsby507d3RJ4r5NmzYRExPDihUr2Lp1Kz/88IPuvu3btzN+/HgGDhzIoUOH2LJlC9bW1vTv359bt24BkJKSQt++fcnIyGD16tXExMSwdOlSbt++zYULFwAIDAxkw4YNZGZmltwLNiGSKIykZs2atGvXjri4ON1tmZmZzJkzhxdffJE2bdowffp00tPTdff/9NNPBAUF0axZMzp16kRUVBQAqampvP3227Rr14727dszd+5ctFotAOvXr6dfv34ATJ8+nTlz5uSKIzQ0lKVLlwKQlJTEmDFjaNWqFd7e3nz33Xe67b788kvGjh3LhAkTaNasGRs2bHjkNX3yyScEBQUxaNAgKlWqRNWqVXnzzTdp2rQpX375JQDR0dF06NCB+fPn88ILL+Dt7c2PP/6oVxvcf+yCBQto27YtU6dO5datW4wcOZJWrVrh6enJyJEjSUxMBGDu3LkcOXKEsLAwPDw8CAsLA9RfoefPnwdgypQpfPDBB4wYMQIPDw9CQkJ0Xw6gDnX4+fnRvHlz3n//fV555RXWrl2b53v6xRdf4OHhwdSpU6lVqxYAderU4bPPPsPOzk633ebNm3nxxRd54YUXCA8P190eGxvLSy+9RIsWLWjXrh1hYWG5vpjq16/PypUr6dy5M507dwZgxowZeHl50axZM3r27MmRI0d022u1WubPn0+nTp3w8PCgZ8+eXLlyhZdffhmAoKAgPDw82Lp1K6D+8g4KCqJFixb07duX06dP6/bl7e3NggULCAwMxN3dnezsbLy9vTlw4IAu9p49e9KsWTPatGnD7NmzAXjllVcA8PT0xMPDg5iYmFyfSYC//vqLIUOG0LJlS9q0acP8+fPzbN+oqCg8PT3zvA/AxcWFZs2a6f5PKYrCnDlzCA0NpXv37tjY2FCzZk1mzpxJhQoVWLZsGQBLly6lYsWKfPLJJzg5OQHg6OjItGnTaNCgAQAODg5UqVKFY8eO5fv8pZoiSkzHjh2V/fv3K4qiKFeuXFG6deumfPjhh7r7Z8yYoYwcOVK5efOmkpqaqowcOVL59NNPFUVRlOPHjyvNmjVT9u3bp2i1WiUxMVE5c+aMoiiKEhoaqrz77rtKWlqacv36daVXr17KqlWrFEVRlB9++EHp27evoiiKcvjwYaVDhw5KTk6OoiiKkpKSori5uSmJiYmKVqtVgoODlS+//FLJyMhQLly4oHh7eytRUVGKoijKF198oTRs2FDZtWuXotVqlXv37uV6bXfv3lUaNGigHDx48JHXvW7dOqVt27aKoijKoUOHFFdXV2XWrFlKRkaGEh0drTRt2lQ5e/ZsoW1w/7Eff/yxkpGRody7d09JTk5Wtm/frty9e1dJTU1VxowZo4SGhuqe+5VXXlHWrFmTK57nn39eiY+PVxRFUSZPnqx4enoqx48fV7KyspS33npLeeONNxRFUZQbN24oHh4eyo4dO5SsrCxl2bJlSsOGDR/Z331t2rRR1q1bl9/br1y8eFF5/vnnlXfeeUe5d++eEhcXpzRq1Ej3Pp44cUKJiYlRsrKylIsXLypdunRRli5dmivuwYMHKzdv3tS1/8aNG5Xk5GQlKytLWbx4sdKmTRslPT1dURRFWbhwodKtWzfl7NmzSk5OjhIXF6ckJyc/0gaKoignT55UWrVqpRw7dkzJzs5W1q9fr3Ts2FHJyMhQFEX97Hbv3l25fPmy7rkf/jz36dNH2bBhg6IoinLnzh0lJiYm12vOysrSPdfDn8nU1FSlbdu2yuLFi5X09HQlNTVVOXbsWJ7tN2bMGGXhwoW5bnv4dZw5c0Zp27atrs3OnDmjPP/888qFCxce2de8efOUPn36KIqiKCEhIcq8efPyedceGDlypPK///2v0O1KI+lRlLDXX38dDw8PvLy8qF69OmPHjgXUXz9r167l7bffpmrVqlSqVImRI0cSEREBwLp16+jVqxdt27bFwsICe3t7nnvuOa5fv05UVBRvv/02FSpUoEaNGgwePFj3uIe1aNECjUaj+9W5Y8cO3N3dsbe358SJEyQnJzN69Gisra1xdnamT58+ul+bAO7u7nTq1AkLCwtsbGxy7fvWrVvk5ORQs2bNR563Zs2a3Lx5M9dt48aNw9rampYtW+Ll5cW2bdsKbQMACwsLxo4di7W1NTY2NlSrVg0/Pz9sbW2pVKkSoaGh/Prrr0V6T3x9fWnSpAlWVlZ0795d94s0KiqKevXq0blzZ6ysrBg4cCBPPfVUvvtJSUnJ8/X/2+jRo7GxsaFBgwY0aNBA98u9cePGuLu7Y2VlhZOTEy+99NIjr2XEiBFUrVpV1/5BQUFUq1YNKysrhg4dSmZmJufOnQNg7dq1jBs3jjp16qDRaGjQoAHVqlXLM6Y1a9bw0ksv0bRpUywtLQkODqZcuXK5fkEPGDAAR0fHR957ACsrKy5cuEBycjIVK1Z8ZHgoP7t37+app55i6NChlC9fnkqVKtG0adM8t01NTc1zPi84OBh3d3f8/f1p2bIl/fv3B9B95u737h728GdS3/etYsWK3L59W6/XVdoYd0atDPr6669p06YNhw8fZvz48dy8eRM7OzuSk5O5d+8ePXv21G2rKAo5OTkAXLlyBS8vr0f2d/nyZbKzs2nXrp3utpycHBwdHR/ZVqPR4O/vz5YtW/D09GTz5s10794dgEuXLnH16lVatGih216r1ea6/vDk37/Z2dlhYWHBtWvXeO6553Ldd+3atVxfUHZ2dlSoUEF3/T//+Q9Xr14ttA0AqlWrRvny5XXX7927x+zZs9m7d69uzDktLQ2tVptrQrMgD3/529jYcPfuXUCd4Hz4NWs0mgLboGrVqly7dq1Iz2dra6t7vnPnzvHRRx9x8uRJ7t27h1arpVGjRrke++/3dcmSJaxdu5arV6+i0Wi4c+eO7gswMTGRZ555ptB4QP0cbdy4kRUrVuhuy8rK4urVq/k+98NmzpzJF198QdeuXXFycmL06NF07Nix0Oe9cuWK3jHa2dnlOdezYcMGnnnmGbZt28Znn33G3bt3sba21n3mrl69irOzc67HPPyZ1Pd9S0tLyzWEWJZIojCSli1b0rNnT+bMmcM333xDtWrVsLGxISIiAnt7+0e2d3R0zDV2fp+DgwPW1tYcOnRIr6NKunXrxtChQxkxYgSxsbF8/fXXuv07OTmxc+fOfB+r0Wjyva9ChQq4u7uzfft2WrVqleu+bdu25brt9u3b3L17V5csrly5Qr169Qptg7xiWLJkCefOnWPNmjXUrFmTuLg4evTogVIMB/PVrFmTpKQk3XVFUXTzH3lp3bo1O3fupFevXo/1fO+//z4NGzbks88+o1KlSixbtowdO3bk2ubh13/kyBEWLlzIsmXLqFevnu4os/uv3cHBgQsXLvD8888X+tyOjo6MGjWK0NDQfLcp6P1/9tln+fzzz8nJyWHnzp2MHTuW6OjoAh9z/3nz6v3mpX79+sTHx+cbm7+/P5GRkXz99de888471KlTBwcHB7Zv386rr76q2/Z+jD4+PoD6vu3atYvRo0djYZH/IMvff/9dZg81l6EnIxo0aBAHDhwgLi4OCwsLQkJCmDVrlu547aSkJPbu3QtA7969Wb9+PQcPHiQnJ4ekpCTOnj1LrVq1aNu2LR999BF37twhJyeHCxcucPjw4Tyfs2HDhlSvXp1p06bRrl073S+kJk2aUKlSJRYsWEB6ejparZY///yT2NhYvV/P+PHj2bhxI9999x137tzh1q1bzJ07l2PHjjF69Ohc23755ZdkZmZy5MgRdu/eTZcuXQptg7ykpaVRvnx57OzsSElJ4auvvsp1/1NPPfXYR1x5eXnxxx9/8NNPP5Gdnc3KlSu5fv16vtuPHTuWmJgY5syZo/uFev78eSZMmKDXkEVaWhoVK1akYsWKnD17llWrVhW6vaWlJdWrVyc7O5uvvvqKO3fu6O4PCQlh3rx5xMfHoygKp0+f1vU2/t0uISEhrF69muPHj6MoCnfv3mX37t259leQTZs2kZycjIWFhe4zdT82CwuLfN+DF198kevXr7Ns2TIyMzO5c+cOx48fz3NbLy+vQocVR4wYwZo1a7h27RoajYbJkycTHh7O5s2bSU9P59q1a7zzzjvcuXOHwYMHAzBkyBDS0tKYPHkyly5dAtTP3ezZs3XDgklJSdy6dUvvIbXSRhKFEVWvXp2goCC++eYbACZOnIiLiwt9+vShWbNmDB48WDfe3KRJE2bPns2sWbNo3rw5r7zyCpcvXwbg448/JisrC39/fzw9PRk7dmyBXemAgAAOHDhAt27ddLdZWloSHh7O6dOn8fHxoVWrVkybNk3vLwpQ50AWLVrErl27aN++PR07diQuLo7vv/+eZ599VrfdU089hZ2dHe3bt2fChAm8//77uuGqgtogL4MGDSIjI4NWrVrx0ksv0b59+1z3Dxw4kB07duDp6cmMGTP0fi2gvj/z5s3jk08+4YUXXuDMmTM0btyYcuXK5bn9M888w+rVq7l06RLdunWjefPmjBkzhsaNG+u1Vmby5Mls2bKFZs2a8e677+Lv71/g9u3ataNDhw74+fnh7e1N+fLlcw0PDRkyhK5duzJ06FCaNWvGO++8Q0ZGBqDOk0yZMoUWLVqwdetW3Nzc+PDDDwkLC8PT05POnTuzfv16vdtq7969BAQE4OHhwcyZM5k7dy7ly5fH1taWUaNG0a9fP1q0aPHIUUOVKlViyZIl/PLLL7Rt2xY/P798F/E1atSISpUq5ZtIQO11eHp6snjxYgD8/f35+OOPWbZsGa1atSIgIICMjAxWrVqVa+hp1apVWFlZ0adPHzw8PBg0aBCVK1fWrTPZvHkzPXr0wNraWu82KU1kwZ0oUdHR0UycOFF3aK85ycnJoUOHDnz66aePDK+JkrFv3z6+//573Y+rkpCZmUn37t1ZuXIlNWrUKLHnNSUyRyFEAfbu3UvTpk2xsbFh0aJFAGV2+MEUtGvXLteBGyXB2tqa7du3l+hzmhqDDT1NnTqV1q1b5xreeJiiKMyYMQNfX18CAwP5/fffDRWKEI/t2LFj+Pr68sILL+hKQeR1eKgQpZnBhp5+/fVXKlSooBt3/bc9e/awfPlyFi5cyPHjx5k5c2a+K16FEEIYj8F6FJ6enlSpUiXf+yMjI+nRowcajQZ3d3du376d65htIYQQpsFocxRJSUm5Fi85ODiQlJSU5yrKh/32228FHutcluTk5Ehb/MOc2uLhPvz9v/W7TVPoYwBychQ0Gs1Dt2ke8zk1RXpMXjHqE+/9xxV8f36P1xSxDYs3TnPgwnmqkkKOW4PHnl8zWqLIa8SrsMU5oJZw8PDwMERIZkFRICdH/Xfq1Gnq1Wugu56TA1otel8vyramvq/k5BTs7KoaPQ599mVuXzSPy8ICLC3Vy3///aTX9d327t1U7OwqF8u+ijMug+9Lo6jXLTXYrdyN1c2rXG6Rd2kUfRgtUTg4OORa5ZqYmFhobyIvv/wCBw6YxpdVSewr95dMgyd/I0yQRlP0/yBabUXKl3/8/2xWVmb0JVDIvi5dusCzzz5j1Dj1+M1XIuLiEnB1dTV2GCXr0iUIDYWXXoKXX4Zp6mr7yw9Vqi4qoyUKb29vVqxYQUBAAMePH6dy5cqPlShefx0efv0Pf8mU1H9cKyvDfQkUFOf161dxdKxlkl9WT3L9cb5k4uLOlL0vhHzExaUhTVEGKQosWgQTJkBWFgQEFNuuDZYo3nrrLQ4fPszNmzfp0KEDY8aM0Z3wpF+/fnh5ebFnzx58fX2xtbVl1qxZj/U89+6pSfN//zOtXzIlIS7uBq6uRU+uQohS5uxZePVVdYilY0dYuBD+VZzzSRgsUXz++ecF3q/RaHjvvfee+HkyM8HGRv1FKoQQZdKJE/Dbb7BgAQwfXuy/mM1+ZXZWFpTR8itCiLLs5Ek4ehQGDoQePeDvv8FAJUYsDLLXEpSZCfnUaBNCiNInMxPefx+aNYN33oH7p0s2YB2qUpEopEchhCgToqPVBPHBB+pRTTEx6ti7gZn90JMkCiFEmXDpErRvD/b2sGVLsR7VVBiz7lHcX2MgiUIIUWr9+ad6+fTT8H//B7//XqJJAsw8UWRlqZeSKIQQpU5KCowYAQ0awP3ztwQHgxHO223WQ0+ZmeqlTGYLIUqVH39UV1cnJsLEieDpadRwSkWikB6FEKLUGD4cFi8GNzfYtAlatDB2ROadKGToSQhRKtwv4qbRqInBxQUmTzaZLzezThTSoxBCmL2LF2HUKOjbFwYMUP82MWY9mS1zFEIIs5WTA+Hh0KgR7N4NGRnGjihf0qMQQoiS9tdf6lxEVBR06qTWaKpd29hR5UsShRBClLRTpyA2FpYsgcGDTb7stVknCpnMFkKYjePH4dgxGDQIgoLUIn7Vqhk7Kr2UijkKSRRCCJOVkQHvvqsezfTuuw+K+JlJkoBSkihkMlsIYZIOHgQPD5gxA/r3L7EifsXNrIeepEchhDBZly6Blxc4OMDWrdC1q7Ejemxm3aOQOQohhMmJi1Mvn34a1qxRi/iZcZIAM08U0qMQQpiMmzdh6FBo2BD27lVv69EDKlc2blzFoFQMPckchRDCqDZsgNdeg2vXYOpUoxfxK26lIlFIj0IIYTRDh8LSpeDuDhER6hnoShmzThQyRyGEMIqHi/i1agX16sGECaV2eMOsE4X0KIQQJe78eRg5Uj3cdeBA9eRCpVypmMwupUlcCGFKcnLg66+hcWPYt+/BkEYZID0KIYQozB9/qEX89u2Dzp3h22/h2WeNHVWJkUQhhBCF+eMPdT3EsmXqcJOJF/ErbmadKO73/GToSQhR7GJi1CJ+Q4ZA9+5qEb+qVY0dlVGY/RxFuXJlLrkLIQwpPR3efltdC/H++w+K+JXRJAGlJFEIIUSx2L9fXQ8xe7Y6xHTsmFkW8StuZj30lJkp8xNCiGJy6RJ07KjWaNqxQ520FoCZ9yiysiRRCCGe0KlT6uXTT8MPP8CJE5Ik/sWsE4X0KIQQjy05WT0NaaNG6rmrAQIDoVIlo4Zlisx+6EnmKIQQRfbDD/D663DjBrzzDrRsaeyITJrZJwrpUQghimTwYPjf/9Tifdu3q5PXokBmnShkjkIIoZeHi/i1aQOurjB+PFiZ9VdgiTHoHEVUVBR+fn74+vqyYMGCR+6/fPkyAwYMoEePHgQGBrJnz54i7V96FEKIQp07p05Of/eden3ECJg8WZJEERgsUWi1WsLCwli0aBERERFs2bKFM2fO5NomPDycrl27snHjRubOncsHH3xQpOeQOQohRL60WqotX64W8Tt06EGvQhSZwRJFbGwsLi4uODs7Y21tTUBAAJGRkbm20Wg03LlzB4DU1FRq1apVpOeQHoUQIk9xcdC+PQ6zZ4OXl1qnafBgY0dltgzW90pKSsLBwUF33d7entjY2FzbjB49mmHDhrFixQru3bvH0qVLC91vTk4Ocf+cvPzWLRdsbXOIi7tYvMGbifT0dF1blHXSFg9IW0ClX37B8dQpEj78kHs9e0Jampo8xGMxWKJQ8ujmaf5VlCkiIoLg4GCGDh1KTEwMkyZNYsuWLVhY5N/RsbCwwNXVFVCHGKtVQ3e9rImLiyuzr/3fpC0eKLNt8dtvcPy4empSV1d45RXuXbpUNtsiD0/y48FgQ08ODg4kJibqriclJT0ytLRu3Tq6du0KgIeHBxkZGdy8eVPv55ChJyEE9+7BlCnwwgvw4YcPivjZ2Rk3rlLEYInCzc2N+Ph4Ll68SGZmJhEREXh7e+faxtHRkYMHDwJw9uxZMjIyqF69ut7PIZPZQpRxUVHQtCnMmaPOQcTESBE/AzDY0JOVlRXTp09n+PDhaLVaevXqRb169Zg3bx6NGzfGx8eHKVOmMG3aNJYtW4ZGo+Gjjz56ZHiqINKjEKIMu3QJfHzA2Rl++kn9WxiEQQ8k9vLywsvLK9dt48aN0/1dt25dVq9e/dj7lwV3QpRBJ06Am5taxG/DBrXia8WKxo6qVJOigEII83D9OgwYAE2aPCji162bJIkSYNZLE2WOQogyQFFg7VoYPRpu3oT33lMnrkWJMftEIT0KIUq5QYNg+XJo0QIiI9VhJ1GizDpRyByFEKXUw0X8vLzU4aY33pD6TEZitnMUiiI9CiFKpb//hk6dYNky9fqwYTBhgiQJIzLbRJGdrV5KohCilNBq4b//VYeWfv0VCqjQIEqW2abozEz1UiazhSgFTp1SS29ER0NAAMyfD05Oxo5K/MPsE4X0KIQoBc6dg7Nn4fvvoW9fdW5CmAyzTRRZWeqlJAohzNSvv8KxY/Dqq2ov4u+/oXJlY0cl8mC2g4DSoxDCTN29q05Ot2oFs2c/KOInScJkmX2ikDkKIczI7t3qoa6ffab2JKSIn1kw26En6VEIYWYSEsDXF1xc4Oef1RpNwiyYbY9C5iiEMBPHj6uXTk6waRPExkqSMDNmmyikRyGEibt2Dfr3B3d32LNHvc3fHypUMG5cosjMfuhJ5iiEMDGKAqtXw9ixcOsWfPABtG5t7KjEEzD7RCE9CiFMzIABsHKlWuF18WJo1MjYEYknZLaJQuYohDAhOTnqIjmNRp1/aN5c7VFYWho7MlEMZI5CCPFkzpxRT0O6dKl6fdgwePNNSRKliCQKIcTjyc6GTz9Vi/jFxMh/xlLMbIeeZDJbCCM6eRKGDIEjRyAoCL75Bv7zH2NHJQzE7BOF/IgRwgguXIDz59Wjm/r0kSJ+pZzZJgqZzBaihEVHq4vnRoxQ10P8/TdUqmTsqEQJkDkKIUTB0tLgrbfUtRAffwwZGertkiTKDLNPFDJHIYQB/fyzWsRv7lwYNQqOHoXy5Y0dlShhZjv0JD0KIQwsIQH8/KB2bbUER4cOxo5IGInZ9ihkjkIIA4mJUS+dnGDzZnVeQpJEmWa2iUKGnoQoZklJ8NJL0KzZgyJ+XbqAra1x4xJGZ9aJwtJSFn8K8cQUBVasgIYNYeNGmDED2rQxdlTChJj1HIX0JoQoBv37q+shWrdWi/i5uho7ImFizDZRZGXJ/IQQj+3hIn6dO6tJ4vXXpYsu8mTWQ0+SKIR4DH/+qVZ4XbJEvT5kiFR6FQWSRCFEWZGdrS6Ya9pUPR2pTFILPZnt0JPMUQhRBLGxMHQo/PYbBAfD11+Do6OxoxJmwqwThfQohNBTQgJcvAhr10KvXlLETxSJQYeeoqKi8PPzw9fXlwULFuS5zdatW/H39ycgIIDx48frvW+ZzBaiEAcOwPz56t/3i/j17i1JQhSZwXoUWq2WsLAwli5dir29Pb1798bb25u6devqtomPj2fBggWsWrWKKlWqcOPGDb33Lz0KIfKmSUuDcePgyy/huefUyery5aFiRWOHJsyUwXoUsbGxuLi44OzsjLW1NQEBAURGRubaZs2aNbz88stUqVIFgBo1aui9f5mjECIPO3dSJyhITRKvvy5F/ESxMFiPIikpCQcHB911e3t7YmNjc20THx8PQN++fcnJyWH06NF0KKSmTE5ODnFxcaSkPEN2toa4uPPFHru5SE9PJy4uzthhmARpC7C6coW6AQHkODkR/9133GveXJ2bKMPkc1E8DJYoFEV55DbNv8ZGtVot58+fZ/ny5SQmJvLyyy+zZcsW7Ozs8t2vhYUFrq6ulCun9qRdy/Aq0ri4uDL9+h9Wptvit9+geXN1RfXWrcTXrEkDd3djR2USyvTn4l+eJGEabOjJwcGBxMRE3fWkpCRq1aqVaxt7e3t8fHwoV64czs7O1K5dW9fLKIzMUYgyLzERQkKgRYsHRfx8fVFkqEkUM4MlCjc3N+Lj47l48SKZmZlERETg7e2da5tOnToRHR0NQHJyMvHx8Tg7O+u1f0kUosxSFPjf/9Qifps3w6xZUsRPGJTBhp6srKyYPn06w4cPR6vV0qtXL+rVq8e8efNo3LgxPj4+tG/fnv379+Pv74+lpSWTJk2iWrVqeu1fJrNFmdW3L6xZA23bwqJF0KCBsSMSpZxBF9x5eXnh5eWV67Zx48bp/tZoNEydOpWpU6cWed/SoxBlysNF/Pz9oX17eO01sDDbKjzCjJjtp0wW3Iky4/Rp9Qxzixer1wcNgtGjJUmIEmO2nzTpUYhSLytLnX9o2hROnYJKlYxweLG9AAAgAElEQVQdkSijzLrWk8xRiFLr2DF1RfWxY2rZjS+/hIfWJQlRksw6UUiPQpRaiYnqvx9+gJ49jR2NKOMKTBRLly4t8MFDhgwp1mCKQuYoRKmzb59aDvy116BLFzh7FipUMHZUQhScKNLS0koqjiKTHoUoNVJTYepU9RwR9erBsGFqfSZJEsJEFJgoRo8eXVJxFIlWqx4tKHMUwuzt2AEjRqjnihg3DmbMkCJ+wuQUmChmzJhR4IOnTZtWrMHoKzNTvZQehTBrFy9Ct25Qt6467CSrq4WJKjBRNGrUqKTiKJKsLPVSEoUwO4oCv/4KLVuCszNs2wbt2oGNjbEjEyJfBSaK4ODgkoqjSKRHIczSlSvqOSI2bIDdu8HLCzp1MnZUQhRKr8Njk5OTWbhwIWfOnCEjI0N3+3fffWewwAoiiUKYFUWBZcvgrbcgPR3mzFHrNAlhJvRamT1hwgTq1KlDQkICo0eP5umnn8bNzc3QseXrfqKQyWxhFvr0gaFDwc0Njh+HSZPAymyXMIkySK9EkZKSQkhICFZWVrRs2ZLZs2dz/PhxQ8eWL+lRCJN3/9A8gMBA+OYbdbjp+eeNGpYQj0OvRGH1z6+fWrVqsXv3bk6dOpXrpEQlTSazhUmLi1Oru94v4jdwIISGShE/Ybb06v+GhoaSmprK5MmT+fDDD0lLS3us0uDFRXoUwiRlZanzDx9+qBbwq1LF2BEJUSz0ShQdO3YEoHLlyixfvtygAelD5iiEyYmJgcGD1RIcL70EX3wB/zr1rxDmSq++8OTJk7l9+7bu+q1bt6RHIcTDkpLg+nXYuBFWr5YkIUoVvXoUf/zxB3Z2drrrVapUIS4uzmBBFUbmKIRJiIqCEyfUtRFdusCZM2Bra+yohCh2evUocnJyuHXrlu56SkoKWq3WYEEVRnoUwqhu31YrvHp5qUNM99cWSZIQpZRePYqhQ4fSt29f/Pz80Gg0bNu2jVGjRhk6tnxJohBGs3UrjBwJly+rC+jCwqSInyj19EoUPXr0oHHjxhw6dAhFUfjqq6+oW7euoWPLl0xmC6O4eBGCgqB+fVi3Dl54wdgRCVEi9D6wOyUlBVtbWwYMGED16tW5ePGiIeMqkMxRiBKjKHDokPq3szPs3AlHj0qSEGWKXoniq6++YtGiRSxYsACArKwsJk6caNDACiJDT6JEXL4MPXpA69awZ496W8eO8sETZY5eiWLXrl2Eh4dj+89knb29vVHPfieJQhiUosCiRdCwodqD+PRTKeInyjS95ijKlSuHRqNBo9EAcPfuXYMGVRiZoxAG1bs3rF+vHtW0aJF6YiEhyjC9EkXXrl2ZPn06t2/fZs2aNfzwww+EhIQYOrZ8SY9CFDutFjQatR5Tjx7QuTO8+qrUZxICPRPFsGHD2L9/PxUrVuTcuXOMHTuWtkbsistktihWJ0/C8OEwbJiaHAYMMHZEQpgUvYvit23bVpcctFotP/74I927dzdYYAWRHoUoFpmZMHs2zJypFvCrVs3YEQlhkgrsV9+5c4dvv/2WsLAw9u3bh6IorFixgk6dOrFt27aSivER9xOFpaXRQhDm7rffoHlzeP99CAmBU6fUuQkhxCMK7FFMnDiRKlWq4O7uztq1a1m8eDFZWVl88803uLq6llSMj8jMVHsT/8ytC1F0N25ASgps3gzduhk7GiFMWoGJIiEhgfDwcABCQkJo1aoVv/zyC5UqVSqR4PKTlSXDTuIx/PKLWsRv7Fh1svqvv8DGxthRCWHyChx6snrovL6WlpY4OTkZPUnAgx6FEHq5dUutz+TtDeHhD4r4SZIQQi8F9ihOnz5Ns2bNAFAUhYyMDJo1a4aiKGg0Go4ePVoiQf6bJAqht82bYdQoSEyECRPggw+kiJ8QRVRgojDmOScKkpkpi+2EHi5ehF69oEED9YRCnp7GjkgIs2SWq4lkjkLkS1HgwAH17/tF/I4ckSQhxBMwaKKIiorCz88PX19fXUHBvGzfvp369etz4sQJvfYrQ08iTwkJ0L27WpfpfhG/F1+UD4sQT8hgiUKr1RIWFsaiRYuIiIhgy5YtnDlz5pHt7ty5w/Lly2natKne+5ZEIXLJyaHq//2fWsQvMhI+/xzatTN2VEKUGgZLFLGxsbi4uODs7Iy1tTUBAQFERkY+st28efMYPnw45YswwShzFCKXXr1w/OADdXjp5El4801ZjSlEMdK7hEdRJSUl4eDgoLtub29PbGxsrm1OnTpFYmIiHTt2ZMmSJXrtNycnh5s309BqNcTFnS/WmM1Nenq6yR5wYHDZ2WrBPgsL7Fq1QuvmRlrfvuqhr2W1Tf5Rpj8X/yJtUTwMligURXnkNs1DS6lzcnKYPXs2s2fPLtJ+LSwssLauiI0NRl0dbgri4uLKZhvExqoF/IYPV9dHuLqW3bbIg7TFA9IWDzxJwjTY0JODgwOJiYm660lJSdSqVUt3PS0tjT///JOBAwfi7e3NsWPHCA0N1WtCW+YoyqiMDHjvPbVG0/nzULOmsSMSokwwWI/Czc2N+Ph4Ll68iL29PREREXz22We6+ytXrkx0dLTu+oABA5g0aRJubm6F7lvmKMqgX3+FwYPV4n0DBsDcuVCjhrGjEqJMMFiisLKyYvr06QwfPhytVkuvXr2oV68e8+bNo3Hjxvj4+Dz2vqVHUQbdvAl37sDWrdC1q7GjEaJMMViiAPDy8sLLyyvXbePGjctz2+XLl+u9X1lwV0b8/LNaxG/cOLWI359/SvkNIYzALFdmS4+ilEtJUc805+MD3377oIifJAkhjEIShTAtmzapC+eWLIFJk9QTDEmCEMKoDDr0ZCgymV1KXbignm3O1RV+/BFatDB2REIIzLRHIXMUpYiiwN696t/PPAM//aQe4SRJQgiTYZaJQoaeSokLFyAgADp0eFDEr0MHeXOFMDGSKETJy8mBb76BRo0gKgq++EKK+AlhwsxujkJR1DI/Mkdhxnr2VCetfX1hwQJ49lljRySEKIBZJgqQHoXZeaiIHy+9BEFB6krrh+p/CSFMk9kNPUmiMEPHj8MLL6i9B4B+/WDIEEkSQpgJSRTCcNLTYdo09QimhAR4qOy8EMJ8yNCTMIzDh2HQIDh9Wr38/HOoXt3YUQkhHoPZJgqZzDZxt2/DvXuwfTv4+Rk7GiHEEzDDRKGOa0uPwgTt3Am//66eirRTJ/jjDym/IUQpIHMU4sndvKlOTvv5weLFUsRPiFJGEoV4MuvXq0X8li+HqVPhyBFJEEKUMmY49KReyhyFCbhwAfr2hcaN1RMKeXgYOyIhhAGYXY/iPulRGImiPKjL9Mwz6smFoqMlSQhRipldopDJbCM6f149DemLLz5IFu3aSfdOiFLODBOFeimJogTl5MBXX6lF/Pbtgy+/hPbtjR2VEKKEyByFKFyPHrB5s3pU07ffgouLsSMSQpQgs00U0qMwsKwssLRUi/j16we9e8OAAVKfSYgySIaexKOOHoWWLWH+fPV6v34wcKAkCSHKKEkU4oF799S1EC1bQmIiODsbOyIhhAkww6EnOerJIA4dUov3/fknDB0Kn34K1aoZOyohhAkww0ShXspkdjFLS1PnJXbtUus0CSHEP8w2UUiPohhs364W8Rs/Hnx81JLg0rBCiH+ROYqy6MYNdZipa1f43/8gM1O9XRpVCJEHSRRliaLAunVqEb/vv1fPPvfrr9KYQogCmeHQkzqZLXMUj+HCBejfH5o0Uc8d0bSpsSMSQpgBs+xRWFnJIf16UxS1cB+oK6p371aPcJIkIYTQk1kmChkp0dO5c9C5szpRfb+IX5s2aqYVQgg9SaIojbRamDdPPU9EdDSEh0sRPyHEYzO7n5aSKPQQFAQREeDvr5bhkBXWQognYIaJQiMT2Xl5uIjfgAFqfab+/WUyRwjxxAw69BQVFYWfnx++vr4sWLDgkfuXLl2Kv78/gYGBDBo0iEuXLhW6T+lR5OHIEWjRQh1iAnjpJXj5ZUkSQohiYbBEodVqCQsLY9GiRURERLBlyxbOnDmTaxtXV1d++OEHNm/ejJ+fH5988kmh+5VE8YAmPR0mT4YXXoBr1+Q8EUIIgzBYooiNjcXFxQVnZ2esra0JCAggMjIy1zatWrXC1tYWAHd3dxITEwvdrySKfxw8SO3gYPj4Y7WI36lT0K2bsaMSQpRCBpujSEpKwsHBQXfd3t6e2NjYfLdft24dHTp0KHS/igJa7T3i4uKLI0yzVeH0aRy0Ws4vXszd1q3hyhX1XxmVnp5OXFycscMwCdIWD0hbFA+DJQrlfq2Nh2jyGTPftGkTJ0+eZMWKFXrsF+zsbHF1dX3iGM3O1q1qEb+JE8HVlbjmzXFt0sTYUZmEuLi4svmZyIO0xQPSFg88ScI02NCTg4NDrqGkpKQkatWq9ch2Bw4cYP78+YSHh2Otx5iSomjK3tDT9evwyisQEAArVz4o4ieHfwkhSoDBEoWbmxvx8fFcvHiRzMxMIiIi8Pb2zrXNqVOnmD59OuHh4dSoUUOv/ZapOQpFgdWrwdUV1qyB996Dw4fLUAMIIUyBwYaerKysmD59OsOHD0er1dKrVy/q1avHvHnzaNy4MT4+Pnz88cfcvXuXcePGAeDo6Mj8++dpzoeilKEf0hcuqOXAmzaFxYvBzc3YEQkhyiCDLrjz8vLCy8sr1233kwLAsmXLirzPUt+jUBSIjFTPMufiotZo8vRUF9MJIYQRSK0nU3L2rFrAz9f3QRG/Vq0kSQghjMoME0UpnMzWauHzz9Whpd9+g2+/lSJ+QgiTYYa1nkphjyIwELZtUxfMhYeDk5OxIxJCCB2zTBSlYjI7M1M9L4SFBQwerBby69tX6jMJIUyOGQ49lYIexeHD0Lw5fPONer1PH7XaqyQJIYQJkkRRku7ehfHjoXVruHkTnnvO2BEJIUShzHLoySwTxb596pqIv/+GkSNhzhyoUsXYUQkhRKHMMFGY6YmL7p9Y6Jdf4MUXjR2NEELozewSBZhRj2LzZoiLg0mToGNHtRS4lVk2uRCiDDO7OQowg0Rx7Zp6GtLu3WHVqgdF/CRJCCHMkCSK4qQo8P33ahG/desgLAyio004YCGEKJxZ/sQ12TmKCxdgyBDw8FCL+DVqZOyIhBDiiUmP4knl5MCOHerfLi6wdy/s3y9JQghRakiieBJ//QXe3tClC0RFqbe1bClF/IQQpYokiseRnQ2ffAJNmsCxY+owkxTxE0KUUmY5R2H0RNGtmzrcFBSkluH4z3+MHJAQxpWVlUVCQgLp6enGDiWXrKysJzpXtDmysbHBycmJcsU4mWuWicIok9kZGeoTW1jA8OEwdCiEhEh9JiGAhIQEKleuzLPPPovGhP5P3Lt3D1tbW2OHUWIUReHGjRskJCRQu3btYtuvDD3p49AhaNYMvv5avd67t1rIz4T+QwhhTOnp6dSoUcOkkkRZpNFoqFGjRrH37CRRFCQtDd58E9q0gdRUqFevhJ5YCPMjScI0GOJ9MMuhpxJJFHv3qkX8zp2D116D2bPBzq4EnlgIIUyLWfYoSmSOIjtbfaI9e9QhJ0kSQpi8Xbt2Ub9+fc6ePau7LTo6mpEjR+babsqUKWzfvh1QJ7w//fRTOnfuTLdu3ejduzd77p+z/gl8++23+Pr64ufnx969e/Pc5uDBgwQHB9OtWzcmT55MdnY2AKmpqYwaNYru3bsTEBDADz/8oHvM5cuXGTp0KF27dsXf35+EhIQnjrUw0qN42MaNahG/qVPVIn6//y71mYQwI1u2bKF58+Zs3bqVMWPG6PWYefPmce3aNbZs2YK1tTXXr1/n8OHDTxTHmTNniIiIICIigqSkJIYMGcKOHTuwfGiNVU5ODlOmTGHZsmXUrl2befPmsWHDBkJCQli5ciXPPfcc8+fPJzk5mS5duhAYGIi1tTWTJ09m1KhRtG3blrS0NCwsDP973yy/BYs9USQlwZgxsHatOmk9frz6JJIkhCiy776DJUuKd59Dh8LAgQVvk5aWxtGjR/nuu+8IDQ3VK1Hcu3ePtWvXEhkZifU/XyxPPfUU/v7+TxRvZGQkAQEBWFtb4+zsjIuLC7GxsXh4eOi2SUlJwdraWnd0Utu2bfn2228JCQlBo9GQlpaGoiikpaVRpUoVrKysOHPmDNnZ2bRt2xaAihUrPlGc+jLLb8JiSxSKAitWwBtvwJ07MHMmTJxowsWkhBD5+emnn2jfvj21a9ematWq/P7779SpU6fAx5w/fx5HR0cqVapU6P5nzZpFdHT0I7cHBAQwYsSIXLclJSXRtGlT3XV7e3uSkpJybVOtWjWys7M5ceIEbm5ubN++ncTERABefvllQkNDad++PWlpacydOxcLCwvi4+Oxs7Nj9OjRJCQk0Lp1ayZMmJCrp2IIZTtRXLigrolo0UJdXd2gQTHtWIiya+DAwn/9G0JERASDBg0CwN/fny1btjB27Nh8jwIq6tFBb7/9tt7bKopS6PNpNBo+//xzZs+eTWZmJm3bttV94e/btw9XV1e+++47Lly4wJAhQ2jRogXZ2dkcOXKEjRs34ujoyJtvvsn69esJCQkp0mspKrNMFE/0g/9+Eb+uXdUifvv3q9VepT6TEGbr5s2bHDp0iL/++guNRoNWq0Wj0TBmzBiqVq3KrVu3cm2fkpJCtWrVcHFx4cqVK9y5c6fQXkVRehQODg663gGoPYxatWo98lgPDw++//57QE0O8fHxAKxfv54RI0ag0WhwcXHBycmJv//+GwcHBxo2bIizszMAPj4+HD9+vPAGekJmedTTY/co/vxTPQ2pv796NBOovQlJEkKYtR07dtCjRw9++eUXfv75Z/bs2YOTkxMxMTE8++yzXL16VXck1KVLl/jjjz9wdXXF1taWXr16MXPmTDL/OcHY1atX2bRp0yPP8fbbb7Np06ZH/v07SQB4e3sTERFBZmYmFy9eJD4+niZNmjyy3Y0bNwDIzMxk4cKF9O3bFwBHR0cOHjwIwPXr1zl37hxOTk64ublx69YtkpOTAfWIrrp16xZDCxbMLHsURU4U2dnw2Wfw3ntgawtLl0KHDgaJTQhR8iIiInj11Vdz3da5c2e2bdtG27Zt+eSTT5g6dSoZGRlYWVkxY8YMKleuDMAbb7zBf//7XwICAihfvjy2traMHTv2ieKpV6+e7vBVS0tLpk+frhtWevXVV5kxYwb29vYsWrSI3bt3k5OTQ79+/WjdujUAr732GlOnTiUwMBBFUZgwYQLVq1cHYPLkybohtkaNGhl82AlAo+Q1mGbCli+PIyTEFRubIjzIzw927oSePdU1EQ4OBouvJMXFxeHq6mrsMEyCtMUDxmgLU23/slbr6b683o8neY/Mskeh1xxFerq6oaUljBih/uvVy+CxCSFEaWN2cxQajR5TCvv3g7v7gyJ+vXpJkhBCiMdkhomigJGyO3dg7Fj1JELp6WCCXWEhSiszG8UutQzxPphhosjnjj17oHFj+OorGD0aTp4EX98SjU2IssrGxoYbN25IsjCy++ejsCnSJG7hzHKOIl8VKqhVX/9Z3i6EKBlOTk4kJCRw7do1Y4eSS1ZWVrGe6c0c3D/DXXEyu0SRq0exfj2cPg1vvw1eXnDihKyJEMIIypUrV6xnVCsupno0lrkx6NBTVFQUfn5++Pr6smDBgkfuz8zM5I033sDX15eQkBC9yuVqNEBionqWuV69YMMG+GehjCQJIYQofgZLFFqtlrCwMBYtWkRERARbtmzhzJkzubZZu3YtdnZ27Nq1i8GDB/Ppp58Wut/qynV1knrLFvVkQgcOGOHcqEIIUXYYLFHExsbi4uKCs7Mz1tbWBAQEEBkZmWubn3/+meDgYAD8/Pw4ePBgoZNhT2dfVCetjx+HKVOk0qsQQhiYweYokpKScHhoBbS9vT2xsbGPbOPo6KgGYmVF5cqVuXnzpm6pel6y3BoRt2CBWtwvLs4wwZuROGkDHWmLB6QtHpC2UGVkZDz2Yw2WKPQps6vPNv/m7u7+ZIEJIYQoEoMNPelTZtfBwYErV64AkJ2dTWpqKlWrVjVUSEIIIR6DwRKFm5sb8fHxXLx4kczMTCIiIvD29s61jbe3Nxs2bADUMsGtWrUq8slEhBBCGJZBq8fu2bOHWbNmodVq6dWrF6GhocybN4/GjRvj4+NDRkYGEydOJC4ujipVqjB37lzdCTmEEEKYBrMrMy6EEKJkmV2tJyGEECVLEoUQQogCmWyiMET5D3NVWFssXboUf39/AgMDGTRoEJcuXTJClCWjsLa4b/v27dSvX58TJ06UYHQlS5+22Lp1K/7+/gQEBDB+/PgSjrDkFNYWly9fZsCAAfTo0YPAwED27NljhCgNb+rUqbRu3Zpu3brleb+iKMyYMQNfX18CAwP5/fff9duxYoKys7MVHx8f5cKFC0pGRoYSGBio/PXXX7m2WbFihfLuu+8qiqIoW7ZsUcaNG2eMUA1On7Y4ePCgcvfuXUVRFGXlypVlui0URVFSU1OV/v37KyEhIUpsbKwRIjU8fdri3LlzSlBQkJKSkqIoiqJcv37dGKEanD5tMW3aNGXlypWKoijKX3/9pXTs2NEYoRrc4cOHlZMnTyoBAQF53r97925l2LBhSk5OjhITE6P07t1br/2aZI/CUOU/zJE+bdGqVSvdeYHd3d1zrV8pTfRpC4B58+YxfPhwypcvb4QoS4Y+bbFmzRpefvllqlSpAkCNGjWMEarB6dMWGo2GO3fuAJCamvrImq7SwtPTU/d+5yUyMpIePXqg0Whwd3fn9u3bXL16tdD9mmSiyKv8R1JS0iPb5FX+o7TRpy0etm7dOjp06FASoZU4fdri1KlTJCYm0rFjx5IOr0Tp0xbx8fGcO3eOvn370qdPH6Kioko6zBKhT1uMHj2azZs306FDB0aMGMG0adNKOkyT8O+2cnBwKPD75D6TTBR59QyKo/yHOSrK69y0aRMnT55k+PDhhg7LKApri5ycHGbPns3kyZNLMiyj0OdzodVqOX/+PMuXL+ezzz5j2rRp3L59u6RCLDH6tEVERATBwcFERUWxYMECJk2aRE5OTkmFaDIe93vTJBOFlP94QJ+2ADhw4ADz588nPDwc61Jadr2wtkhLS+PPP/9k4MCBeHt7c+zYMUJDQ0vlhLY+nwt7e3t8fHwoV64czs7O1K5dm/j4+BKO1PD0aYt169bRtWtXADw8PMjIyCiVIxCF+XdbJSYm6jUMZ5KJQsp/PKBPW5w6dYrp06cTHh5easehofC2qFy5MtHR0fz888/8/PPPuLu7Ex4ejpubmxGjNgx9PhedOnUiOjoagOTkZOLj40tl5QN92sLR0ZGDBw8CcPbsWTIyMgqsUl1aeXt7s3HjRhRF4dixY1SuXFmvRGGSp0K1srJi+vTpDB8+XFf+o169ernKf/Tu3ZuJEyfi6+urK/9RGunTFh9//DF3795l3LhxgPqfYv78+UaOvPjp0xZlhT5t0b59e/bv34+/vz+WlpZMmjSJatWqGTv0YqdPW0yZMoVp06axbNkyNBoNH330Uan8YfnWW29x+PBhbt68SYcOHRgzZgzZ2dkA9OvXDy8vL/bs2YOvry+2trbMmjVLr/1KCQ8hhBAFMsmhJyGEEKZDEoUQQogCSaIQQghRIEkUQgghCiSJQgghRIEkUYgS5+rqSlBQkO5fQZV/ExISdJUwo6OjGTlyZLHEEB0dzdGjR/O9/6effuKrr74C4NdffyU4OJiGDRuyffv2fB/z999/M2DAAIKCgujatSvvvvtuscR6X2RkpK4yanJyMiEhIfTo0YMjR47w6quvFrjqetWqVWzcuBGA9evX61W2YfDgwdy6dat4ghdmzSTXUYjSzcbGhk2bNhk1hsOHD1OhQgWaNWuW5/2LFi3im2++AdR1KbNnz2bJkiUF7nPmzJkMGjSITp06AfDHH38Ua8w+Pj66tSIHDx6kTp06zJkzB4AWLVoU+Nh+/frp/t6wYQP16tXD3t6+wMcEBQXx/fffExoa+oSRC3MnPQphEhISEujfvz/BwcEEBwcX+Gs/LwcPHtSda2Dq1KlkZmYC6krU5ORkAE6cOMGAAQNISEhg9erVLFu2jKCgII4cOZJrX+fOnaNcuXK6lbtOTk40aNAAC4uC/7tcvXo1V8G1+vXrA+ov+NDQUIYNG4afn5+upwJqfa7evXsTFBTE9OnT0Wq1gHp+heDgYLp3786gQYN0+wkLCyMuLo5PPvmEPXv2EBQURHp6eq7XuXHjRgIDA+nevTsTJ04E4Msvv2Tx4sVs376dkydPMmHCBIKCgti9ezevv/66Lp79+/czevRoXdtFREQU5W0QpZT0KESJS09PJygoCFC/hL/++mtq1KjB0qVLKV++PPHx8bz11lusX79er/1lZGQwZcoUli1bRu3atZk0aRLff/89gwcPznN7Jycn+vbtS4UKFRg2bNgj9x89epRGjRoV+XUNHjyYQYMG4eHhQbt27ejZsyd2dnaAmqQ2b96Mra0tvXv3xsvLiwoVKrBt2zZWrVpFuXLleP/993UVTt99911WrFiBs7MzKSkpuZ7H1dWVsWPHcvLkSaZPn57rvr/++ovw8HBWrVpF9erVH3lsly5dWLlyJZMmTcLNzQ1FUfjoo49ITk6mevXqrF+/np49ewJQpUoVMjMzuXnzZqlc0S30J4lClLi8hp6ys7MJCwvj9OnTWFhYFKl43blz53BycqJ27doABAcHs3LlynwTRWGuXbv2WHWAevXqRbt27di7dy+RkZGsXr2aH3/8EYA2bdrovmx9fX357bffsLKy4uTJk/Tu3RtQE2iNGjU4duwYLVq00NVlKkqxy0OHDtGlSxdd/IU9VqPREBQUxI8//kjPnj2JiYnRDWcBVK9enatXr0qiKOMkUQiTsGzZMp566ik2bdpETk4OTZo0KXD7YZ/+YIAAAAKzSURBVMOGcf36dRo3bswrr7yS73aWlpa60soZGRl6xWJjY0Nqamqh282dO5fdu3cD6BKfvb09vXv3pnfv3nTr1o0///wTeLSUs0ajQVEUgoODHzlFaWRk5GPXIXqcijw9e/YkNDQUa2trunTpgpXVg6+FzMxMbGxsHisWUXrIHIUwCampqdSsWRMLCws2bdqkG6vPz+LFi9m0aRMzZ86kTp06XLp0ifPnzwPql7anpycATz/9NCdPngRg586dusdXrFiRtLS0PPddp04d3b4K8uabb7Jp0yZdkoiKiiIrKwtQeyUpKSm6CeP9+/eTkpJCeno6P/30E82aNaN169bs2LGDGzduAJCSksKlS5fw8PDg119/5eLFi7rb9dW6dWu2b9+uK6Gd12P//drt7e2pVasW4eHhumEnUJPOtWvXePrpp/V+flE6SaIQJqF///5s2LCBPn36EB8fT4UKFfR+bPny5Zk9ezbjxo0jMDAQjUajO8pn9OjRzJo1i/79+2Npaal7TMeOHdm1a1eek9menp7ExcXpfp3HxsbSoUMHtm/fznvvvUdAQECecezfv59u3brRvXt3hg0bxsSJE6lZsyYAzZs3Z9KkSQQFBeHn54ebmxt169bljTfeYOjQoQQGBjJ06FDdsFdYWBhjxoyhe/fuvPnmm3q3Rb169Rg1ahQDBgyge/fufPTRR49sExwczHvvvaebCAcIDAzE0dGRunXr6rY7efIk7u7uuXoYomyS6rFC5GHGjBl4e3vTpk2bJ97X+vXr85x4NiVhYWG4uroSEhKiu23GjBn4+PjQunVrI0YmTIH0KITIw6hRo7h3756xwygRPXv25I8//tAdiXbf888/L0lCANKjEEIIUQjpUQghhCiQJAohhBAFkkQhhBCiQJIohBBCFEgShRBCiAL9P/5vny4N+1X6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sp_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_spsam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  3 11:32:29 2019\n",
      "Train on 1064361 samples, validate on 266091 samples\n",
      "Epoch 1/200\n",
      "1064361/1064361 [==============================] - 22s 21us/step - loss: 0.2238 - acc: 0.8861 - val_loss: 0.1619 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16189, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1064361/1064361 [==============================] - 21s 20us/step - loss: 0.1264 - acc: 0.9339 - val_loss: 0.1133 - val_acc: 0.9420\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16189 to 0.11330, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1064361/1064361 [==============================] - 22s 20us/step - loss: 0.1074 - acc: 0.9446 - val_loss: 0.1080 - val_acc: 0.9452\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11330 to 0.10805, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1064361/1064361 [==============================] - 22s 20us/step - loss: 0.0930 - acc: 0.9529 - val_loss: 0.0838 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10805 to 0.08377, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1064361/1064361 [==============================] - 22s 21us/step - loss: 0.0816 - acc: 0.9595 - val_loss: 0.0739 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08377 to 0.07386, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1064361/1064361 [==============================] - 22s 21us/step - loss: 0.0728 - acc: 0.9644 - val_loss: 0.0638 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07386 to 0.06375, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1064361/1064361 [==============================] - 22s 21us/step - loss: 0.0661 - acc: 0.9679 - val_loss: 0.0628 - val_acc: 0.9685\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06375 to 0.06283, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1064361/1064361 [==============================] - 22s 21us/step - loss: 0.0601 - acc: 0.9712 - val_loss: 0.0534 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06283 to 0.05341, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1064361/1064361 [==============================] - 23s 21us/step - loss: 0.0561 - acc: 0.9732 - val_loss: 0.0504 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05341 to 0.05042, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1064361/1064361 [==============================] - 22s 21us/step - loss: 0.0521 - acc: 0.9750 - val_loss: 0.0474 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05042 to 0.04744, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1064361/1064361 [==============================] - 22s 21us/step - loss: 0.0492 - acc: 0.9764 - val_loss: 0.0509 - val_acc: 0.9756\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.04744\n",
      "Epoch 12/200\n",
      "1064361/1064361 [==============================] - 23s 22us/step - loss: 0.0468 - acc: 0.9777 - val_loss: 0.0453 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04744 to 0.04534, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1064361/1064361 [==============================] - 23s 21us/step - loss: 0.0443 - acc: 0.9788 - val_loss: 0.0452 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.04534 to 0.04516, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1064361/1064361 [==============================] - 23s 21us/step - loss: 0.0426 - acc: 0.9798 - val_loss: 0.0438 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04516 to 0.04383, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1064361/1064361 [==============================] - 23s 22us/step - loss: 0.0408 - acc: 0.9804 - val_loss: 0.0405 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.04383 to 0.04051, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 16/200\n",
      "1064361/1064361 [==============================] - 23s 22us/step - loss: 0.0396 - acc: 0.9810 - val_loss: 0.0392 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.04051 to 0.03924, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1064361/1064361 [==============================] - 23s 22us/step - loss: 0.0382 - acc: 0.9817 - val_loss: 0.0389 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03924 to 0.03886, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1064361/1064361 [==============================] - 23s 22us/step - loss: 0.0371 - acc: 0.9821 - val_loss: 0.0339 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03886 to 0.03392, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 19/200\n",
      "1064361/1064361 [==============================] - 24s 22us/step - loss: 0.0360 - acc: 0.9828 - val_loss: 0.0363 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.03392\n",
      "Epoch 20/200\n",
      "1064361/1064361 [==============================] - 24s 22us/step - loss: 0.0351 - acc: 0.9831 - val_loss: 0.0310 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.03392 to 0.03104, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1064361/1064361 [==============================] - 24s 22us/step - loss: 0.0341 - acc: 0.9835 - val_loss: 0.0305 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.03104 to 0.03048, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 22/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0335 - acc: 0.9840 - val_loss: 0.0306 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03048\n",
      "Epoch 23/200\n",
      "1064361/1064361 [==============================] - 24s 22us/step - loss: 0.0329 - acc: 0.9842 - val_loss: 0.0339 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03048\n",
      "Epoch 24/200\n",
      "1064361/1064361 [==============================] - 24s 22us/step - loss: 0.0324 - acc: 0.9845 - val_loss: 0.0327 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03048\n",
      "Epoch 25/200\n",
      "1064361/1064361 [==============================] - 23s 22us/step - loss: 0.0322 - acc: 0.9848 - val_loss: 0.0288 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.03048 to 0.02882, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 26/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0316 - acc: 0.9850 - val_loss: 0.0295 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02882\n",
      "Epoch 27/200\n",
      "1064361/1064361 [==============================] - 24s 22us/step - loss: 0.0313 - acc: 0.9852 - val_loss: 0.0305 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02882\n",
      "Epoch 28/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0306 - acc: 0.9854 - val_loss: 0.0336 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02882\n",
      "Epoch 29/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0299 - acc: 0.9857 - val_loss: 0.0274 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.02882 to 0.02743, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 30/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0302 - acc: 0.9856 - val_loss: 0.0283 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02743\n",
      "Epoch 31/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0294 - acc: 0.9859 - val_loss: 0.0293 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02743\n",
      "Epoch 32/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0292 - acc: 0.9860 - val_loss: 0.0276 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02743\n",
      "Epoch 33/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0288 - acc: 0.9862 - val_loss: 0.0291 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02743\n",
      "Epoch 34/200\n",
      "1064361/1064361 [==============================] - 24s 23us/step - loss: 0.0288 - acc: 0.9863 - val_loss: 0.0316 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02743\n",
      "Time elapsed (hh:mm:ss.ms) 0:13:10.686781\n"
     ]
    }
   ],
   "source": [
    "hist_sp_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = sp_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_spsam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_sp_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_sp_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_sp_ann_2h_unisoftsigbinlosadam, './Figures/sp_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict(sp_ann_2h_unisoftsigbinlosadam,enc_test_x_spsam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_spsam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  3 11:45:40 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.3708 - acc: 0.8142\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1726 - acc: 0.9136\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1459 - acc: 0.9251\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1338 - acc: 0.9303\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1245 - acc: 0.9353\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1185 - acc: 0.9391\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1121 - acc: 0.9425\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1068 - acc: 0.9451\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1013 - acc: 0.9478\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0976 - acc: 0.9508\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0923 - acc: 0.9533\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0884 - acc: 0.9557\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0854 - acc: 0.9574\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0810 - acc: 0.9594\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0780 - acc: 0.9614\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0744 - acc: 0.9639\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0719 - acc: 0.9650\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0690 - acc: 0.9666\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0664 - acc: 0.9676\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0659 - acc: 0.9680\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0627 - acc: 0.9699\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0604 - acc: 0.9707\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0592 - acc: 0.9715\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0574 - acc: 0.9723\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0558 - acc: 0.9731\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0549 - acc: 0.9734\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0535 - acc: 0.9744\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0524 - acc: 0.9747\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0512 - acc: 0.9753\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0490 - acc: 0.9766\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0484 - acc: 0.9766\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0471 - acc: 0.9777\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0477 - acc: 0.9773\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0459 - acc: 0.9779\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0456 - acc: 0.9783\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0447 - acc: 0.9785\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0431 - acc: 0.9791\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0437 - acc: 0.9793\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0427 - acc: 0.9796\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0411 - acc: 0.9805\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0407 - acc: 0.9803\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0415 - acc: 0.9802\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0393 - acc: 0.9811\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0387 - acc: 0.9814\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0385 - acc: 0.9816\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0373 - acc: 0.9820\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0377 - acc: 0.9819\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0374 - acc: 0.9819\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0363 - acc: 0.9828\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0360 - acc: 0.9829\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0365 - acc: 0.9825\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0355 - acc: 0.9831\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0358 - acc: 0.9829\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0350 - acc: 0.9832\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0342 - acc: 0.9835\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0343 - acc: 0.9836\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0339 - acc: 0.9841\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0331 - acc: 0.9843\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0333 - acc: 0.9841\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0338 - acc: 0.9838\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0320 - acc: 0.9849\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0329 - acc: 0.9845\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0324 - acc: 0.9845\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0318 - acc: 0.9849\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0316 - acc: 0.9851\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0325 - acc: 0.9844\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0310 - acc: 0.9854\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0312 - acc: 0.9851\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0310 - acc: 0.9857\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0311 - acc: 0.9852\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0314 - acc: 0.9851\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0294 - acc: 0.9858\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0308 - acc: 0.9857\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0297 - acc: 0.9857\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0294 - acc: 0.9858\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0297 - acc: 0.9859\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0286 - acc: 0.9864\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9858\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0287 - acc: 0.9862\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0290 - acc: 0.9862\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0292 - acc: 0.9863\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0288 - acc: 0.9865\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0282 - acc: 0.9865\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0282 - acc: 0.9866\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0277 - acc: 0.9867\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0281 - acc: 0.9867\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0285 - acc: 0.9867\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0285 - acc: 0.9867\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0269 - acc: 0.9871\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0281 - acc: 0.9867\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0274 - acc: 0.9872\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0278 - acc: 0.9867\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0271 - acc: 0.9872\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0272 - acc: 0.9870\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0278 - acc: 0.9870\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0269 - acc: 0.9874\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0275 - acc: 0.9874\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0269 - acc: 0.9875\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0268 - acc: 0.9874\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0271 - acc: 0.9873\n",
      "83154/83154 [==============================] - 1s 7us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.3630 - acc: 0.8158\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.1721 - acc: 0.9130\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.1459 - acc: 0.9241\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1350 - acc: 0.9285\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1274 - acc: 0.9334\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.1215 - acc: 0.9365\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.1122 - acc: 0.9418\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.1078 - acc: 0.9446\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.1018 - acc: 0.9484\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0956 - acc: 0.9518\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0918 - acc: 0.9541\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0864 - acc: 0.9569\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0833 - acc: 0.9587\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0784 - acc: 0.9614\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0759 - acc: 0.9630\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0717 - acc: 0.9646\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0705 - acc: 0.9659\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0674 - acc: 0.9675\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0648 - acc: 0.9690\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0641 - acc: 0.9690\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0597 - acc: 0.9714\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0591 - acc: 0.9718\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0580 - acc: 0.9726\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0550 - acc: 0.9736\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0548 - acc: 0.9739\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0531 - acc: 0.9746\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0521 - acc: 0.9746\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0516 - acc: 0.9752\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0502 - acc: 0.9761\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0491 - acc: 0.9765\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0471 - acc: 0.9774\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0467 - acc: 0.9777\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0460 - acc: 0.9779\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0460 - acc: 0.9777\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0447 - acc: 0.9784\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0438 - acc: 0.9788\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0430 - acc: 0.9791\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0427 - acc: 0.9794\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0425 - acc: 0.9793\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0415 - acc: 0.9798\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0404 - acc: 0.9804\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0404 - acc: 0.9802\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0396 - acc: 0.9808\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0383 - acc: 0.9813\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0391 - acc: 0.9809\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0391 - acc: 0.9811\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0376 - acc: 0.9817\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0378 - acc: 0.9816\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0374 - acc: 0.9818\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0365 - acc: 0.9821\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0363 - acc: 0.9823\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0352 - acc: 0.9829\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0359 - acc: 0.9826\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0352 - acc: 0.9826\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0353 - acc: 0.9829\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0347 - acc: 0.9831\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0338 - acc: 0.9835\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0336 - acc: 0.9838\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0338 - acc: 0.9835\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0335 - acc: 0.9838\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0328 - acc: 0.9838\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0333 - acc: 0.9838\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0321 - acc: 0.9842\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0325 - acc: 0.9839\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0323 - acc: 0.9844\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0312 - acc: 0.9846\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0319 - acc: 0.9845\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0313 - acc: 0.9847\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0310 - acc: 0.9848\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0308 - acc: 0.9850\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0307 - acc: 0.9850\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0310 - acc: 0.9852\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0304 - acc: 0.9851\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0304 - acc: 0.9853\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0294 - acc: 0.9858\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0307 - acc: 0.9855\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0305 - acc: 0.9851\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0287 - acc: 0.9860\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0287 - acc: 0.9861\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0302 - acc: 0.9856\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0288 - acc: 0.9860\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0289 - acc: 0.9862\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0287 - acc: 0.9863\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0288 - acc: 0.9862\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0276 - acc: 0.9867\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0287 - acc: 0.9863\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0279 - acc: 0.9867\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0284 - acc: 0.9864\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0281 - acc: 0.9863\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0281 - acc: 0.9866\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0283 - acc: 0.9866\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0274 - acc: 0.9869\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0279 - acc: 0.9864\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0280 - acc: 0.9867\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0273 - acc: 0.9870\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0269 - acc: 0.9871\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.0278 - acc: 0.9867\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0267 - acc: 0.9872\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0273 - acc: 0.9872\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0263 - acc: 0.9875\n",
      "83154/83154 [==============================] - 1s 8us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 7s 22us/step - loss: 0.3711 - acc: 0.8136\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1720 - acc: 0.9139\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1475 - acc: 0.9235\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1350 - acc: 0.9289\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1264 - acc: 0.9338\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1180 - acc: 0.9388\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1120 - acc: 0.9426\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1058 - acc: 0.9458\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1001 - acc: 0.9491\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0961 - acc: 0.9512\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0909 - acc: 0.9542\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0878 - acc: 0.9561\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0840 - acc: 0.9578\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0804 - acc: 0.9599\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0773 - acc: 0.9619\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0744 - acc: 0.9632\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0723 - acc: 0.9643\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0689 - acc: 0.9663\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0668 - acc: 0.9672\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0652 - acc: 0.9681\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0625 - acc: 0.9695\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0608 - acc: 0.9706\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0597 - acc: 0.9713\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0573 - acc: 0.9722\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0574 - acc: 0.9722\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0551 - acc: 0.9734\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0548 - acc: 0.9734\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0518 - acc: 0.9750\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0521 - acc: 0.9749\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0504 - acc: 0.9758\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0494 - acc: 0.9756\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0488 - acc: 0.9764\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0469 - acc: 0.9772\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0464 - acc: 0.9775\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0457 - acc: 0.9778\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0459 - acc: 0.9777\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0436 - acc: 0.9786\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0432 - acc: 0.9787\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0430 - acc: 0.9787\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0421 - acc: 0.9796\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0418 - acc: 0.9795\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0412 - acc: 0.9799\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0410 - acc: 0.9798\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0410 - acc: 0.9800\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0401 - acc: 0.9803\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0395 - acc: 0.9806\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0389 - acc: 0.9808\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0385 - acc: 0.9809\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0381 - acc: 0.9812\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0367 - acc: 0.9818\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0374 - acc: 0.9818\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0376 - acc: 0.9815\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0370 - acc: 0.9819\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0361 - acc: 0.9823\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0358 - acc: 0.9821\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0360 - acc: 0.9823\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0355 - acc: 0.9824\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0347 - acc: 0.9828\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0346 - acc: 0.9830\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0347 - acc: 0.9828\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0350 - acc: 0.9826\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0341 - acc: 0.9831\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0338 - acc: 0.9834\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0329 - acc: 0.9838\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0336 - acc: 0.9837\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0322 - acc: 0.9840\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0329 - acc: 0.9839\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0323 - acc: 0.9842\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0321 - acc: 0.9844\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0322 - acc: 0.9843\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0321 - acc: 0.9843\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0315 - acc: 0.9847\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0310 - acc: 0.9847\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0311 - acc: 0.9850\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0313 - acc: 0.9850\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0309 - acc: 0.9849\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0309 - acc: 0.9849\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0311 - acc: 0.9851\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0298 - acc: 0.9855\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9857\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0298 - acc: 0.9856\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0302 - acc: 0.9857\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0296 - acc: 0.9856\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0294 - acc: 0.9860\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0292 - acc: 0.9861\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0287 - acc: 0.9862\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0298 - acc: 0.9858\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0289 - acc: 0.9863\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0290 - acc: 0.9862\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0288 - acc: 0.9863\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0288 - acc: 0.9861\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0284 - acc: 0.9862\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0288 - acc: 0.9860\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0290 - acc: 0.9859\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0274 - acc: 0.9868\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0290 - acc: 0.9862\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0277 - acc: 0.9866\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0275 - acc: 0.9870\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0280 - acc: 0.9868\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0280 - acc: 0.9867\n",
      "83153/83153 [==============================] - 1s 9us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 7s 22us/step - loss: 0.3599 - acc: 0.8196\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1706 - acc: 0.9145\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1463 - acc: 0.9250\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1316 - acc: 0.9317\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1243 - acc: 0.9363\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1141 - acc: 0.9417\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1090 - acc: 0.9448\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1026 - acc: 0.9485\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0969 - acc: 0.9512\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0910 - acc: 0.9546\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0870 - acc: 0.9570\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0824 - acc: 0.9596\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0793 - acc: 0.9611\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0752 - acc: 0.9633\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0723 - acc: 0.9647\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0695 - acc: 0.9664\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0675 - acc: 0.9674\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0654 - acc: 0.9686\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0625 - acc: 0.9700\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0612 - acc: 0.9708\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0592 - acc: 0.9715\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0563 - acc: 0.9728\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0569 - acc: 0.9731\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0537 - acc: 0.9747\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0535 - acc: 0.9746\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0518 - acc: 0.9754\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0502 - acc: 0.9762\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0482 - acc: 0.9770\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0482 - acc: 0.9771\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 6s 17us/step - loss: 0.0476 - acc: 0.9773\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0462 - acc: 0.9780\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0462 - acc: 0.9778\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0445 - acc: 0.9790\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0439 - acc: 0.9790\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0433 - acc: 0.9796\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0425 - acc: 0.9800\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0419 - acc: 0.9800\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0416 - acc: 0.9803\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0413 - acc: 0.9804\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0398 - acc: 0.9812\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0395 - acc: 0.9811\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0394 - acc: 0.9813\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0379 - acc: 0.9818\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0385 - acc: 0.9816\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0383 - acc: 0.9816\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0368 - acc: 0.9824\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0369 - acc: 0.9822\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0356 - acc: 0.9830\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0365 - acc: 0.9825\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0357 - acc: 0.9828\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0352 - acc: 0.9831\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0346 - acc: 0.9835\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0345 - acc: 0.9834\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0342 - acc: 0.9835\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0337 - acc: 0.9837\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0336 - acc: 0.9838\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0328 - acc: 0.9843\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0333 - acc: 0.9842\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0327 - acc: 0.9843\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0326 - acc: 0.9845\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0319 - acc: 0.9846\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0316 - acc: 0.9848\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0311 - acc: 0.9851\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0311 - acc: 0.9852\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0325 - acc: 0.9844\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0306 - acc: 0.9853\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0309 - acc: 0.9852\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0307 - acc: 0.9852\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0300 - acc: 0.9855\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0302 - acc: 0.9855\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0297 - acc: 0.9857\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0301 - acc: 0.9858\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0299 - acc: 0.9858\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0288 - acc: 0.9861\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0297 - acc: 0.9860\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0294 - acc: 0.9860\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0287 - acc: 0.9862\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0282 - acc: 0.9864\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0286 - acc: 0.9863\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0282 - acc: 0.9867\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0287 - acc: 0.9863\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0280 - acc: 0.9867\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0285 - acc: 0.9864\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0277 - acc: 0.9866\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0280 - acc: 0.9867\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0274 - acc: 0.9869\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0273 - acc: 0.9870\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0283 - acc: 0.9865\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0272 - acc: 0.9872\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0295 - acc: 0.9868\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0278 - acc: 0.9869\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0264 - acc: 0.9874\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0271 - acc: 0.9874\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0269 - acc: 0.9873\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0268 - acc: 0.9872\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0263 - acc: 0.9875\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0266 - acc: 0.9875\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0271 - acc: 0.9872\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0259 - acc: 0.9876\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0268 - acc: 0.9874\n",
      "83153/83153 [==============================] - 1s 8us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.3579 - acc: 0.8190\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1651 - acc: 0.9157\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1426 - acc: 0.9259\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.1323 - acc: 0.9304\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1250 - acc: 0.9337\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.1181 - acc: 0.9379\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1118 - acc: 0.9413\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.1064 - acc: 0.9446\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0996 - acc: 0.9482\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0970 - acc: 0.9498\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0914 - acc: 0.9532\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0873 - acc: 0.9556\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0843 - acc: 0.9577\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0808 - acc: 0.9595\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0764 - acc: 0.9616\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0740 - acc: 0.9633\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0704 - acc: 0.9653\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0677 - acc: 0.9669\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0650 - acc: 0.9683\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0632 - acc: 0.9693\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0607 - acc: 0.9704\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0593 - acc: 0.9710\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0582 - acc: 0.9718\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0563 - acc: 0.9727\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0538 - acc: 0.9739\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0528 - acc: 0.9745\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0516 - acc: 0.9748\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0497 - acc: 0.9756\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0494 - acc: 0.9761\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0485 - acc: 0.9766\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0470 - acc: 0.9773\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0468 - acc: 0.9773\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0448 - acc: 0.9782\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0442 - acc: 0.9785\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0448 - acc: 0.9781\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0427 - acc: 0.9789\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0438 - acc: 0.9787\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0419 - acc: 0.9795\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0420 - acc: 0.9794\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0412 - acc: 0.9800\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0399 - acc: 0.9804\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0394 - acc: 0.9809\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0399 - acc: 0.9807\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0379 - acc: 0.9814\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0382 - acc: 0.9812\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0388 - acc: 0.9811\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0370 - acc: 0.9818\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0365 - acc: 0.9821\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0366 - acc: 0.9822\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0362 - acc: 0.9822\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0351 - acc: 0.9828\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0353 - acc: 0.9825\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0357 - acc: 0.9824\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0341 - acc: 0.9833\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0342 - acc: 0.9830\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0344 - acc: 0.9831\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0333 - acc: 0.9836\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0327 - acc: 0.9841\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0327 - acc: 0.9841\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0332 - acc: 0.9838\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0323 - acc: 0.9841\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0328 - acc: 0.9841\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0320 - acc: 0.9843\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0317 - acc: 0.9845\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0310 - acc: 0.9847\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0317 - acc: 0.9845\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0310 - acc: 0.9849\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0316 - acc: 0.9848\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0299 - acc: 0.9855\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0308 - acc: 0.9853\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0302 - acc: 0.9854\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0299 - acc: 0.9856\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0300 - acc: 0.9853\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0294 - acc: 0.9858\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0297 - acc: 0.9856\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0295 - acc: 0.9857\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0293 - acc: 0.9861\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0293 - acc: 0.9857\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0292 - acc: 0.9861\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0287 - acc: 0.9860\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0293 - acc: 0.9858\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0284 - acc: 0.9858\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0289 - acc: 0.9862\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0282 - acc: 0.9864\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0283 - acc: 0.9864\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0286 - acc: 0.9861\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0280 - acc: 0.9865\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0286 - acc: 0.9863\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0280 - acc: 0.9866\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0279 - acc: 0.9866\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0279 - acc: 0.9867\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0279 - acc: 0.9866\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0283 - acc: 0.9864\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0270 - acc: 0.9870\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0277 - acc: 0.9870\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0272 - acc: 0.9867\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0273 - acc: 0.9868\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0284 - acc: 0.9866\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 5s 16us/step - loss: 0.0267 - acc: 0.9873\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 5s 15us/step - loss: 0.0267 - acc: 0.9871\n",
      "83153/83153 [==============================] - 1s 9us/step\n",
      "Time elapsed (hh:mm:ss.ms) 0:48:37.120242\n",
      "Overall accuracy of Neural Network model: 0.9863962267327614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 48.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9863    0.9864    0.9864    207840\n",
      "           1     0.9864    0.9864    0.9864    207927\n",
      "\n",
      "   micro avg     0.9864    0.9864    0.9864    415767\n",
      "   macro avg     0.9864    0.9864    0.9864    415767\n",
      "weighted avg     0.9864    0.9864    0.9864    415767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_sp_ann_2h_prob_unisoftsigbinlosadam,pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with no encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nodr_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=train_x,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  3 14:02:20 2019\n",
      "Train on 1064361 samples, validate on 266091 samples\n",
      "Epoch 1/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.1467 - acc: 0.9236 - val_loss: 0.0951 - val_acc: 0.9501\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09509, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0823 - acc: 0.9579 - val_loss: 0.0651 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09509 to 0.06507, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0592 - acc: 0.9709 - val_loss: 0.0482 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06507 to 0.04823, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0481 - acc: 0.9768 - val_loss: 0.0416 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04823 to 0.04160, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0406 - acc: 0.9805 - val_loss: 0.0350 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04160 to 0.03498, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0363 - acc: 0.9825 - val_loss: 0.0349 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03498 to 0.03494, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0336 - acc: 0.9843 - val_loss: 0.0326 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03494 to 0.03260, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0311 - acc: 0.9852 - val_loss: 0.0275 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03260 to 0.02746, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0293 - acc: 0.9861 - val_loss: 0.0263 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02746 to 0.02626, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0272 - acc: 0.9870 - val_loss: 0.0235 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02626 to 0.02346, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0261 - acc: 0.9876 - val_loss: 0.0213 - val_acc: 0.9901\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02346 to 0.02128, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1064361/1064361 [==============================] - 37s 34us/step - loss: 0.0251 - acc: 0.9880 - val_loss: 0.0213 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02128\n",
      "Epoch 13/200\n",
      "1064361/1064361 [==============================] - 37s 34us/step - loss: 0.0240 - acc: 0.9888 - val_loss: 0.0216 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02128\n",
      "Epoch 14/200\n",
      "1064361/1064361 [==============================] - 37s 34us/step - loss: 0.0231 - acc: 0.9892 - val_loss: 0.0209 - val_acc: 0.9901\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02128 to 0.02095, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1064361/1064361 [==============================] - 36s 34us/step - loss: 0.0222 - acc: 0.9896 - val_loss: 0.0212 - val_acc: 0.9905\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02095\n",
      "Epoch 16/200\n",
      "1064361/1064361 [==============================] - 37s 34us/step - loss: 0.0221 - acc: 0.9895 - val_loss: 0.0182 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02095 to 0.01818, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1064361/1064361 [==============================] - 37s 35us/step - loss: 0.0216 - acc: 0.9898 - val_loss: 0.0192 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01818\n",
      "Epoch 18/200\n",
      "1064361/1064361 [==============================] - 37s 35us/step - loss: 0.0209 - acc: 0.9902 - val_loss: 0.0183 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01818\n",
      "Epoch 19/200\n",
      "1064361/1064361 [==============================] - 37s 35us/step - loss: 0.0205 - acc: 0.9906 - val_loss: 0.0203 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01818\n",
      "Epoch 20/200\n",
      "1064361/1064361 [==============================] - 37s 35us/step - loss: 0.0202 - acc: 0.9906 - val_loss: 0.0194 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01818\n",
      "Epoch 21/200\n",
      "1064361/1064361 [==============================] - 37s 35us/step - loss: 0.0197 - acc: 0.9910 - val_loss: 0.0194 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01818\n",
      "Time elapsed (hh:mm:ss.ms) 0:12:49.723302\n"
     ]
    }
   ],
   "source": [
    "hist_nodr_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = nodr_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = train_x,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_nodr_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_nodr_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_nodr_ann_2h_unisoftsigbinlosadam, './Figures/nodr_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict(nodr_ann_2h_unisoftsigbinlosadam,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=train_x\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=test_x\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  3 14:15:10 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 8s 25us/step - loss: 0.2143 - acc: 0.8906\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.1248 - acc: 0.9334\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.1082 - acc: 0.9436\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0931 - acc: 0.9521\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0824 - acc: 0.9585\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0736 - acc: 0.9632\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0670 - acc: 0.9664\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0610 - acc: 0.9698\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0567 - acc: 0.9719\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0527 - acc: 0.9739\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0494 - acc: 0.9759\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0471 - acc: 0.9768\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0441 - acc: 0.9784\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0437 - acc: 0.9788\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0409 - acc: 0.9799\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0394 - acc: 0.9808\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0386 - acc: 0.9812\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0377 - acc: 0.9817\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0361 - acc: 0.9829\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0352 - acc: 0.9831\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0341 - acc: 0.9836\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0340 - acc: 0.9835\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0330 - acc: 0.9839\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0319 - acc: 0.9848\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0318 - acc: 0.9849\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0308 - acc: 0.9852\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0298 - acc: 0.9858\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0298 - acc: 0.9857\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0289 - acc: 0.9862\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0286 - acc: 0.9866\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0277 - acc: 0.9869\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0275 - acc: 0.9869\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0260 - acc: 0.9875\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0267 - acc: 0.9875\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0262 - acc: 0.9877\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0259 - acc: 0.9881\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0254 - acc: 0.9881\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 7s 23us/step - loss: 0.0247 - acc: 0.9885\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0246 - acc: 0.9886\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0248 - acc: 0.9886\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0235 - acc: 0.9890\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0241 - acc: 0.9887\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0233 - acc: 0.9893\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0231 - acc: 0.9894\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0234 - acc: 0.9891\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0224 - acc: 0.9895\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0220 - acc: 0.9898\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0223 - acc: 0.9897\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0217 - acc: 0.9902\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0215 - acc: 0.9902\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0219 - acc: 0.9897\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0213 - acc: 0.9902\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0206 - acc: 0.9905\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0209 - acc: 0.9904\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0210 - acc: 0.9905\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0210 - acc: 0.9906\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0211 - acc: 0.9903\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0195 - acc: 0.9911\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0208 - acc: 0.9905\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0197 - acc: 0.9910\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0197 - acc: 0.9909\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0202 - acc: 0.9908\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0193 - acc: 0.9912\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0201 - acc: 0.9912\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 8s 24us/step - loss: 0.0201 - acc: 0.9909\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 8s 23us/step - loss: 0.0202 - acc: 0.9910\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 8s 24us/step - loss: 0.0195 - acc: 0.9913\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0192 - acc: 0.9918\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 7s 22us/step - loss: 0.0192 - acc: 0.9914\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0193 - acc: 0.9915\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0193 - acc: 0.9914\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0191 - acc: 0.9916\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0188 - acc: 0.9915\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0188 - acc: 0.9916\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0183 - acc: 0.9918\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0184 - acc: 0.9916\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0186 - acc: 0.9918\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0180 - acc: 0.9919\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0181 - acc: 0.9919\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0183 - acc: 0.9918\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0186 - acc: 0.9917\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0182 - acc: 0.9917\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0182 - acc: 0.9920\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0178 - acc: 0.9920\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0177 - acc: 0.9919\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0177 - acc: 0.9921\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0179 - acc: 0.9919\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0175 - acc: 0.9920\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0181 - acc: 0.9920\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0180 - acc: 0.9922\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0174 - acc: 0.9922\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0175 - acc: 0.9921\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0173 - acc: 0.9921\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0174 - acc: 0.9921\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0181 - acc: 0.9920\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0177 - acc: 0.9922\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0166 - acc: 0.9928\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0179 - acc: 0.9921\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0166 - acc: 0.9926\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0187 - acc: 0.9920\n",
      "83154/83154 [==============================] - 1s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_59 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 8s 25us/step - loss: 0.2160 - acc: 0.8900\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.1238 - acc: 0.9336\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.1059 - acc: 0.9449\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0898 - acc: 0.9534\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0787 - acc: 0.9600\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0698 - acc: 0.9649\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0646 - acc: 0.9679\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0581 - acc: 0.9710\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0543 - acc: 0.9731\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0515 - acc: 0.9745\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0485 - acc: 0.9760\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0457 - acc: 0.9774\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0445 - acc: 0.9780\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0424 - acc: 0.9790\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0404 - acc: 0.9802\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0387 - acc: 0.9812\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0380 - acc: 0.9816\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0365 - acc: 0.9821\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0357 - acc: 0.9827\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0342 - acc: 0.9834\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0342 - acc: 0.9834\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0332 - acc: 0.9845\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0327 - acc: 0.9844\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0313 - acc: 0.9850\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0309 - acc: 0.9853\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0300 - acc: 0.9859\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0296 - acc: 0.9860\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0291 - acc: 0.9864\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0278 - acc: 0.9869\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0274 - acc: 0.9871\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0268 - acc: 0.9873\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0274 - acc: 0.9872\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0261 - acc: 0.9875\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0256 - acc: 0.9880\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0260 - acc: 0.9878\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0246 - acc: 0.9885\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0247 - acc: 0.9884\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0240 - acc: 0.9887\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0249 - acc: 0.9884\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0236 - acc: 0.9890\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0229 - acc: 0.9894\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0240 - acc: 0.9890\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0234 - acc: 0.9894\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0225 - acc: 0.9895\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0223 - acc: 0.9896\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0223 - acc: 0.9897\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0216 - acc: 0.9898\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0218 - acc: 0.9899\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0215 - acc: 0.9900\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0211 - acc: 0.9902\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0206 - acc: 0.9906\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0208 - acc: 0.9904\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0201 - acc: 0.9906\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0204 - acc: 0.9906\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0200 - acc: 0.9908\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0204 - acc: 0.9907\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0190 - acc: 0.9912\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0203 - acc: 0.9905\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0199 - acc: 0.9909\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0193 - acc: 0.9912\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0194 - acc: 0.9909\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0191 - acc: 0.9912\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0191 - acc: 0.9913\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0199 - acc: 0.9911\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0196 - acc: 0.9912\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0187 - acc: 0.9914\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0190 - acc: 0.9914\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0187 - acc: 0.9915\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0185 - acc: 0.9915\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0183 - acc: 0.9918\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0186 - acc: 0.9916\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0193 - acc: 0.9912\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0180 - acc: 0.9919\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0187 - acc: 0.9915\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0182 - acc: 0.9918\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0185 - acc: 0.9915\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0173 - acc: 0.9921\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0182 - acc: 0.9918\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0172 - acc: 0.9922\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0180 - acc: 0.9919\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0180 - acc: 0.9919\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0176 - acc: 0.9920\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0174 - acc: 0.9922\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0180 - acc: 0.9920\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0181 - acc: 0.9917\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0169 - acc: 0.9924\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0176 - acc: 0.9921\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0173 - acc: 0.9921\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0173 - acc: 0.9922\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0181 - acc: 0.9920\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0170 - acc: 0.9923\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0176 - acc: 0.9920\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0175 - acc: 0.9922\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0166 - acc: 0.9924\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0172 - acc: 0.9924\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0172 - acc: 0.9925\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 7s 21us/step - loss: 0.0165 - acc: 0.9925\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0167 - acc: 0.9925\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0167 - acc: 0.9925\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0167 - acc: 0.9925\n",
      "83154/83154 [==============================] - 1s 12us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 8s 25us/step - loss: 0.2161 - acc: 0.8899\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.1243 - acc: 0.9337\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.1082 - acc: 0.9439\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0916 - acc: 0.9532\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0801 - acc: 0.9596\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0722 - acc: 0.9632\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0666 - acc: 0.9666\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0617 - acc: 0.9691\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0576 - acc: 0.9713\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0538 - acc: 0.9731\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0511 - acc: 0.9748\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0472 - acc: 0.9767\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0450 - acc: 0.9777\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0437 - acc: 0.9784\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0409 - acc: 0.9797\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0403 - acc: 0.9803\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 7s 20us/step - loss: 0.0384 - acc: 0.9815\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0373 - acc: 0.9818\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0361 - acc: 0.9827\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 7s 20us/step - loss: 0.0344 - acc: 0.9833\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0345 - acc: 0.9830\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 7s 20us/step - loss: 0.0324 - acc: 0.9842\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0327 - acc: 0.9844\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.0316 - acc: 0.9848\n",
      "Epoch 25/100\n",
      "184590/332614 [===============>..............] - ETA: 3s - loss: 0.0295 - acc: 0.9854"
     ]
    }
   ],
   "source": [
    "pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_ = PCA(n_components = 0.95, svd_solver = 'full').fit(train_x)\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# n_coml = [pca_.n_components_]\n",
    "\n",
    "# plt.plot(np.cumsum(pca_.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components', fontsize=14)\n",
    "# plt.ylabel('Variance (%)', fontsize=14) #for each component\n",
    "# plt.title('Pulsar Dataset Explained Variance '+str(dsnum)+' node DS', fontsize=14)\n",
    "\n",
    "# n_coml = [*n_coml]\n",
    "\n",
    "# for i, v in enumerate(n_coml):\n",
    "#     plt.text(v-0.8, i+0.94, '{:.0f}'.format(v), color='navy', fontsize=14)\n",
    "\n",
    "# plt.savefig('./Figures/PCA_components_ds'+str(dsnum)+'bal.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators=300, \n",
    "#                              criterion='gini', \n",
    "#                              max_depth=16, \n",
    "#                              #min_samples_split=2, \n",
    "#                              #min_samples_leaf=1, \n",
    "#                              max_features=0.3, \n",
    "#                              #bootstrap=True,\n",
    "#                              oob_score=True,\n",
    "#                              random_state=23)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(enc_train_x_asam, train_y)\n",
    "\n",
    "# pred_y_ae_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(enc_test_x_asam),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_ae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_ae_RF, pred_y_ae_RF, './Figures/ROC_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(enc_train_x_spsam, train_y)\n",
    "\n",
    "# pred_y_spae_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(enc_test_x_spsam),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_spae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_spae_RF, pred_y_spae_RF, './Figures/ROC_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with pca DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# print(datetime.ctime(start_time))\n",
    "\n",
    "# clf.fit(train_x_pca, train_y)\n",
    "\n",
    "# pred_y_pca_RF = cross_val_predict(estimator=clf,\n",
    "#                               X=np.array(test_x_pca),\n",
    "#                               y=test_y,\n",
    "#                               cv=KFold(n_splits=5, random_state=23),\n",
    "#                               n_jobs=2)\n",
    "\n",
    "# time_elapsed = datetime.now() - start_time \n",
    "# print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "# print(sm.classification_report(test_y, pred_y_pca_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_y_pca_RF, pred_y_pca_RF, './Figures/ROC_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_ae_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_ae_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "# print(pred_y_ae_RF.shape)\n",
    "# print(pred_y_spae_RF.shape)\n",
    "# print(pred_y_pca_RF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate_ae_ann, recall_ae_ann, thresholds_ae_ann = roc_curve(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_ae_ann = auc(false_positive_rate_ae_ann, recall_ae_ann)\n",
    "false_positive_rate_sp_ann, recall_sp_ann, thresholds_sp_ann = roc_curve(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_sp_ann = auc(false_positive_rate_sp_ann, recall_sp_ann)\n",
    "false_positive_rate_nodr_ann, recall_nodr_ann, thresholds_nodr_ann = roc_curve(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_nodr_ann = auc(false_positive_rate_nodr_ann, recall_nodr_ann)\n",
    "\n",
    "# false_positive_rate_ae_RF, recall_ae_RF, thresholds_ae_RF = roc_curve(test_y, pred_y_ae_RF)\n",
    "# roc_auc_ae_RF = auc(false_positive_rate_ae_RF, recall_ae_RF)\n",
    "# false_positive_rate_spae_RF, recall_spae_RF, thresholds_spae_RF = roc_curve(test_y, pred_y_spae_RF)\n",
    "# roc_auc_spae_RF = auc(false_positive_rate_spae_RF, recall_spae_RF)\n",
    "# false_positive_rate_pca_RF, recall_pca_RF, thresholds_pca_RF = roc_curve(test_y, pred_y_pca_RF)\n",
    "# roc_auc_pca_RF = auc(false_positive_rate_pca_RF, recall_pca_RF)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC)', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "# plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "# plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "# plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "# plt.ylim([0.97,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC) Zoom in', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "# plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "# plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "# plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "# plt.ylim([0.0,1.0])\n",
    "plt.ylim([0.955,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal_zoom.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ae_ann = \"AE+DNN\"\n",
    "acc_ae_ann = (sm.accuracy_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_ae_ann = (sm.precision_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_ae_ann = (sm.recall_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_ae_ann = (sm.f1_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_sp_ann = \"SAE+DNN\"\n",
    "acc_sp_ann = (sm.accuracy_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_sp_ann = (sm.precision_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_sp_ann = (sm.recall_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_sp_ann = (sm.f1_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_nodr_ann = \"DNN\"\n",
    "acc_nodr_ann = (sm.accuracy_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_nodr_ann = (sm.precision_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_nodr_ann = (sm.recall_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_nodr_ann = (sm.f1_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "# classi_ae_RF = \"AE+RF\"\n",
    "# acc_ae_RF = (sm.accuracy_score(test_y, pred_y_ae_RF)*100) \n",
    "# pre_ae_RF = (sm.precision_score(test_y, pred_y_ae_RF)*100) \n",
    "# recall_ae_RF = (sm.recall_score(test_y, pred_y_ae_RF)*100) \n",
    "# f1score_ae_RF = (sm.f1_score(test_y, pred_y_ae_RF)*100)\n",
    "\n",
    "# classi_spae_RF = \"SAE+RF\"\n",
    "# acc_spae_RF = (sm.accuracy_score(test_y, pred_y_spae_RF)*100) \n",
    "# pre_spae_RF = (sm.precision_score(test_y, pred_y_spae_RF)*100) \n",
    "# recall_spae_RF = (sm.recall_score(test_y, pred_y_spae_RF)*100) \n",
    "# f1score_spae_RF = (sm.f1_score(test_y, pred_y_spae_RF)*100)\n",
    "\n",
    "# classi_pca_RF = \"PCA+RF\"\n",
    "# acc_pca_RF = (sm.accuracy_score(test_y, pred_y_pca_RF)*100) \n",
    "# pre_pca_RF = (sm.precision_score(test_y, pred_y_pca_RF)*100) \n",
    "# recall_pca_RF = (sm.recall_score(test_y, pred_y_pca_RF)*100) \n",
    "# f1score_pca_RF = (sm.f1_score(test_y, pred_y_pca_RF)*100)\n",
    "\n",
    "\n",
    "print('Classifier\\tAcc\\tPreci\\tRecall\\tF1Score')\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_ann, acc_ae_ann, pre_ae_ann, recall_ae_ann, f1score_ae_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_sp_ann, acc_sp_ann, pre_sp_ann, recall_sp_ann, f1score_sp_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_nodr_ann, acc_nodr_ann, pre_nodr_ann, recall_nodr_ann, f1score_nodr_ann))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_RF, acc_ae_RF, pre_ae_RF, recall_ae_RF, f1score_ae_RF))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_spae_RF, acc_spae_RF, pre_spae_RF, recall_spae_RF, f1score_spae_RF))\n",
    "# print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_pca_RF, acc_pca_RF, pre_pca_RF, recall_pca_RF, f1score_pca_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1list = [[\"AE+DNN\",f1score_ae_ann],[\"SAE+DNN\",f1score_sp_ann],[\"DNN\",f1score_nodr_ann]]#,\n",
    "#           [\"AE+RF\",f1score_ae_RF],[\"SAE+RF\",f1score_spae_RF],[\"PCA+RF\",f1score_pca_RF]]\n",
    "\n",
    "xs, ys = [*zip(*f1list)]\n",
    "\n",
    "'{:.2f}'.format(f1score_ae_ann)\n",
    "\n",
    "plt.figure(figsize=(8,6), )\n",
    "plt.barh(xs, ys, color = \"purple\")\n",
    "plt.title(\"F1 score vs Classifier\", fontsize=16)\n",
    "plt.xlabel(\"Classifier\", fontsize=14)\n",
    "plt.ylabel(\"F1 score\", fontsize=14)\n",
    "plt.xticks(np.arange(0, 101, 10), fontsize=12)\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "for i, v in enumerate(ys):\n",
    "    plt.text(v+1, i+0.1, '{:.2f}'.format(v), color='purple', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/F1scoreplot_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
