{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Donâ€™t pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "from keras import optimizers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Activation, Dense, Dropout, Input\n",
    "from keras import backend as k\n",
    "\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "#k.tensorflow_backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------Import modules------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(23)\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from datetime import datetime \n",
    "import os.path\n",
    "\n",
    "dsnum=100\n",
    "verbose_level=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/01Code/00Datasets_final/00BalancedDS/FullCloneID100bal_stdscal.csv\n"
     ]
    }
   ],
   "source": [
    "pathds = os.path.abspath('/home/user/01Code/00Datasets_final/00BalancedDS')\n",
    "file_name = \"FullCloneID\"+str(dsnum)+\"bal_stdscal.csv\"\n",
    "full_path = os.path.join(pathds,file_name)\n",
    "print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2078832, 211)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "neurons=df.shape[1]-1\n",
    "batch_size=df.shape[1]-1\n",
    "print(neurons)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Explaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1039416\n",
      "Class 1: 1039416\n",
      "Proportion: 1.0 : 1\n"
     ]
    }
   ],
   "source": [
    "#if you don't have an intuitive sense of how imbalanced these two classes are, let's go visual\n",
    "count_classes = pd.value_counts(df['class'], sort = True)\n",
    "print('Class 0:', count_classes[0])\n",
    "print('Class 1:', count_classes[1])\n",
    "print('Proportion:', round(count_classes[0] / count_classes[1], 3), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.xticks(range(2), ['Normal [0]','Malicious [1]'])\n",
    "plt.title(\"Frequency by observation number\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Observations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed and percentage of test data\n",
    "RANDOM_SEED = 23 #used to help randomly select the data points\n",
    "TEST_PCT = 0.20 # 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_df = train_test_split(df, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ df -> original dataset \n",
    "+ train -> subset of 80% from original dataset \n",
    "+ test_df -> subset of 20% from original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(train, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train -> subset of 80% from original dataset \n",
    "+ train_df -> subset of 80% from train\n",
    "+ dev_df -> subset of 20% from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49990228884619664\n",
      "0.500260061993969\n",
      "0.5001046259082611\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of mal samples in train and test set\n",
    "print(train_df.iloc[:, batch_size].sum()/train_df.shape[0]) \n",
    "print(dev_df.iloc[:, batch_size].sum()/dev_df.shape[0]) \n",
    "print(test_df.iloc[:, batch_size].sum()/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.iloc[:, :batch_size] \n",
    "dev_x = dev_df.iloc[:, :batch_size] \n",
    "test_x = test_df.iloc[:, :batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_x -> features of train_df **Training subset for AE**\n",
    "+ dev_x -> features of dev_df **Validation subset for AE**\n",
    "+ test_x -> features of test_df **Testing subset for ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final train and test sets\n",
    "train_y = train_df.iloc[:,batch_size]\n",
    "dev_y = dev_df.iloc[:,batch_size]\n",
    "test_y = test_df.iloc[:,batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_y -> **Labels for supervised training of ANN**\n",
    "+ dev_y -> labels of dev_df  *not used for AE neither ANN*\n",
    "+ test_y -> labels of test_df  **Ground Truth for predictions of supervised ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "train_x =np.array(train_x)\n",
    "dev_x =np.array(dev_x)\n",
    "test_x = np.array(test_x)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "dev_y = np.array(dev_y)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "print(train_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(factor_enc_dim, enc_activation, dec_activation, \n",
    "                optimizer, loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer #RELU\n",
    "    encoded = Dense(encoding_dim, activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer #SIMOID\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spae(factor_enc_dim,dec_activation,enc_activation,\n",
    "         optimizer,loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer\n",
    "    encoded = Dense(encoding_dim, activity_regularizer=regularizers.l1(1e-4), activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pca(thr):\n",
    "    #train_x_pca,test_x_pca = to_pca(0.95)\n",
    "    pca = PCA(n_components = thr, svd_solver = 'full')\n",
    "    train_x_ = np.array(train_x)\n",
    "    print(type(train_x_))\n",
    "\n",
    "    test_x_ = np.array(test_x)\n",
    "    print(type(test_x_))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(time.ctime(start_time))\n",
    "\n",
    "    train_x_pca = pca.fit_transform(train_x_)\n",
    "    print(train_x_pca.shape)\n",
    "\n",
    "    test_x_pca = pca.fit_transform(test_x_)\n",
    "    print(test_x_pca.shape)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "\n",
    "    print(\"--- PCA spent %s seconds ---\" %elapsed_time )\n",
    "    \n",
    "    return  train_x_pca,test_x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ae(checkpoint_file, autoencoder,\n",
    "           epochs, batch_size, shuffle):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    hist_auto = autoencoder.fit(train_x, train_x,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    verbose=verbose_level,\n",
    "                    callbacks=[early_stopping, cp, tb],\n",
    "                    validation_data=(dev_x, dev_x))\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "    \n",
    "    return hist_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_auto(hist_auto, fig_file):\n",
    "    best_loss_value = hist_auto.history['loss'][-1]\n",
    "    print('Best loss value:', best_loss_value)\n",
    "\n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist_auto.history['loss'])\n",
    "    plt.plot(hist_auto.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.savefig(fig_file)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h(neurons,encoded_train_x,init_mode,activation_input,\n",
    "               weight_constraint,dropout_rate,activation_output,\n",
    "               loss,optimizer):\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=encoded_train_x.shape[1],\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h_():\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=input_dim,\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(checkpoint_file,ann,enc_train_x,train_y,epochs,shuffle,batch_size):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    history = ann.fit(enc_train_x,\n",
    "                      train_y,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[early_stopping, cp, tb],\n",
    "                      epochs=epochs,\n",
    "                      shuffle=shuffle,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=verbose_level)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict(ann,enc_test_x):\n",
    "    pred_ann_prob = ann.predict(enc_test_x)\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "    \n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob, pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict_():\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))  \n",
    "\n",
    "    modelk = KerasClassifier(build_fn=ann_2h_,\n",
    "                             epochs=epochs, \n",
    "                             batch_size=batch_size, \n",
    "                             verbose=verbose_level\n",
    "                            )\n",
    "\n",
    "    pred_ann_prob = cross_val_predict(modelk,\n",
    "                                      enc_test_x,\n",
    "                                      test_y,\n",
    "                                      cv=KFold(n_splits=5, random_state=23),\n",
    "                                      verbose=1)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "\n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob,pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cm(pred_ann_prob, pred_ann_01, roc_file, cm_file):\n",
    "    false_positive_rate, recall, thresholds = roc_curve(test_y, pred_ann_prob)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.savefig(roc_file)\n",
    "    plt.show()\n",
    "    \n",
    "    cm = confusion_matrix(test_y, pred_ann_01)\n",
    "    labels = ['Normal', 'Malicious']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=\"RdYlGn\", vmin = 0.2);\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.savefig(cm_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- PCA Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Mon Aug 26 14:26:54 2019\n",
      "(1330452, 55)\n",
      "(415767, 55)\n",
      "--- PCA spent 525.9837417602539 seconds ---\n"
     ]
    }
   ],
   "source": [
    "train_x_pca,test_x_pca = to_pca(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- AE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 14:35:42.860680 139973542926144 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0826 14:35:45.242545 139973542926144 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0826 14:36:01.903543 139973542926144 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0826 14:36:21.638278 139973542926144 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 140)               29540     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 210)               29610     \n",
      "=================================================================\n",
      "Total params: 59,150\n",
      "Trainable params: 59,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 14:36:27.012237 139973542926144 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0826 14:36:28.631128 139973542926144 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ae_sigmoid_adam_mse,enc_train_x_asam,enc_test_x_asam = ae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ae_sigmoid_adam_mse = load_model('ae_sigmoid_adam_mse_redds10bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 14:38:06 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 14:38:06.794190 139973542926144 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0826 14:38:06.809647 139973542926144 deprecation_wrapper.py:119] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1330452 samples, validate on 332613 samples\n",
      "Epoch 1/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0567 - acc: 0.3425 - val_loss: 0.0519 - val_acc: 0.3991\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05188, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0495 - acc: 0.3343 - val_loss: 0.0512 - val_acc: 0.3320\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05188 to 0.05117, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0493 - acc: 0.2394 - val_loss: 0.0510 - val_acc: 0.2366\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05117 to 0.05103, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.2777 - val_loss: 0.0510 - val_acc: 0.2537\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05103 to 0.05101, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.2111 - val_loss: 0.0510 - val_acc: 0.2080\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05101 to 0.05100, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1486 - val_loss: 0.0510 - val_acc: 0.1549\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.05100 to 0.05099, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1322 - val_loss: 0.0510 - val_acc: 0.1418\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.05099 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1414 - val_loss: 0.0510 - val_acc: 0.1420\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1295 - val_loss: 0.0510 - val_acc: 0.1428\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05098\n",
      "Epoch 10/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1347 - val_loss: 0.0510 - val_acc: 0.1527\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1433 - val_loss: 0.0510 - val_acc: 0.1288\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1100 - val_loss: 0.0510 - val_acc: 0.1368\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1266 - val_loss: 0.0510 - val_acc: 0.1418\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1369 - val_loss: 0.0510 - val_acc: 0.1431\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1309 - val_loss: 0.0510 - val_acc: 0.1327\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.05098\n",
      "Epoch 16/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1380 - val_loss: 0.0510 - val_acc: 0.1367\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1327 - val_loss: 0.0510 - val_acc: 0.1292\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1233 - val_loss: 0.0510 - val_acc: 0.1219\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.05098\n",
      "Epoch 19/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1354 - val_loss: 0.0510 - val_acc: 0.1379\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05098 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1324 - val_loss: 0.0510 - val_acc: 0.1298\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.05097\n",
      "Epoch 21/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1289 - val_loss: 0.0510 - val_acc: 0.1284\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 22/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1326 - val_loss: 0.0510 - val_acc: 0.1478\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 23/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1395 - val_loss: 0.0510 - val_acc: 0.1345\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 24/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1357 - val_loss: 0.0510 - val_acc: 0.1319\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 25/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1407 - val_loss: 0.0510 - val_acc: 0.1439\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.05097\n",
      "Epoch 26/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1211 - val_loss: 0.0510 - val_acc: 0.1108\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.05097\n",
      "Epoch 27/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0492 - acc: 0.1227 - val_loss: 0.0510 - val_acc: 0.1257\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05097\n",
      "Epoch 28/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1521 - val_loss: 0.0510 - val_acc: 0.1651\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 29/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1549 - val_loss: 0.0510 - val_acc: 0.1539\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 30/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1680 - val_loss: 0.0510 - val_acc: 0.1499\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 31/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1562 - val_loss: 0.0510 - val_acc: 0.1569\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.05097\n",
      "Epoch 32/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1570 - val_loss: 0.0510 - val_acc: 0.1573\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.05097\n",
      "Epoch 33/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1581 - val_loss: 0.0510 - val_acc: 0.1569\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 34/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1544 - val_loss: 0.0510 - val_acc: 0.1547\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 35/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1631 - val_loss: 0.0510 - val_acc: 0.1560\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.05097\n",
      "Epoch 36/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1437 - val_loss: 0.0510 - val_acc: 0.1404\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05097\n",
      "Epoch 37/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1563 - val_loss: 0.0510 - val_acc: 0.1646\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.05097\n",
      "Epoch 38/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.1556 - val_loss: 0.0510 - val_acc: 0.1555\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.05097\n",
      "Epoch 39/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0492 - acc: 0.1589 - val_loss: 0.0510 - val_acc: 0.1602\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.05097\n",
      "Time elapsed (hh:mm:ss.ms) 0:08:59.439437\n"
     ]
    }
   ],
   "source": [
    "hist_ae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/ae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = ae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size*2,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.049174800142044395\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_ae_sigmoid_adam_mse  = plot_hist_auto(hist_ae_sigmoid_adam_mse, './Figures/hist_ae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- SPAE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 140)               29540     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 210)               29610     \n",
      "=================================================================\n",
      "Total params: 59,150\n",
      "Trainable params: 59,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "spae_sigmoid_adam_mse,enc_train_x_spsam,enc_test_x_spsam = spae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spae_sigmoid_adam_mse = load_model('spae_sigmoid_adam_mse_redds20bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 14:47:30 2019\n",
      "Train on 1330452 samples, validate on 332613 samples\n",
      "Epoch 1/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.1694 - acc: 9.8463e-05 - val_loss: 0.1104 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11045, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.1012 - acc: 3.4575e-05 - val_loss: 0.0994 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11045 to 0.09936, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0966 - acc: 2.5555e-05 - val_loss: 0.0979 - val_acc: 1.8039e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09936 to 0.09789, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0959 - acc: 2.4052e-05 - val_loss: 0.0977 - val_acc: 1.8039e-05\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09789 to 0.09766, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0958 - acc: 2.5555e-05 - val_loss: 0.0976 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09766 to 0.09763, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0958 - acc: 2.8562e-05 - val_loss: 0.0976 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09763 to 0.09762, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0958 - acc: 2.8562e-05 - val_loss: 0.0976 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09762 to 0.09761, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0958 - acc: 3.3071e-05 - val_loss: 0.0976 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09761 to 0.09761, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0958 - acc: 3.3071e-05 - val_loss: 0.0976 - val_acc: 4.5097e-05\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09761 to 0.09761, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0958 - acc: 3.2320e-05 - val_loss: 0.0976 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09761 to 0.09760, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0949 - acc: 2.3300e-05 - val_loss: 0.0954 - val_acc: 1.8039e-05\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09760 to 0.09541, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0920 - acc: 1.8039e-05 - val_loss: 0.0924 - val_acc: 2.7058e-05\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09541 to 0.09236, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0900 - acc: 1.7287e-05 - val_loss: 0.0914 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09236 to 0.09140, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0893 - acc: 1.8039e-05 - val_loss: 0.0908 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09140 to 0.09078, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0887 - acc: 1.9542e-05 - val_loss: 0.0903 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09078 to 0.09030, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 16/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0883 - acc: 2.3300e-05 - val_loss: 0.0899 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09030 to 0.08992, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0880 - acc: 2.1045e-05 - val_loss: 0.0896 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.08992 to 0.08962, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0877 - acc: 2.1797e-05 - val_loss: 0.0894 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08962 to 0.08938, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 19/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0875 - acc: 2.4804e-05 - val_loss: 0.0892 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08938 to 0.08918, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0873 - acc: 2.6307e-05 - val_loss: 0.0890 - val_acc: 3.3071e-05\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08918 to 0.08901, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0871 - acc: 2.7058e-05 - val_loss: 0.0889 - val_acc: 3.3071e-05\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08901 to 0.08886, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 22/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0870 - acc: 2.7810e-05 - val_loss: 0.0887 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08886 to 0.08874, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 23/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0869 - acc: 2.8562e-05 - val_loss: 0.0886 - val_acc: 3.3071e-05\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08874 to 0.08863, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 24/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0868 - acc: 2.8562e-05 - val_loss: 0.0885 - val_acc: 3.6078e-05\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.08863 to 0.08853, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 25/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0867 - acc: 3.1568e-05 - val_loss: 0.0884 - val_acc: 3.6078e-05\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.08853 to 0.08844, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 26/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0866 - acc: 3.3823e-05 - val_loss: 0.0884 - val_acc: 3.6078e-05\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.08844 to 0.08836, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 27/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0865 - acc: 3.2320e-05 - val_loss: 0.0883 - val_acc: 3.6078e-05\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08836 to 0.08830, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 28/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0865 - acc: 3.2320e-05 - val_loss: 0.0882 - val_acc: 3.6078e-05\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.08830 to 0.08824, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 29/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0864 - acc: 3.4575e-05 - val_loss: 0.0882 - val_acc: 3.6078e-05\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08824 to 0.08818, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 30/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0863 - acc: 3.4575e-05 - val_loss: 0.0881 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.08818 to 0.08813, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 31/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0863 - acc: 3.5326e-05 - val_loss: 0.0881 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.08813 to 0.08808, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 32/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0863 - acc: 3.4575e-05 - val_loss: 0.0880 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.08808 to 0.08804, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 33/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0862 - acc: 3.6078e-05 - val_loss: 0.0880 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.08804 to 0.08800, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 34/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0862 - acc: 3.2320e-05 - val_loss: 0.0880 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.08800 to 0.08796, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 35/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0861 - acc: 3.8333e-05 - val_loss: 0.0879 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.08796 to 0.08792, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 36/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0861 - acc: 4.2843e-05 - val_loss: 0.0879 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.08792 to 0.08789, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 37/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0861 - acc: 3.8333e-05 - val_loss: 0.0879 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.08789 to 0.08786, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 38/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0860 - acc: 4.2843e-05 - val_loss: 0.0878 - val_acc: 3.6078e-05\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.08786 to 0.08783, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 39/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0860 - acc: 3.7581e-05 - val_loss: 0.0878 - val_acc: 4.5097e-05\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08783 to 0.08781, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 40/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0860 - acc: 4.2091e-05 - val_loss: 0.0878 - val_acc: 4.5097e-05\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.08781 to 0.08778, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 41/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0860 - acc: 5.1110e-05 - val_loss: 0.0878 - val_acc: 5.1110e-05\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.08778 to 0.08776, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 42/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0859 - acc: 5.4117e-05 - val_loss: 0.0877 - val_acc: 5.4117e-05\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.08776 to 0.08774, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 43/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0859 - acc: 9.0947e-05 - val_loss: 0.0877 - val_acc: 3.9986e-04\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.08774 to 0.08772, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 44/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0859 - acc: 1.1274e-04 - val_loss: 0.0877 - val_acc: 9.9214e-04\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.08772 to 0.08770, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 45/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0859 - acc: 3.8558e-04 - val_loss: 0.0877 - val_acc: 0.0032\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.08770 to 0.08768, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 46/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0859 - acc: 8.4633e-04 - val_loss: 0.0877 - val_acc: 0.0076\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.08768 to 0.08766, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 47/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0858 - acc: 0.0015 - val_loss: 0.0876 - val_acc: 0.0223\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.08766 to 0.08764, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 48/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0858 - acc: 0.0029 - val_loss: 0.0876 - val_acc: 0.0458\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.08764 to 0.08763, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 49/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0858 - acc: 0.0067 - val_loss: 0.0876 - val_acc: 0.0635\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.08763 to 0.08761, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 50/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0858 - acc: 0.0136 - val_loss: 0.0876 - val_acc: 0.0698\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.08761 to 0.08760, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 51/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0858 - acc: 0.0196 - val_loss: 0.0876 - val_acc: 0.0390\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.08760 to 0.08756, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 52/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0858 - acc: 0.0229 - val_loss: 0.0876 - val_acc: 0.0619\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.08756 to 0.08755, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 53/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0241 - val_loss: 0.0875 - val_acc: 0.0623\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.08755 to 0.08754, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 54/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0259 - val_loss: 0.0875 - val_acc: 0.0587\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.08754 to 0.08752, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 55/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0273 - val_loss: 0.0875 - val_acc: 0.0697\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.08752 to 0.08752, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 56/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0287 - val_loss: 0.0875 - val_acc: 0.0706\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.08752 to 0.08751, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 57/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0857 - acc: 0.0303 - val_loss: 0.0875 - val_acc: 0.0700\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.08751 to 0.08749, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 58/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0292 - val_loss: 0.0875 - val_acc: 0.0760\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.08749\n",
      "Epoch 59/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0285 - val_loss: 0.0875 - val_acc: 0.0774\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.08749 to 0.08749, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 60/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0282 - val_loss: 0.0875 - val_acc: 0.0785\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.08749 to 0.08748, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 61/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0304 - val_loss: 0.0875 - val_acc: 0.0773\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.08748 to 0.08747, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 62/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0313 - val_loss: 0.0875 - val_acc: 0.0687\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.08747 to 0.08746, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 63/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 0.0326 - val_loss: 0.0874 - val_acc: 0.0692\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.08746 to 0.08745, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 64/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0331 - val_loss: 0.0874 - val_acc: 0.0781\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.08745 to 0.08744, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 65/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0346 - val_loss: 0.0874 - val_acc: 0.0793\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.08744 to 0.08744, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 66/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0351 - val_loss: 0.0874 - val_acc: 0.0796\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.08744 to 0.08743, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 67/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0362 - val_loss: 0.0874 - val_acc: 0.0730\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.08743 to 0.08741, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 68/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0375 - val_loss: 0.0874 - val_acc: 0.0776\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.08741 to 0.08741, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 69/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0382 - val_loss: 0.0874 - val_acc: 0.0805\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.08741 to 0.08741, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 70/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0856 - acc: 0.0396 - val_loss: 0.0874 - val_acc: 0.0807\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.08741 to 0.08741, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 71/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0406 - val_loss: 0.0874 - val_acc: 0.0801\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.08741 to 0.08739, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 72/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0412 - val_loss: 0.0874 - val_acc: 0.0806\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.08739 to 0.08739, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 73/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0856 - acc: 0.0426 - val_loss: 0.0874 - val_acc: 0.0780\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.08739 to 0.08737, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 74/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0856 - acc: 0.0430 - val_loss: 0.0874 - val_acc: 0.0792\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.08737 to 0.08737, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 75/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0856 - acc: 0.0443 - val_loss: 0.0874 - val_acc: 0.0747\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.08737 to 0.08736, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 76/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0440 - val_loss: 0.0873 - val_acc: 0.0580\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.08736 to 0.08735, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 77/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 0.0457 - val_loss: 0.0874 - val_acc: 0.0775\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.08735\n",
      "Epoch 78/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0856 - acc: 0.0444 - val_loss: 0.0873 - val_acc: 0.0384\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.08735 to 0.08734, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 79/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0855 - acc: 0.0441 - val_loss: 0.0873 - val_acc: 0.0785\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.08734 to 0.08734, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 80/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0855 - acc: 0.0454 - val_loss: 0.0873 - val_acc: 0.0818\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.08734\n",
      "Epoch 81/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0453 - val_loss: 0.0873 - val_acc: 0.0648\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.08734 to 0.08732, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 82/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0493 - val_loss: 0.0873 - val_acc: 0.0769\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.08732\n",
      "Epoch 83/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0495 - val_loss: 0.0873 - val_acc: 0.0730\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.08732 to 0.08732, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 84/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0855 - acc: 0.0495 - val_loss: 0.0873 - val_acc: 0.0800\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.08732\n",
      "Epoch 85/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0490 - val_loss: 0.0873 - val_acc: 0.0525\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.08732 to 0.08731, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 86/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0499 - val_loss: 0.0873 - val_acc: 0.0802\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.08731 to 0.08731, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 87/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0495 - val_loss: 0.0873 - val_acc: 0.0599\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.08731\n",
      "Epoch 88/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0509 - val_loss: 0.0873 - val_acc: 0.0790\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.08731 to 0.08730, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 89/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0532 - val_loss: 0.0873 - val_acc: 0.0822\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.08730\n",
      "Epoch 90/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0537 - val_loss: 0.0873 - val_acc: 0.0531\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.08730 to 0.08729, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 91/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0496 - val_loss: 0.0873 - val_acc: 0.0677\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.08729 to 0.08728, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 92/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0540 - val_loss: 0.0873 - val_acc: 0.0763\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.08728 to 0.08728, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 93/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0525 - val_loss: 0.0873 - val_acc: 0.0821\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.08728\n",
      "Epoch 94/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0511 - val_loss: 0.0873 - val_acc: 0.0683\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.08728 to 0.08727, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 95/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0556 - val_loss: 0.0873 - val_acc: 0.0791\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.08727\n",
      "Epoch 96/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0855 - acc: 0.0551 - val_loss: 0.0873 - val_acc: 0.0817\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.08727\n",
      "Epoch 97/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0538 - val_loss: 0.0873 - val_acc: 0.0448\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.08727\n",
      "Epoch 98/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0535 - val_loss: 0.0873 - val_acc: 0.0816\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.08727 to 0.08726, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 99/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0537 - val_loss: 0.0873 - val_acc: 0.0701\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.08726 to 0.08725, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 100/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0576 - val_loss: 0.0873 - val_acc: 0.0568\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.08725\n",
      "Epoch 101/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0541 - val_loss: 0.0872 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.08725 to 0.08725, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 102/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0590 - val_loss: 0.0872 - val_acc: 0.0798\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.08725\n",
      "Epoch 103/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0538 - val_loss: 0.0872 - val_acc: 0.0647\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.08725 to 0.08724, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 104/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0563 - val_loss: 0.0872 - val_acc: 0.0707\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.08724 to 0.08724, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 105/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0580 - val_loss: 0.0872 - val_acc: 0.0804\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.08724\n",
      "Epoch 106/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0855 - acc: 0.0554 - val_loss: 0.0872 - val_acc: 0.0705\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.08724 to 0.08723, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 107/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0594 - val_loss: 0.0872 - val_acc: 0.0643\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.08723\n",
      "Epoch 108/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0603 - val_loss: 0.0872 - val_acc: 0.0623\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.08723 to 0.08723, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 109/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0570 - val_loss: 0.0872 - val_acc: 0.0815\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.08723\n",
      "Epoch 110/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0575 - val_loss: 0.0872 - val_acc: 0.0798\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.08723 to 0.08722, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 111/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0602 - val_loss: 0.0872 - val_acc: 0.0425\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.08722\n",
      "Epoch 112/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0546 - val_loss: 0.0872 - val_acc: 0.0699\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.08722 to 0.08722, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 113/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0611 - val_loss: 0.0872 - val_acc: 0.0629\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.08722\n",
      "Epoch 114/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0592 - val_loss: 0.0872 - val_acc: 0.0683\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.08722 to 0.08721, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 115/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0593 - val_loss: 0.0872 - val_acc: 0.0711\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.08721 to 0.08721, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 116/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0621 - val_loss: 0.0872 - val_acc: 0.0499\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.08721\n",
      "Epoch 117/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0606 - val_loss: 0.0872 - val_acc: 0.0507\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.08721\n",
      "Epoch 118/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0609 - val_loss: 0.0872 - val_acc: 0.0814\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.08721 to 0.08721, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 119/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0616 - val_loss: 0.0872 - val_acc: 0.0595\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.08721 to 0.08720, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 120/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0628 - val_loss: 0.0872 - val_acc: 0.0827\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.08720\n",
      "Epoch 121/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0607 - val_loss: 0.0872 - val_acc: 0.0819\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.08720 to 0.08720, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 122/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0854 - acc: 0.0593 - val_loss: 0.0872 - val_acc: 0.0772\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.08720 to 0.08720, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 123/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0628 - val_loss: 0.0872 - val_acc: 0.0806\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.08720 to 0.08719, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 124/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0629 - val_loss: 0.0872 - val_acc: 0.0605\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.08719\n",
      "Epoch 125/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0616 - val_loss: 0.0872 - val_acc: 0.0811\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.08719 to 0.08719, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 126/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0613 - val_loss: 0.0872 - val_acc: 0.0776\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.08719 to 0.08719, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 127/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0638 - val_loss: 0.0872 - val_acc: 0.0750\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.08719 to 0.08718, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 128/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0642 - val_loss: 0.0872 - val_acc: 0.0582\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.08718\n",
      "Epoch 129/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0623 - val_loss: 0.0872 - val_acc: 0.0788\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.08718 to 0.08718, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 130/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0627 - val_loss: 0.0872 - val_acc: 0.0814\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.08718\n",
      "Epoch 131/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0616 - val_loss: 0.0872 - val_acc: 0.0799\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.08718 to 0.08718, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 132/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0594 - val_loss: 0.0872 - val_acc: 0.0682\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.08718\n",
      "Epoch 133/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0634 - val_loss: 0.0872 - val_acc: 0.0825\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.08718\n",
      "Epoch 134/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0643 - val_loss: 0.0872 - val_acc: 0.0807\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.08718 to 0.08718, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 135/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0644 - val_loss: 0.0872 - val_acc: 0.0738\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.08718 to 0.08717, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 136/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0654 - val_loss: 0.0872 - val_acc: 0.0807\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.08717 to 0.08717, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 137/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0635 - val_loss: 0.0872 - val_acc: 0.0695\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.08717 to 0.08717, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 138/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0628 - val_loss: 0.0872 - val_acc: 0.0818\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.08717\n",
      "Epoch 139/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0649 - val_loss: 0.0872 - val_acc: 0.0807\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.08717\n",
      "Epoch 140/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0635 - val_loss: 0.0872 - val_acc: 0.0735\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.08717 to 0.08716, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 141/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0652 - val_loss: 0.0872 - val_acc: 0.0738\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.08716 to 0.08716, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 142/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0665 - val_loss: 0.0872 - val_acc: 0.0739\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.08716 to 0.08716, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 143/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0652 - val_loss: 0.0872 - val_acc: 0.0744\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.08716 to 0.08716, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 144/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0667 - val_loss: 0.0872 - val_acc: 0.0579\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.08716\n",
      "Epoch 145/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0665 - val_loss: 0.0872 - val_acc: 0.0828\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.08716\n",
      "Epoch 146/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0627 - val_loss: 0.0872 - val_acc: 0.0767\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.08716 to 0.08715, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 147/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0626 - val_loss: 0.0872 - val_acc: 0.0586\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.08715\n",
      "Epoch 148/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0645 - val_loss: 0.0872 - val_acc: 0.0665\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.08715 to 0.08715, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 149/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0644 - val_loss: 0.0872 - val_acc: 0.0828\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.08715\n",
      "Epoch 150/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0647 - val_loss: 0.0871 - val_acc: 0.0771\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.08715 to 0.08715, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 151/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0627 - val_loss: 0.0871 - val_acc: 0.0783\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.08715 to 0.08715, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 152/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0656 - val_loss: 0.0872 - val_acc: 0.0825\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.08715\n",
      "Epoch 153/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0630 - val_loss: 0.0872 - val_acc: 0.0827\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.08715\n",
      "Epoch 154/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0644 - val_loss: 0.0872 - val_acc: 0.0828\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.08715\n",
      "Epoch 155/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0664 - val_loss: 0.0871 - val_acc: 0.0767\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.08715 to 0.08714, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 156/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0672 - val_loss: 0.0871 - val_acc: 0.0727\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.08714\n",
      "Epoch 157/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0664 - val_loss: 0.0871 - val_acc: 0.0757\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.08714 to 0.08714, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 158/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0683 - val_loss: 0.0871 - val_acc: 0.0729\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.08714 to 0.08714, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 159/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0661 - val_loss: 0.0871 - val_acc: 0.0728\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.08714\n",
      "Epoch 160/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0673 - val_loss: 0.0871 - val_acc: 0.0619\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.08714\n",
      "Epoch 161/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0854 - acc: 0.0620 - val_loss: 0.0871 - val_acc: 0.0791\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.08714\n",
      "Epoch 162/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0654 - val_loss: 0.0871 - val_acc: 0.0804\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.08714 to 0.08713, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 163/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0629 - val_loss: 0.0871 - val_acc: 0.0736\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.08713\n",
      "Epoch 164/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0666 - val_loss: 0.0871 - val_acc: 0.0726\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.08713 to 0.08713, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 165/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0644 - val_loss: 0.0871 - val_acc: 0.0753\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.08713 to 0.08713, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 166/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0668 - val_loss: 0.0872 - val_acc: 0.0570\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.08713\n",
      "Epoch 167/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0679 - val_loss: 0.0871 - val_acc: 0.0809\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.08713\n",
      "Epoch 168/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0640 - val_loss: 0.0871 - val_acc: 0.0559\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.08713\n",
      "Epoch 169/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0617 - val_loss: 0.0871 - val_acc: 0.0758\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.08713 to 0.08713, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 170/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0853 - acc: 0.0645 - val_loss: 0.0871 - val_acc: 0.0693\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.08713\n",
      "Epoch 171/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0653 - val_loss: 0.0871 - val_acc: 0.0715\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.08713 to 0.08712, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 172/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0663 - val_loss: 0.0871 - val_acc: 0.0663\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.08712\n",
      "Epoch 173/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0675 - val_loss: 0.0871 - val_acc: 0.0704\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.08712 to 0.08712, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 174/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0670 - val_loss: 0.0871 - val_acc: 0.0806\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.08712\n",
      "Epoch 175/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0631 - val_loss: 0.0871 - val_acc: 0.0720\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.08712\n",
      "Epoch 176/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0671 - val_loss: 0.0871 - val_acc: 0.0731\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.08712 to 0.08712, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 177/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0641 - val_loss: 0.0871 - val_acc: 0.0823\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.08712\n",
      "Epoch 178/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0634 - val_loss: 0.0871 - val_acc: 0.0708\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.08712 to 0.08711, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 179/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0639 - val_loss: 0.0871 - val_acc: 0.0787\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.08711\n",
      "Epoch 180/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0649 - val_loss: 0.0871 - val_acc: 0.0719\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.08711 to 0.08711, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 181/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0679 - val_loss: 0.0871 - val_acc: 0.0728\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.08711\n",
      "Epoch 182/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0853 - acc: 0.0632 - val_loss: 0.0871 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.08711\n",
      "Epoch 183/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0667 - val_loss: 0.0871 - val_acc: 0.0737\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.08711 to 0.08711, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 184/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0671 - val_loss: 0.0871 - val_acc: 0.0790\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.08711 to 0.08711, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 185/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0658 - val_loss: 0.0871 - val_acc: 0.0827\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.08711\n",
      "Epoch 186/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0677 - val_loss: 0.0871 - val_acc: 0.0811\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.08711 to 0.08711, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 187/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0663 - val_loss: 0.0871 - val_acc: 0.0713\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.08711 to 0.08711, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 188/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0643 - val_loss: 0.0871 - val_acc: 0.0813\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.08711\n",
      "Epoch 189/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0643 - val_loss: 0.0871 - val_acc: 0.0687\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.08711 to 0.08711, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 190/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0667 - val_loss: 0.0871 - val_acc: 0.0825\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.08711\n",
      "Epoch 191/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0666 - val_loss: 0.0871 - val_acc: 0.0550\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.08711\n",
      "Epoch 192/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0633 - val_loss: 0.0871 - val_acc: 0.0496\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.08711\n",
      "Epoch 193/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0653 - val_loss: 0.0871 - val_acc: 0.0728\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.08711 to 0.08710, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 194/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0654 - val_loss: 0.0871 - val_acc: 0.0828\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.08710\n",
      "Epoch 195/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0637 - val_loss: 0.0871 - val_acc: 0.0682\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.08710 to 0.08710, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 196/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0624 - val_loss: 0.0871 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.08710\n",
      "Epoch 197/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0631 - val_loss: 0.0871 - val_acc: 0.0701\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.08710\n",
      "Epoch 198/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0658 - val_loss: 0.0871 - val_acc: 0.0695\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.08710\n",
      "Epoch 199/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0628 - val_loss: 0.0871 - val_acc: 0.0687\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.08710 to 0.08710, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 200/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0853 - acc: 0.0648 - val_loss: 0.0871 - val_acc: 0.0768\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.08710\n",
      "Time elapsed (hh:mm:ss.ms) 0:49:14.830534\n"
     ]
    }
   ],
   "source": [
    "hist_spae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/spae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = spae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size*2,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.08532756941825267\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_spae_sigmoid_adam_mse  = plot_hist_auto(hist_spae_sigmoid_adam_mse, './Figures/hist_spae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valueDict = {\n",
    "    'loss_value_ae_sigmoid_adam_mse': best_loss_value_ae_sigmoid_adam_mse,\n",
    "    'loss_value_spae_sigmoid_adam_mse': best_loss_value_spae_sigmoid_adam_mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_value_ae_sigmoid_adam_mse': 0.049174800142044395,\n",
       " 'loss_value_spae_sigmoid_adam_mse': 0.08532756941825267}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330452, 140)\n",
      "(415767, 140)\n",
      "(1330452, 140)\n",
      "(415767, 140)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train_x_asam.shape)\n",
    "print(enc_test_x_asam.shape)\n",
    "\n",
    "print(enc_train_x_spsam.shape)\n",
    "print(enc_test_x_spsam.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 15:36:46.626637 139973542926144 deprecation.py:323] From /home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_asam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 15:36:46 2019\n",
      "Train on 1064361 samples, validate on 266091 samples\n",
      "Epoch 1/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.2148 - acc: 0.8911 - val_loss: 0.1276 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12761, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.1214 - acc: 0.9364 - val_loss: 0.1054 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12761 to 0.10542, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.1016 - acc: 0.9478 - val_loss: 0.0980 - val_acc: 0.9494\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10542 to 0.09796, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0878 - acc: 0.9560 - val_loss: 0.0830 - val_acc: 0.9573\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09796 to 0.08296, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0764 - acc: 0.9624 - val_loss: 0.0721 - val_acc: 0.9634\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08296 to 0.07207, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0681 - acc: 0.9668 - val_loss: 0.0635 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07207 to 0.06347, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0612 - acc: 0.9703 - val_loss: 0.0573 - val_acc: 0.9719\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06347 to 0.05726, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0558 - acc: 0.9730 - val_loss: 0.0660 - val_acc: 0.9703\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05726\n",
      "Epoch 9/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0516 - acc: 0.9751 - val_loss: 0.0463 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05726 to 0.04634, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0487 - acc: 0.9763 - val_loss: 0.0456 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04634 to 0.04563, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0455 - acc: 0.9780 - val_loss: 0.0411 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04563 to 0.04110, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0432 - acc: 0.9789 - val_loss: 0.0433 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04110\n",
      "Epoch 13/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0412 - acc: 0.9797 - val_loss: 0.0476 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04110\n",
      "Epoch 14/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0397 - acc: 0.9806 - val_loss: 0.0379 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04110 to 0.03788, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0379 - acc: 0.9815 - val_loss: 0.0378 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03788 to 0.03785, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 16/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0363 - acc: 0.9821 - val_loss: 0.0372 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03785 to 0.03722, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0355 - acc: 0.9824 - val_loss: 0.0323 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03722 to 0.03229, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0338 - acc: 0.9833 - val_loss: 0.0363 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03229\n",
      "Epoch 19/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0333 - acc: 0.9835 - val_loss: 0.0311 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.03229 to 0.03111, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1064361/1064361 [==============================] - 19s 17us/step - loss: 0.0330 - acc: 0.9837 - val_loss: 0.0275 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.03111 to 0.02747, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0322 - acc: 0.9841 - val_loss: 0.0280 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02747\n",
      "Epoch 22/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0309 - acc: 0.9847 - val_loss: 0.0282 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02747\n",
      "Epoch 23/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0302 - acc: 0.9851 - val_loss: 0.0294 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02747\n",
      "Epoch 24/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0299 - acc: 0.9853 - val_loss: 0.0294 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02747\n",
      "Epoch 25/200\n",
      "1064361/1064361 [==============================] - 18s 17us/step - loss: 0.0289 - acc: 0.9858 - val_loss: 0.0290 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02747\n",
      "Time elapsed (hh:mm:ss.ms) 0:07:43.967735\n"
     ]
    }
   ],
   "source": [
    "hist_ae_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ae_ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = ae_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_asam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_ae_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_ae_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_ae_ann_2h_unisoftsigbinlosadam, './Figures/ae_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_ae_ann_2h_prob_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict(ae_ann_2h_unisoftsigbinlosadam,enc_test_x_asam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_asam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 15:44:31 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.3671 - acc: 0.8163\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1706 - acc: 0.9146\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1465 - acc: 0.9237\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1359 - acc: 0.9281\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1299 - acc: 0.9318\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1211 - acc: 0.9363\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1150 - acc: 0.9401\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1103 - acc: 0.9427\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1057 - acc: 0.9456\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1003 - acc: 0.9491\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0957 - acc: 0.9518\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0921 - acc: 0.9534\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0883 - acc: 0.9562\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0832 - acc: 0.9588\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0805 - acc: 0.9607\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0779 - acc: 0.9620\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0741 - acc: 0.9639\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0728 - acc: 0.9644\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0688 - acc: 0.9666\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0667 - acc: 0.9681\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0641 - acc: 0.9689\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0632 - acc: 0.9697\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0610 - acc: 0.9705\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0604 - acc: 0.9711\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0577 - acc: 0.9726\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0560 - acc: 0.9730\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0550 - acc: 0.9737\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0526 - acc: 0.9748\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0526 - acc: 0.9750\n",
      "Epoch 30/100\n",
      "   420/332613 [..............................] - ETA: 2:00:47 - loss: 0.0660 - acc: 0.9690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.870729). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.939721). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332613/332613 [==============================] - 20s 60us/step - loss: 0.0518 - acc: 0.9755\n",
      "Epoch 31/100\n",
      "   420/332613 [..............................] - ETA: 1:13:44 - loss: 0.0230 - acc: 0.9952"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.564531). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/home/user/anaconda3/envs/deepl/lib/python3.7/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.282880). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332613/332613 [==============================] - 17s 52us/step - loss: 0.0510 - acc: 0.9756\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0497 - acc: 0.9763\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0490 - acc: 0.9765\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0477 - acc: 0.9774\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0463 - acc: 0.9779\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0462 - acc: 0.9781\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0442 - acc: 0.9788\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0440 - acc: 0.9790\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0440 - acc: 0.9789\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0432 - acc: 0.9795\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0422 - acc: 0.9796\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0415 - acc: 0.9804\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0408 - acc: 0.9804\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0405 - acc: 0.9806\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0402 - acc: 0.9808\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0396 - acc: 0.9811\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0385 - acc: 0.9815\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0387 - acc: 0.9816\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0386 - acc: 0.9814\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0385 - acc: 0.9820\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.0376 - acc: 0.9822\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0360 - acc: 0.9827\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0369 - acc: 0.9822\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0364 - acc: 0.9826\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0358 - acc: 0.9826\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.0362 - acc: 0.9830\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0342 - acc: 0.9837\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0355 - acc: 0.9830\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0344 - acc: 0.9835\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0347 - acc: 0.9835\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0345 - acc: 0.9836\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0337 - acc: 0.9840\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0331 - acc: 0.9844\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0329 - acc: 0.9843\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0326 - acc: 0.9846\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0327 - acc: 0.9846\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0324 - acc: 0.9847\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.0322 - acc: 0.9846\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0317 - acc: 0.9850\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0316 - acc: 0.9852\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0311 - acc: 0.9853\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0309 - acc: 0.9855\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0308 - acc: 0.9854\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0305 - acc: 0.9854\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0304 - acc: 0.9857\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0306 - acc: 0.9855\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0307 - acc: 0.9857\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0296 - acc: 0.9861\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0295 - acc: 0.9860\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0299 - acc: 0.9860\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0291 - acc: 0.9862\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0294 - acc: 0.9862\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0294 - acc: 0.9865\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0294 - acc: 0.9861\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0296 - acc: 0.9861\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0289 - acc: 0.9863\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0288 - acc: 0.9865\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0280 - acc: 0.9870\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0283 - acc: 0.9870\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0288 - acc: 0.9865\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0282 - acc: 0.9870\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0282 - acc: 0.9869\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0280 - acc: 0.9869\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0285 - acc: 0.9867\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0279 - acc: 0.9871\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0275 - acc: 0.9873\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0280 - acc: 0.9871\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0276 - acc: 0.9872\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0281 - acc: 0.9871\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0275 - acc: 0.9873\n",
      "83154/83154 [==============================] - 2s 21us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.3667 - acc: 0.8158\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.1707 - acc: 0.9138\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1440 - acc: 0.9245\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1350 - acc: 0.9288\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1273 - acc: 0.9320\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1198 - acc: 0.9373\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1142 - acc: 0.9407\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1073 - acc: 0.9440\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.1017 - acc: 0.9468\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0983 - acc: 0.9490\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0933 - acc: 0.9520\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0893 - acc: 0.9542\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0846 - acc: 0.9569\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0823 - acc: 0.9585\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0787 - acc: 0.9600\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0754 - acc: 0.9625\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0737 - acc: 0.9633\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0699 - acc: 0.9651\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0687 - acc: 0.9665\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0659 - acc: 0.9673\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0645 - acc: 0.9683\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0620 - acc: 0.9697\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0612 - acc: 0.9699\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0596 - acc: 0.9713\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0580 - acc: 0.9720\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0570 - acc: 0.9724\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0553 - acc: 0.9729\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0548 - acc: 0.9733\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.0530 - acc: 0.9743\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0519 - acc: 0.9747\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0509 - acc: 0.9754\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0509 - acc: 0.9753\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0483 - acc: 0.9767\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0484 - acc: 0.9767\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0478 - acc: 0.9769\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0461 - acc: 0.9778\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0463 - acc: 0.9778\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0451 - acc: 0.9778\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0439 - acc: 0.9789\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0442 - acc: 0.9785\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0430 - acc: 0.9790\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0434 - acc: 0.9791\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0416 - acc: 0.9798\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 10s 32us/step - loss: 0.0416 - acc: 0.9798\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0406 - acc: 0.9802\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0400 - acc: 0.9807\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0404 - acc: 0.9805\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0400 - acc: 0.9807\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0397 - acc: 0.9809\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0380 - acc: 0.9813\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0389 - acc: 0.9811\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0379 - acc: 0.9816\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0375 - acc: 0.9817\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0384 - acc: 0.9813\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0372 - acc: 0.9819\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0370 - acc: 0.9820\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0357 - acc: 0.9827\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0359 - acc: 0.9826\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0360 - acc: 0.9823\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0360 - acc: 0.9825\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0344 - acc: 0.9830\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0354 - acc: 0.9828\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0338 - acc: 0.9837\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0349 - acc: 0.9830\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0344 - acc: 0.9834\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0339 - acc: 0.9835\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0346 - acc: 0.9831\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0332 - acc: 0.9837\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0332 - acc: 0.9839\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0323 - acc: 0.9842\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 10s 30us/step - loss: 0.0340 - acc: 0.9834\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0326 - acc: 0.9840\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0324 - acc: 0.9840\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0322 - acc: 0.9843\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0312 - acc: 0.9849\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0322 - acc: 0.9845\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0321 - acc: 0.9846\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0314 - acc: 0.9847\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0314 - acc: 0.9844\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0314 - acc: 0.9849\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0312 - acc: 0.9850\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0310 - acc: 0.9849\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0317 - acc: 0.9846\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0303 - acc: 0.9853\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0312 - acc: 0.9849\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0304 - acc: 0.9854\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0311 - acc: 0.9850\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0299 - acc: 0.9855\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0315 - acc: 0.9849\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0301 - acc: 0.9855\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0293 - acc: 0.9858\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0299 - acc: 0.9856\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0301 - acc: 0.9855\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0299 - acc: 0.9856\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0288 - acc: 0.9861\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0299 - acc: 0.9856\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0290 - acc: 0.9860\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0302 - acc: 0.9854\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0292 - acc: 0.9859\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 10s 31us/step - loss: 0.0294 - acc: 0.9859\n",
      "83154/83154 [==============================] - 1s 7us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.3635 - acc: 0.8168\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1726 - acc: 0.9131\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.1458 - acc: 0.9236\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1349 - acc: 0.9287\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1277 - acc: 0.9323\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1219 - acc: 0.9352\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1154 - acc: 0.9395\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1112 - acc: 0.9415\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1077 - acc: 0.9434\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.1026 - acc: 0.9465\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0987 - acc: 0.9486\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0968 - acc: 0.9500\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0928 - acc: 0.9522\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0890 - acc: 0.9543\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0865 - acc: 0.9559\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0827 - acc: 0.9580\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0809 - acc: 0.9587\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0784 - acc: 0.9603\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0750 - acc: 0.9624\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0733 - acc: 0.9637\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0709 - acc: 0.9647\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0686 - acc: 0.9658\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.0673 - acc: 0.9670\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0655 - acc: 0.9681\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0639 - acc: 0.9684\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0623 - acc: 0.9695\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0602 - acc: 0.9706\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0592 - acc: 0.9715\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0583 - acc: 0.9719\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0565 - acc: 0.9726\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0548 - acc: 0.9736\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0546 - acc: 0.9738\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0528 - acc: 0.9744\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0517 - acc: 0.9751\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0520 - acc: 0.9754\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0500 - acc: 0.9760\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0490 - acc: 0.9765\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0483 - acc: 0.9767\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0474 - acc: 0.9770\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.0470 - acc: 0.9773\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0466 - acc: 0.9777\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0455 - acc: 0.9782\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0446 - acc: 0.9786\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0442 - acc: 0.9787\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0434 - acc: 0.9791\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.0428 - acc: 0.9794\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0426 - acc: 0.9796\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0421 - acc: 0.9797\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0413 - acc: 0.9802\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0403 - acc: 0.9806\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0407 - acc: 0.9803\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0399 - acc: 0.9808\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0386 - acc: 0.9812\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0397 - acc: 0.9807\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0390 - acc: 0.9812\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0381 - acc: 0.9816\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0382 - acc: 0.9814\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0372 - acc: 0.9821\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0376 - acc: 0.9817\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0372 - acc: 0.9818\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0364 - acc: 0.9826\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0372 - acc: 0.9819\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0363 - acc: 0.9826\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0366 - acc: 0.9823\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0358 - acc: 0.9827\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0349 - acc: 0.9830\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0354 - acc: 0.9828\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0348 - acc: 0.9831\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0351 - acc: 0.9831\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0347 - acc: 0.9834\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0352 - acc: 0.9830\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0338 - acc: 0.9835\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0340 - acc: 0.9835\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0338 - acc: 0.9837\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0333 - acc: 0.9842\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0334 - acc: 0.9838\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0327 - acc: 0.9839\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0326 - acc: 0.9842\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0323 - acc: 0.9844\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0321 - acc: 0.9844\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0325 - acc: 0.9842\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0319 - acc: 0.9847\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0318 - acc: 0.9847\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0313 - acc: 0.9850\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0323 - acc: 0.9848\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0312 - acc: 0.9851\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0310 - acc: 0.9853\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0319 - acc: 0.9848\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0305 - acc: 0.9855\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0309 - acc: 0.9852\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0306 - acc: 0.9856\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0304 - acc: 0.9854\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 10s 31us/step - loss: 0.0306 - acc: 0.9854\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0313 - acc: 0.9851\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0297 - acc: 0.9858\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0302 - acc: 0.9856\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0299 - acc: 0.9856\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0299 - acc: 0.9857\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0298 - acc: 0.9860\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0304 - acc: 0.9855\n",
      "83153/83153 [==============================] - 1s 8us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.3696 - acc: 0.8132\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.1697 - acc: 0.9143\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1445 - acc: 0.9249\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1355 - acc: 0.9290\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1265 - acc: 0.9337\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1204 - acc: 0.9370\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.1148 - acc: 0.9402\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1101 - acc: 0.9433\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.1051 - acc: 0.9453\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1004 - acc: 0.9485\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0956 - acc: 0.9516\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0914 - acc: 0.9538\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0884 - acc: 0.9552\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0851 - acc: 0.9573\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0826 - acc: 0.9589\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0794 - acc: 0.9603\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0768 - acc: 0.9616\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0737 - acc: 0.9635\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0727 - acc: 0.9646\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0687 - acc: 0.9659\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0678 - acc: 0.9670\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0654 - acc: 0.9683\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0636 - acc: 0.9687\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0619 - acc: 0.9698\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0608 - acc: 0.9704\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0598 - acc: 0.9711\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0576 - acc: 0.9721\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0567 - acc: 0.9729\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0554 - acc: 0.9736\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0544 - acc: 0.9738\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0519 - acc: 0.9749\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0526 - acc: 0.9749\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0513 - acc: 0.9755\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0495 - acc: 0.9761\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0503 - acc: 0.9762\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0481 - acc: 0.9770\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0480 - acc: 0.9772\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0471 - acc: 0.9773\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0463 - acc: 0.9778\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0455 - acc: 0.9782\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0441 - acc: 0.9788\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0445 - acc: 0.9789\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0433 - acc: 0.9793\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0438 - acc: 0.9790\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0426 - acc: 0.9797\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0423 - acc: 0.9800\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0411 - acc: 0.9803\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0409 - acc: 0.9805\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0410 - acc: 0.9805\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0395 - acc: 0.9810\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0397 - acc: 0.9811\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0394 - acc: 0.9812\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0387 - acc: 0.9816\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0393 - acc: 0.9812\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0377 - acc: 0.9820\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0384 - acc: 0.9816\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0375 - acc: 0.9823\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0375 - acc: 0.9822\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0364 - acc: 0.9827\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0363 - acc: 0.9827\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0359 - acc: 0.9830\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0360 - acc: 0.9830\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0350 - acc: 0.9834\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0347 - acc: 0.9835\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0347 - acc: 0.9835\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0349 - acc: 0.9836\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0344 - acc: 0.9834\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0348 - acc: 0.9836\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0335 - acc: 0.9840\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0333 - acc: 0.9842\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0337 - acc: 0.9841\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0337 - acc: 0.9840\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0330 - acc: 0.9844\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0322 - acc: 0.9844\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0334 - acc: 0.9841\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0321 - acc: 0.9846\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0325 - acc: 0.9849\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0318 - acc: 0.9850\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0316 - acc: 0.9847\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0326 - acc: 0.9847\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0316 - acc: 0.9849\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0316 - acc: 0.9848\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0320 - acc: 0.9850\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0307 - acc: 0.9853\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0316 - acc: 0.9851\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0312 - acc: 0.9851\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0314 - acc: 0.9849\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0298 - acc: 0.9857\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0311 - acc: 0.9855\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0308 - acc: 0.9852\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0304 - acc: 0.9856\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0295 - acc: 0.9859\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0307 - acc: 0.9855\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0303 - acc: 0.9857\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0308 - acc: 0.9856\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0293 - acc: 0.9862\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0307 - acc: 0.9855\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0294 - acc: 0.9863\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0301 - acc: 0.9860\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0303 - acc: 0.9857\n",
      "83153/83153 [==============================] - 1s 9us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.3606 - acc: 0.8189\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1720 - acc: 0.9138\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1468 - acc: 0.9235\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1350 - acc: 0.9290\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1274 - acc: 0.9324\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1225 - acc: 0.9357\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1144 - acc: 0.9401\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1109 - acc: 0.9426\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1045 - acc: 0.9459\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1011 - acc: 0.9481\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0954 - acc: 0.9509\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0911 - acc: 0.9533\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0873 - acc: 0.9554\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0833 - acc: 0.9578\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0803 - acc: 0.9598\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0770 - acc: 0.9617\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0737 - acc: 0.9632\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0724 - acc: 0.9641\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0692 - acc: 0.9653\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0673 - acc: 0.9667\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0657 - acc: 0.9676\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0633 - acc: 0.9688\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0628 - acc: 0.9689\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0606 - acc: 0.9700\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0594 - acc: 0.9708\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0587 - acc: 0.9707\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0570 - acc: 0.9720\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0550 - acc: 0.9728\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0555 - acc: 0.9732\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0535 - acc: 0.9734\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0524 - acc: 0.9744\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0521 - acc: 0.9745\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0497 - acc: 0.9756\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0498 - acc: 0.9756\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0483 - acc: 0.9764\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0482 - acc: 0.9764\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0477 - acc: 0.9766\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0464 - acc: 0.9771\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0457 - acc: 0.9774\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0454 - acc: 0.9777\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0442 - acc: 0.9782\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0446 - acc: 0.9781\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0440 - acc: 0.9788\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0428 - acc: 0.9788\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0419 - acc: 0.9793\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0420 - acc: 0.9792\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0408 - acc: 0.9800\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0419 - acc: 0.9794\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0402 - acc: 0.9803\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0400 - acc: 0.9802\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0398 - acc: 0.9804\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0392 - acc: 0.9808\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0385 - acc: 0.9811\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0388 - acc: 0.9809\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0384 - acc: 0.9811\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0372 - acc: 0.9816\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0380 - acc: 0.9814\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0366 - acc: 0.9822\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0372 - acc: 0.9818\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0359 - acc: 0.9822\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0367 - acc: 0.9822\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0355 - acc: 0.9826\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0358 - acc: 0.9827\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0354 - acc: 0.9827\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0353 - acc: 0.9830\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0343 - acc: 0.9833\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0339 - acc: 0.9834\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0337 - acc: 0.9839\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0340 - acc: 0.9835\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0346 - acc: 0.9833\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0334 - acc: 0.9836\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0333 - acc: 0.9839\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0330 - acc: 0.9840\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0330 - acc: 0.9840\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0312 - acc: 0.9848\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0326 - acc: 0.9842\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0321 - acc: 0.9844\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0318 - acc: 0.9847\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0319 - acc: 0.9847\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0316 - acc: 0.9848\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 10s 32us/step - loss: 0.0314 - acc: 0.9849\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0318 - acc: 0.9849\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0304 - acc: 0.9854\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0315 - acc: 0.9847\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0299 - acc: 0.9855\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0311 - acc: 0.9850\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0303 - acc: 0.9853\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0306 - acc: 0.9853\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0308 - acc: 0.9852\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0296 - acc: 0.9859\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0301 - acc: 0.9857\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0296 - acc: 0.9859\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0296 - acc: 0.9858\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0296 - acc: 0.9859\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0303 - acc: 0.9853\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0289 - acc: 0.9860\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0294 - acc: 0.9860\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0294 - acc: 0.9857\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 11s 32us/step - loss: 0.0286 - acc: 0.9863\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0284 - acc: 0.9862\n",
      "83153/83153 [==============================] - 1s 10us/step\n",
      "Time elapsed (hh:mm:ss.ms) 1:30:12.663542\n",
      "Overall accuracy of Neural Network model: 0.9854149078690709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 90.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9833    0.9876    0.9854    207840\n",
      "           1     0.9876    0.9832    0.9854    207927\n",
      "\n",
      "    accuracy                         0.9854    415767\n",
      "   macro avg     0.9854    0.9854    0.9854    415767\n",
      "weighted avg     0.9854    0.9854    0.9854    415767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_ae_ann_2h_prob_unisoftsigbinlosadam,pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sp_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_spsam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 17:14:47 2019\n",
      "Train on 1064361 samples, validate on 266091 samples\n",
      "Epoch 1/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.2181 - acc: 0.8894 - val_loss: 0.1290 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12900, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.1192 - acc: 0.9385 - val_loss: 0.1138 - val_acc: 0.9452\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12900 to 0.11378, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0981 - acc: 0.9509 - val_loss: 0.1009 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11378 to 0.10089, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0822 - acc: 0.9596 - val_loss: 0.0732 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10089 to 0.07322, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0709 - acc: 0.9654 - val_loss: 0.0662 - val_acc: 0.9686\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07322 to 0.06621, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0618 - acc: 0.9701 - val_loss: 0.0605 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.06621 to 0.06048, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0555 - acc: 0.9733 - val_loss: 0.0546 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06048 to 0.05461, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0506 - acc: 0.9755 - val_loss: 0.0531 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05461 to 0.05311, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0469 - acc: 0.9772 - val_loss: 0.0562 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05311\n",
      "Epoch 10/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0439 - acc: 0.9789 - val_loss: 0.0396 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05311 to 0.03964, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0413 - acc: 0.9799 - val_loss: 0.0467 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03964\n",
      "Epoch 12/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0396 - acc: 0.9806 - val_loss: 0.0372 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03964 to 0.03723, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0378 - acc: 0.9815 - val_loss: 0.0350 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03723 to 0.03505, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0365 - acc: 0.9823 - val_loss: 0.0339 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03505 to 0.03393, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0347 - acc: 0.9832 - val_loss: 0.0346 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03393\n",
      "Epoch 16/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0341 - acc: 0.9835 - val_loss: 0.0410 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03393\n",
      "Epoch 17/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0322 - acc: 0.9844 - val_loss: 0.0335 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03393 to 0.03347, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0316 - acc: 0.9846 - val_loss: 0.0338 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03347\n",
      "Epoch 19/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0307 - acc: 0.9850 - val_loss: 0.0274 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.03347 to 0.02740, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0298 - acc: 0.9854 - val_loss: 0.0260 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02740 to 0.02597, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0286 - acc: 0.9860 - val_loss: 0.0268 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02597\n",
      "Epoch 22/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0285 - acc: 0.9861 - val_loss: 0.0270 - val_acc: 0.9876\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02597\n",
      "Epoch 23/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0279 - acc: 0.9865 - val_loss: 0.0233 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02597 to 0.02331, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 24/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0269 - acc: 0.9870 - val_loss: 0.0249 - val_acc: 0.9876\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02331\n",
      "Epoch 25/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0262 - acc: 0.9874 - val_loss: 0.0247 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02331\n",
      "Epoch 26/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0265 - acc: 0.9873 - val_loss: 0.0263 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02331\n",
      "Epoch 27/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0257 - acc: 0.9876 - val_loss: 0.0235 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02331\n",
      "Epoch 28/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0250 - acc: 0.9879 - val_loss: 0.0222 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02331 to 0.02225, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 29/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0246 - acc: 0.9882 - val_loss: 0.0313 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02225\n",
      "Epoch 30/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0243 - acc: 0.9882 - val_loss: 0.0215 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.02225 to 0.02153, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 31/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0236 - acc: 0.9885 - val_loss: 0.0211 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.02153 to 0.02107, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 32/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0238 - acc: 0.9885 - val_loss: 0.0233 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02107\n",
      "Epoch 33/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0232 - acc: 0.9889 - val_loss: 0.0343 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02107\n",
      "Epoch 34/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0230 - acc: 0.9890 - val_loss: 0.0207 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.02107 to 0.02071, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 35/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0224 - acc: 0.9892 - val_loss: 0.0202 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.02071 to 0.02023, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 36/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0222 - acc: 0.9893 - val_loss: 0.0208 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02023\n",
      "Epoch 37/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0220 - acc: 0.9895 - val_loss: 0.0236 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02023\n",
      "Epoch 38/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0217 - acc: 0.9897 - val_loss: 0.0190 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.02023 to 0.01901, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 39/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0208 - acc: 0.9900 - val_loss: 0.0201 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01901\n",
      "Epoch 40/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0214 - acc: 0.9898 - val_loss: 0.0239 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01901\n",
      "Epoch 41/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0210 - acc: 0.9901 - val_loss: 0.0204 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01901\n",
      "Epoch 42/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0202 - acc: 0.9903 - val_loss: 0.0185 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01901 to 0.01846, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 43/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0204 - acc: 0.9903 - val_loss: 0.0179 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01846 to 0.01790, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 44/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0199 - acc: 0.9907 - val_loss: 0.0194 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01790\n",
      "Epoch 45/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0201 - acc: 0.9905 - val_loss: 0.0166 - val_acc: 0.9918\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.01790 to 0.01660, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 46/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0197 - acc: 0.9907 - val_loss: 0.0225 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01660\n",
      "Epoch 47/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0199 - acc: 0.9906 - val_loss: 0.0176 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01660\n",
      "Epoch 48/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0192 - acc: 0.9910 - val_loss: 0.0167 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01660\n",
      "Epoch 49/200\n",
      "1064361/1064361 [==============================] - 20s 18us/step - loss: 0.0193 - acc: 0.9909 - val_loss: 0.0159 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.01660 to 0.01590, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 50/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0192 - acc: 0.9910 - val_loss: 0.0218 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01590\n",
      "Epoch 51/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0189 - acc: 0.9912 - val_loss: 0.0151 - val_acc: 0.9929\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.01590 to 0.01513, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 52/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0191 - acc: 0.9911 - val_loss: 0.0195 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01513\n",
      "Epoch 53/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0185 - acc: 0.9913 - val_loss: 0.0185 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01513\n",
      "Epoch 54/200\n",
      "1064361/1064361 [==============================] - 19s 18us/step - loss: 0.0183 - acc: 0.9913 - val_loss: 0.0167 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01513\n",
      "Epoch 55/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0185 - acc: 0.9913 - val_loss: 0.0169 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01513\n",
      "Epoch 56/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0183 - acc: 0.9914 - val_loss: 0.0163 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01513\n",
      "Time elapsed (hh:mm:ss.ms) 0:18:29.964149\n"
     ]
    }
   ],
   "source": [
    "hist_sp_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = sp_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_spsam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_sp_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_sp_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_sp_ann_2h_unisoftsigbinlosadam, './Figures/sp_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict(sp_ann_2h_unisoftsigbinlosadam,enc_test_x_spsam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_spsam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 17:33:17 2019\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.3625 - acc: 0.8172\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1706 - acc: 0.9145\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1460 - acc: 0.9245\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1366 - acc: 0.9289\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1266 - acc: 0.9338\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1214 - acc: 0.9364\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.1144 - acc: 0.9398\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1083 - acc: 0.9435\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.1043 - acc: 0.9459\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1008 - acc: 0.9478\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0947 - acc: 0.9516\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0912 - acc: 0.9534\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.0884 - acc: 0.9549\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0839 - acc: 0.9573\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.0818 - acc: 0.9591\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0782 - acc: 0.9608\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.0760 - acc: 0.9622\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0734 - acc: 0.9632\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0705 - acc: 0.9650\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0695 - acc: 0.9656\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0677 - acc: 0.9666\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0640 - acc: 0.9683\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0628 - acc: 0.9687\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0622 - acc: 0.9695\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0600 - acc: 0.9705\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0582 - acc: 0.9715\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0560 - acc: 0.9724\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0551 - acc: 0.9733\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0546 - acc: 0.9732\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0528 - acc: 0.9744\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0512 - acc: 0.9750\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0513 - acc: 0.9751\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0502 - acc: 0.9758\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0496 - acc: 0.9761\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0481 - acc: 0.9765\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0477 - acc: 0.9769\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0471 - acc: 0.9773\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0455 - acc: 0.9780\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0454 - acc: 0.9780\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0441 - acc: 0.9785\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0437 - acc: 0.9789\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0428 - acc: 0.9793\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0432 - acc: 0.9791\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0419 - acc: 0.9796\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0418 - acc: 0.9798\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0411 - acc: 0.9803\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0415 - acc: 0.9800\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0396 - acc: 0.9807\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0402 - acc: 0.9807\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0395 - acc: 0.9808\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0389 - acc: 0.9812\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0384 - acc: 0.9813\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0382 - acc: 0.9815\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0372 - acc: 0.9819\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0374 - acc: 0.9823\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0369 - acc: 0.9823\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0368 - acc: 0.9822\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0364 - acc: 0.9824\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0357 - acc: 0.9827\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0355 - acc: 0.9828\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0352 - acc: 0.9830\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0356 - acc: 0.9832\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0343 - acc: 0.9833\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0343 - acc: 0.9835\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0341 - acc: 0.9836\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0341 - acc: 0.9837\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0338 - acc: 0.9839\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0340 - acc: 0.9836\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0335 - acc: 0.9837\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0326 - acc: 0.9843\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0334 - acc: 0.9839\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0327 - acc: 0.9845\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0323 - acc: 0.9846\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0329 - acc: 0.9844\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0319 - acc: 0.9847\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0324 - acc: 0.9847\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0314 - acc: 0.9850\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0312 - acc: 0.9849\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0318 - acc: 0.9847\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0318 - acc: 0.9846\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0314 - acc: 0.9852\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0312 - acc: 0.9851\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0306 - acc: 0.9853\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0304 - acc: 0.9853\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0310 - acc: 0.9853\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0305 - acc: 0.9853\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0306 - acc: 0.9852\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 11s 32us/step - loss: 0.0302 - acc: 0.9858\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0305 - acc: 0.9856\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0298 - acc: 0.9859\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0309 - acc: 0.9853\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0294 - acc: 0.9859\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0310 - acc: 0.9857\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0301 - acc: 0.9858\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0294 - acc: 0.9862\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0300 - acc: 0.9858\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0290 - acc: 0.9860\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0291 - acc: 0.9860\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0292 - acc: 0.9862\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0292 - acc: 0.9861\n",
      "83154/83154 [==============================] - 1s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 13s 38us/step - loss: 0.3633 - acc: 0.8174\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1717 - acc: 0.9140\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1474 - acc: 0.9240\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1351 - acc: 0.9288\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.1269 - acc: 0.9330\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1205 - acc: 0.9368\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.1132 - acc: 0.9417\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1072 - acc: 0.9452\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.1030 - acc: 0.9478\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0969 - acc: 0.9510\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.0908 - acc: 0.9543\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0887 - acc: 0.9556\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0841 - acc: 0.9582\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0806 - acc: 0.9602\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0769 - acc: 0.9622\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.0743 - acc: 0.9635\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.0710 - acc: 0.9654\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0695 - acc: 0.9664\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0672 - acc: 0.9674\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0639 - acc: 0.9693\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0614 - acc: 0.9706\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0608 - acc: 0.9706\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0579 - acc: 0.9724\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0570 - acc: 0.9726\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0553 - acc: 0.9739\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.0541 - acc: 0.9740\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0522 - acc: 0.9752\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0522 - acc: 0.9752\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0502 - acc: 0.9762\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0496 - acc: 0.9763\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0479 - acc: 0.9769\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0468 - acc: 0.9777\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0466 - acc: 0.9778\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0457 - acc: 0.9782\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0454 - acc: 0.9786\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0435 - acc: 0.9792\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0439 - acc: 0.9790\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0427 - acc: 0.9795\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0431 - acc: 0.9793\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0423 - acc: 0.9798\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0412 - acc: 0.9803\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0409 - acc: 0.9805\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0396 - acc: 0.9810\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0398 - acc: 0.9811\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0395 - acc: 0.9812\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0391 - acc: 0.9812\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0374 - acc: 0.9818\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0381 - acc: 0.9816\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0368 - acc: 0.9822\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0371 - acc: 0.9822\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0364 - acc: 0.9824\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0369 - acc: 0.9824\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0362 - acc: 0.9825\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0352 - acc: 0.9831\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0351 - acc: 0.9832\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0350 - acc: 0.9832\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0345 - acc: 0.9835\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0345 - acc: 0.9833\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0340 - acc: 0.9838\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0332 - acc: 0.9840\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0333 - acc: 0.9840\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0327 - acc: 0.9843\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0335 - acc: 0.9838\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0329 - acc: 0.9841\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0322 - acc: 0.9844\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0321 - acc: 0.9844\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0322 - acc: 0.9845\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0322 - acc: 0.9845\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0310 - acc: 0.9851\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0310 - acc: 0.9847\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0309 - acc: 0.9849\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0310 - acc: 0.9851\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0308 - acc: 0.9852\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0308 - acc: 0.9854\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0298 - acc: 0.9855\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0305 - acc: 0.9852\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0304 - acc: 0.9851\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0291 - acc: 0.9859\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0296 - acc: 0.9857\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0297 - acc: 0.9857\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0298 - acc: 0.9857\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0288 - acc: 0.9863\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0289 - acc: 0.9863\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0300 - acc: 0.9856\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0291 - acc: 0.9860\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0283 - acc: 0.9863\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0290 - acc: 0.9862\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0286 - acc: 0.9863\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0284 - acc: 0.9865\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0283 - acc: 0.9864\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0294 - acc: 0.9862\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0275 - acc: 0.9869\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0276 - acc: 0.9867\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0288 - acc: 0.9862\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0280 - acc: 0.9868\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0279 - acc: 0.9868\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0286 - acc: 0.9863\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0271 - acc: 0.9868\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 11s 33us/step - loss: 0.0285 - acc: 0.9863\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0273 - acc: 0.9869\n",
      "83154/83154 [==============================] - 1s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.3685 - acc: 0.8141\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1722 - acc: 0.9135\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1473 - acc: 0.9238\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1330 - acc: 0.9304\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1264 - acc: 0.9339\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1193 - acc: 0.9382\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1121 - acc: 0.9424\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1074 - acc: 0.9450\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1016 - acc: 0.9479\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0964 - acc: 0.9507\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0925 - acc: 0.9531\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0895 - acc: 0.9552\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0839 - acc: 0.9579\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0821 - acc: 0.9593\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0789 - acc: 0.9612\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0757 - acc: 0.9628\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0724 - acc: 0.9650\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0696 - acc: 0.9661\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0683 - acc: 0.9671\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0666 - acc: 0.9681\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0628 - acc: 0.9696\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0617 - acc: 0.9701\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0604 - acc: 0.9711\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0580 - acc: 0.9722\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0567 - acc: 0.9731\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0546 - acc: 0.9740\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0543 - acc: 0.9743\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0528 - acc: 0.9749\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0517 - acc: 0.9755\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0506 - acc: 0.9758\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0493 - acc: 0.9766\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0485 - acc: 0.9769\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0481 - acc: 0.9772\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0467 - acc: 0.9780\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0464 - acc: 0.9779\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0454 - acc: 0.9781\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0447 - acc: 0.9788\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0442 - acc: 0.9791\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0435 - acc: 0.9793\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0428 - acc: 0.9795\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0419 - acc: 0.9801\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0419 - acc: 0.9801\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0411 - acc: 0.9803\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0400 - acc: 0.9808\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0402 - acc: 0.9805\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0394 - acc: 0.9810\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0393 - acc: 0.9811\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0387 - acc: 0.9812\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0382 - acc: 0.9817\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0380 - acc: 0.9816\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0377 - acc: 0.9817\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0372 - acc: 0.9818\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0369 - acc: 0.9822\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0357 - acc: 0.9828\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0359 - acc: 0.9825\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0357 - acc: 0.9829\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0347 - acc: 0.9829\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0352 - acc: 0.9831\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0341 - acc: 0.9832\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0345 - acc: 0.9834\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0345 - acc: 0.9836\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0333 - acc: 0.9837\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0337 - acc: 0.9838\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0332 - acc: 0.9840\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0331 - acc: 0.9841\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0330 - acc: 0.9841\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0325 - acc: 0.9843\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0321 - acc: 0.9844\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0317 - acc: 0.9846\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0325 - acc: 0.9843\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0320 - acc: 0.9848\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0321 - acc: 0.9845\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0310 - acc: 0.9850\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0313 - acc: 0.9849\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0307 - acc: 0.9851\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0313 - acc: 0.9850\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0307 - acc: 0.9852\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0307 - acc: 0.9852\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0303 - acc: 0.9855\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0300 - acc: 0.9853\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0306 - acc: 0.9854\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0295 - acc: 0.9858\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0294 - acc: 0.9856\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0310 - acc: 0.9852\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0294 - acc: 0.9858\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0303 - acc: 0.9853\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0283 - acc: 0.9864\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0296 - acc: 0.9857\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0301 - acc: 0.9858\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0285 - acc: 0.9863\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0291 - acc: 0.9863\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0294 - acc: 0.9863\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0279 - acc: 0.9866\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0290 - acc: 0.9862\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0285 - acc: 0.9864\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0290 - acc: 0.9863\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0280 - acc: 0.9865\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0283 - acc: 0.9865\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0279 - acc: 0.9868\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0284 - acc: 0.9865\n",
      "83153/83153 [==============================] - 1s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 13s 38us/step - loss: 0.3678 - acc: 0.8141\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1720 - acc: 0.9134\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1449 - acc: 0.9240\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1346 - acc: 0.9291\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1268 - acc: 0.9332\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1200 - acc: 0.9372\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1128 - acc: 0.9407\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1075 - acc: 0.9445\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.1024 - acc: 0.9478\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0984 - acc: 0.9504\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0924 - acc: 0.9537\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0878 - acc: 0.9564\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0854 - acc: 0.9578\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0819 - acc: 0.9595\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0788 - acc: 0.9612\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0753 - acc: 0.9632\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0730 - acc: 0.9645\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0710 - acc: 0.9654\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0682 - acc: 0.9668\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0649 - acc: 0.9682\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0642 - acc: 0.9688\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0619 - acc: 0.9700\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0604 - acc: 0.9710\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0591 - acc: 0.9718\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0575 - acc: 0.9726\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0559 - acc: 0.9735\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0549 - acc: 0.9737\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0527 - acc: 0.9746\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0530 - acc: 0.9743\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0505 - acc: 0.9758\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0504 - acc: 0.9760\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0485 - acc: 0.9768\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0481 - acc: 0.9771\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0472 - acc: 0.9775\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0465 - acc: 0.9781\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0456 - acc: 0.9785\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0447 - acc: 0.9786\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0441 - acc: 0.9790\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0435 - acc: 0.9792\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0427 - acc: 0.9797\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0426 - acc: 0.9796\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0420 - acc: 0.9801\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0407 - acc: 0.9806\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0407 - acc: 0.9806\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0396 - acc: 0.9811\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0396 - acc: 0.9809\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0394 - acc: 0.9811\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0386 - acc: 0.9816\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0375 - acc: 0.9821\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0380 - acc: 0.9818\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0370 - acc: 0.9822\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0369 - acc: 0.9822\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0367 - acc: 0.9823\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0361 - acc: 0.9827\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0362 - acc: 0.9825\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0359 - acc: 0.9830\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0346 - acc: 0.9830\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0343 - acc: 0.9835\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0344 - acc: 0.9834\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0343 - acc: 0.9836\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0342 - acc: 0.9836\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0337 - acc: 0.9837\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0327 - acc: 0.9842\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0337 - acc: 0.9840\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0330 - acc: 0.9841\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0333 - acc: 0.9840\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0326 - acc: 0.9843\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0328 - acc: 0.9843\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0325 - acc: 0.9843\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0322 - acc: 0.9846\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0314 - acc: 0.9847\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0317 - acc: 0.9849\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0323 - acc: 0.9845\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0314 - acc: 0.9848\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0311 - acc: 0.9853\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0303 - acc: 0.9854\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0307 - acc: 0.9853\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0312 - acc: 0.9849\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0309 - acc: 0.9851\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0306 - acc: 0.9855\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0305 - acc: 0.9853\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0303 - acc: 0.9856\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0302 - acc: 0.9855\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0297 - acc: 0.9860\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0303 - acc: 0.9856\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0289 - acc: 0.9860\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0304 - acc: 0.9856\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0295 - acc: 0.9860\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0295 - acc: 0.9858\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0287 - acc: 0.9864\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0300 - acc: 0.9857\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0288 - acc: 0.9862\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0284 - acc: 0.9864\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0294 - acc: 0.9859\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0288 - acc: 0.9863\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0291 - acc: 0.9860\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0280 - acc: 0.9866\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0285 - acc: 0.9863\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0289 - acc: 0.9863\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0282 - acc: 0.9865\n",
      "83153/83153 [==============================] - 1s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 13s 39us/step - loss: 0.3594 - acc: 0.8202\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1693 - acc: 0.9142\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1466 - acc: 0.9231\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1333 - acc: 0.9295\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1248 - acc: 0.9341\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1183 - acc: 0.9379\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.1112 - acc: 0.9422\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1064 - acc: 0.9448\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1012 - acc: 0.9478\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0965 - acc: 0.9506\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0921 - acc: 0.9531\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0875 - acc: 0.9555\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0841 - acc: 0.9578\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0805 - acc: 0.9603\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0774 - acc: 0.9616\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0732 - acc: 0.9637\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0713 - acc: 0.9646\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0689 - acc: 0.9663\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0665 - acc: 0.9676\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0640 - acc: 0.9684\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0618 - acc: 0.9698\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0606 - acc: 0.9706\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0593 - acc: 0.9713\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0574 - acc: 0.9722\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0558 - acc: 0.9727\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0543 - acc: 0.9735\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0536 - acc: 0.9737\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0519 - acc: 0.9747\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0517 - acc: 0.9752\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0505 - acc: 0.9757\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0488 - acc: 0.9761\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0478 - acc: 0.9769\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0476 - acc: 0.9770\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0472 - acc: 0.9772\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0454 - acc: 0.9781\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0451 - acc: 0.9782\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0433 - acc: 0.9791\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0437 - acc: 0.9790\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0423 - acc: 0.9793\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0430 - acc: 0.9794\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0421 - acc: 0.9795\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0410 - acc: 0.9802\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0407 - acc: 0.9802\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0403 - acc: 0.9805\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0390 - acc: 0.9811\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0390 - acc: 0.9810\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0395 - acc: 0.9809\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0385 - acc: 0.9813\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0385 - acc: 0.9816\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0379 - acc: 0.9816\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0377 - acc: 0.9815\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0366 - acc: 0.9822\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0372 - acc: 0.9818\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0355 - acc: 0.9825\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0365 - acc: 0.9823\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0357 - acc: 0.9824\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0351 - acc: 0.9826\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0355 - acc: 0.9827\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0349 - acc: 0.9828\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0350 - acc: 0.9830\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0344 - acc: 0.9832\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0342 - acc: 0.9832\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0328 - acc: 0.9839\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0342 - acc: 0.9835\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0339 - acc: 0.9834\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0330 - acc: 0.9840\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0331 - acc: 0.9839\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0329 - acc: 0.9838\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0322 - acc: 0.9841\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0329 - acc: 0.9840\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0321 - acc: 0.9844\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0318 - acc: 0.9843\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0326 - acc: 0.9840\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0316 - acc: 0.9845\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0324 - acc: 0.9842\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0317 - acc: 0.9844\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 11s 33us/step - loss: 0.0316 - acc: 0.9844\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0309 - acc: 0.9848\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0306 - acc: 0.9849\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0314 - acc: 0.9849\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0303 - acc: 0.9851\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0308 - acc: 0.9850\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0303 - acc: 0.9851\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0311 - acc: 0.9849\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0309 - acc: 0.9848\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0300 - acc: 0.9855\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0298 - acc: 0.9854\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0301 - acc: 0.9852\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0302 - acc: 0.9853\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0301 - acc: 0.9855\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0296 - acc: 0.9855\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0303 - acc: 0.9852\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0297 - acc: 0.9855\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0293 - acc: 0.9857\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0295 - acc: 0.9858\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0298 - acc: 0.9855\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0292 - acc: 0.9859\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 11s 34us/step - loss: 0.0285 - acc: 0.9862\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 11s 35us/step - loss: 0.0294 - acc: 0.9857\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0291 - acc: 0.9861\n",
      "83153/83153 [==============================] - 1s 12us/step\n",
      "Time elapsed (hh:mm:ss.ms) 1:34:52.579590\n",
      "Overall accuracy of Neural Network model: 0.9858478426618755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 94.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9861    0.9856    0.9858    207840\n",
      "           1     0.9856    0.9861    0.9859    207927\n",
      "\n",
      "    accuracy                         0.9858    415767\n",
      "   macro avg     0.9858    0.9858    0.9858    415767\n",
      "weighted avg     0.9858    0.9858    0.9858    415767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_sp_ann_2h_prob_unisoftsigbinlosadam,pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with no encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nodr_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=train_x,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 19:08:13 2019\n",
      "Train on 1064361 samples, validate on 266091 samples\n",
      "Epoch 1/200\n",
      "1064361/1064361 [==============================] - 437s 411us/step - loss: 0.1472 - acc: 0.9235 - val_loss: 0.0943 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09430, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0833 - acc: 0.9578 - val_loss: 0.0691 - val_acc: 0.9671\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09430 to 0.06907, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0607 - acc: 0.9698 - val_loss: 0.0525 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06907 to 0.05247, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0493 - acc: 0.9757 - val_loss: 0.0431 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05247 to 0.04307, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0430 - acc: 0.9787 - val_loss: 0.0430 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04307 to 0.04296, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0382 - acc: 0.9812 - val_loss: 0.0345 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04296 to 0.03448, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0349 - acc: 0.9828 - val_loss: 0.0328 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03448 to 0.03276, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1064361/1064361 [==============================] - 30s 29us/step - loss: 0.0326 - acc: 0.9839 - val_loss: 0.0320 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03276 to 0.03203, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0307 - acc: 0.9849 - val_loss: 0.0331 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03203\n",
      "Epoch 10/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0287 - acc: 0.9858 - val_loss: 0.0277 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03203 to 0.02765, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0278 - acc: 0.9862 - val_loss: 0.0305 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02765\n",
      "Epoch 12/200\n",
      "1064361/1064361 [==============================] - 30s 29us/step - loss: 0.0265 - acc: 0.9869 - val_loss: 0.0247 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02765 to 0.02474, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0251 - acc: 0.9878 - val_loss: 0.0248 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02474\n",
      "Epoch 14/200\n",
      "1064361/1064361 [==============================] - 53s 50us/step - loss: 0.0248 - acc: 0.9879 - val_loss: 0.0239 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02474 to 0.02391, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1064361/1064361 [==============================] - 32s 30us/step - loss: 0.0237 - acc: 0.9885 - val_loss: 0.0231 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02391 to 0.02308, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 16/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0233 - acc: 0.9886 - val_loss: 0.0205 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02308 to 0.02053, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0225 - acc: 0.9890 - val_loss: 0.0254 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02053\n",
      "Epoch 18/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0217 - acc: 0.9895 - val_loss: 0.0186 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02053 to 0.01858, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 19/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0212 - acc: 0.9897 - val_loss: 0.0181 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01858 to 0.01808, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0209 - acc: 0.9899 - val_loss: 0.0211 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01808\n",
      "Epoch 21/200\n",
      "1064361/1064361 [==============================] - 29s 28us/step - loss: 0.0204 - acc: 0.9902 - val_loss: 0.0195 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01808\n",
      "Epoch 22/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0196 - acc: 0.9907 - val_loss: 0.0167 - val_acc: 0.9921\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01808 to 0.01667, saving model to ./H5files/ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 23/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0193 - acc: 0.9909 - val_loss: 0.0178 - val_acc: 0.9918\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01667\n",
      "Epoch 24/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0189 - acc: 0.9910 - val_loss: 0.0182 - val_acc: 0.9919\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01667\n",
      "Epoch 25/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0184 - acc: 0.9913 - val_loss: 0.0176 - val_acc: 0.9921\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01667\n",
      "Epoch 26/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0180 - acc: 0.9915 - val_loss: 0.0169 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01667\n",
      "Epoch 27/200\n",
      "1064361/1064361 [==============================] - 30s 28us/step - loss: 0.0181 - acc: 0.9919 - val_loss: 0.0172 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01667\n",
      "Time elapsed (hh:mm:ss.ms) 0:20:50.994282\n"
     ]
    }
   ],
   "source": [
    "hist_nodr_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = nodr_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = train_x,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_nodr_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_nodr_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_nodr_ann_2h_unisoftsigbinlosadam, './Figures/nodr_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict(nodr_ann_2h_unisoftsigbinlosadam,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=train_x\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=test_x\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 19:29:05 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 19s 57us/step - loss: 0.2172 - acc: 0.8891\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.1257 - acc: 0.9325\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.1091 - acc: 0.9422\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0941 - acc: 0.9516\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0828 - acc: 0.9580\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0734 - acc: 0.9634\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0659 - acc: 0.9670\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0596 - acc: 0.9706\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0552 - acc: 0.9728\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0513 - acc: 0.9748\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0489 - acc: 0.9761\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0448 - acc: 0.9780\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0433 - acc: 0.9789\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0413 - acc: 0.9798\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0395 - acc: 0.9807\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0383 - acc: 0.9814\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0367 - acc: 0.9824\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0357 - acc: 0.9826\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0340 - acc: 0.9836\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0331 - acc: 0.9844\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0328 - acc: 0.9842\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0317 - acc: 0.9847\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0313 - acc: 0.9848\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0297 - acc: 0.9857\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0290 - acc: 0.9861\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0292 - acc: 0.9861\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0282 - acc: 0.9865\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0279 - acc: 0.9864\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0268 - acc: 0.9872\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0263 - acc: 0.9876\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0265 - acc: 0.9873\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0260 - acc: 0.9877\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0246 - acc: 0.9884\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0252 - acc: 0.9882\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0247 - acc: 0.9883\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0242 - acc: 0.9888\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0236 - acc: 0.9890\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0235 - acc: 0.9890\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0230 - acc: 0.9893\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0230 - acc: 0.9897\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0236 - acc: 0.9895\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0228 - acc: 0.9896\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0226 - acc: 0.9897\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0224 - acc: 0.9899\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0219 - acc: 0.9898\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0219 - acc: 0.9899\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0218 - acc: 0.9899\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0206 - acc: 0.9906\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0217 - acc: 0.9899\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0212 - acc: 0.9905\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0208 - acc: 0.9905\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0204 - acc: 0.9906\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0208 - acc: 0.9904\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0202 - acc: 0.9907\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0190 - acc: 0.9912\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0198 - acc: 0.9909\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0194 - acc: 0.9911\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0196 - acc: 0.9910\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0189 - acc: 0.9915\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0191 - acc: 0.9913\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0193 - acc: 0.9910\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0195 - acc: 0.9910\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0182 - acc: 0.9917\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0190 - acc: 0.9914\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0186 - acc: 0.9916\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0189 - acc: 0.9913\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0184 - acc: 0.9916\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0182 - acc: 0.9916\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0181 - acc: 0.9918\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0184 - acc: 0.9916\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0186 - acc: 0.9914\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0177 - acc: 0.9919\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0180 - acc: 0.9918\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0182 - acc: 0.9917\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0184 - acc: 0.9918\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0180 - acc: 0.9920\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0179 - acc: 0.9919\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0174 - acc: 0.9922\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 11s 34us/step - loss: 0.0178 - acc: 0.9919\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0177 - acc: 0.9920\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0169 - acc: 0.9923\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0176 - acc: 0.9922\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0175 - acc: 0.9921\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0175 - acc: 0.9922\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0169 - acc: 0.9923\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 11s 35us/step - loss: 0.0177 - acc: 0.9920\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0174 - acc: 0.9921\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0172 - acc: 0.9923\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0165 - acc: 0.9925\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0174 - acc: 0.9921\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0167 - acc: 0.9925\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0170 - acc: 0.9923\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0167 - acc: 0.9926\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0161 - acc: 0.9927\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0172 - acc: 0.9922\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0166 - acc: 0.9927\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0164 - acc: 0.9927\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0171 - acc: 0.9924\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0163 - acc: 0.9926\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0165 - acc: 0.9926\n",
      "83154/83154 [==============================] - 1s 15us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_59 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 15s 45us/step - loss: 0.2145 - acc: 0.8908\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.1243 - acc: 0.9328\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.1077 - acc: 0.9434\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0917 - acc: 0.9527\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0799 - acc: 0.9594\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0723 - acc: 0.9636\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0653 - acc: 0.9677\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0597 - acc: 0.9706\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0550 - acc: 0.9729\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0513 - acc: 0.9750\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0481 - acc: 0.9765\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0450 - acc: 0.9785\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0424 - acc: 0.9794\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0414 - acc: 0.9800\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0392 - acc: 0.9810\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0382 - acc: 0.9816\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0360 - acc: 0.9827\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0361 - acc: 0.9827\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0340 - acc: 0.9837\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0329 - acc: 0.9841\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0329 - acc: 0.9843\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0313 - acc: 0.9850\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0305 - acc: 0.9854\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0304 - acc: 0.9853\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0292 - acc: 0.9860\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0290 - acc: 0.9860\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0283 - acc: 0.9866\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0281 - acc: 0.9865\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0277 - acc: 0.9870\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0264 - acc: 0.9876\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0260 - acc: 0.9875\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0258 - acc: 0.9877\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0254 - acc: 0.9879\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0250 - acc: 0.9881\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0241 - acc: 0.9883\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0249 - acc: 0.9881\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0237 - acc: 0.9889\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0238 - acc: 0.9888\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0231 - acc: 0.9889\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0228 - acc: 0.9890\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0227 - acc: 0.9891\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0232 - acc: 0.9891\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0221 - acc: 0.9896\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0218 - acc: 0.9896\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0217 - acc: 0.9896\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0218 - acc: 0.9897\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0212 - acc: 0.9899\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0211 - acc: 0.9898\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0211 - acc: 0.9902\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0210 - acc: 0.9901\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0210 - acc: 0.9903\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0201 - acc: 0.9906\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0208 - acc: 0.9902\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0196 - acc: 0.9906\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0201 - acc: 0.9905\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0206 - acc: 0.9905\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0203 - acc: 0.9904\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0197 - acc: 0.9907\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0195 - acc: 0.9910\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0197 - acc: 0.9907\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0195 - acc: 0.9909\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0188 - acc: 0.9914\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0184 - acc: 0.9916\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0190 - acc: 0.9911\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0190 - acc: 0.9912\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0187 - acc: 0.9913\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0188 - acc: 0.9913\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0187 - acc: 0.9914\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0188 - acc: 0.9912\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0184 - acc: 0.9914\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0184 - acc: 0.9917\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0183 - acc: 0.9914\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0180 - acc: 0.9915\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0183 - acc: 0.9915\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0178 - acc: 0.9917\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0187 - acc: 0.9915\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0182 - acc: 0.9917\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0179 - acc: 0.9916\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0178 - acc: 0.9920\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0175 - acc: 0.9921\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 12s 37us/step - loss: 0.0177 - acc: 0.9919\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0177 - acc: 0.9918\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0172 - acc: 0.9920\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0177 - acc: 0.9919\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0171 - acc: 0.9922\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0172 - acc: 0.9922\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0170 - acc: 0.9921\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0176 - acc: 0.9921\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0174 - acc: 0.9921\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0168 - acc: 0.9923\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0172 - acc: 0.9921\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0163 - acc: 0.9926\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0170 - acc: 0.9922\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0166 - acc: 0.9926\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0166 - acc: 0.9926\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0168 - acc: 0.9924\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 12s 36us/step - loss: 0.0165 - acc: 0.9926\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0168 - acc: 0.9923\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0162 - acc: 0.9927\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 12s 35us/step - loss: 0.0166 - acc: 0.9924\n",
      "83154/83154 [==============================] - 1s 15us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 14s 41us/step - loss: 0.2154 - acc: 0.8903\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1252 - acc: 0.9327\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.1075 - acc: 0.9424\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0931 - acc: 0.9519\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0822 - acc: 0.9582\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0738 - acc: 0.9630\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0664 - acc: 0.9668\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0608 - acc: 0.9699\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0570 - acc: 0.9717\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0524 - acc: 0.9741\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0496 - acc: 0.9757\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0464 - acc: 0.9774\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0445 - acc: 0.9781\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0430 - acc: 0.9788\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0403 - acc: 0.9802\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0393 - acc: 0.9807\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0378 - acc: 0.9814\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0366 - acc: 0.9823\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0368 - acc: 0.9820\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0342 - acc: 0.9834\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0348 - acc: 0.9830\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0333 - acc: 0.9837\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0315 - acc: 0.9849\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0318 - acc: 0.9847\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0309 - acc: 0.9849\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0299 - acc: 0.9855\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0290 - acc: 0.9861\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0288 - acc: 0.9861\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0286 - acc: 0.9863\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0274 - acc: 0.9869\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0272 - acc: 0.9869\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0266 - acc: 0.9874\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0263 - acc: 0.9872\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0257 - acc: 0.9878\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0255 - acc: 0.9878\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0247 - acc: 0.9883\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0252 - acc: 0.9882\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0249 - acc: 0.9883\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0240 - acc: 0.9885\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0233 - acc: 0.9889\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0234 - acc: 0.9889\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0233 - acc: 0.9890\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0226 - acc: 0.9892\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0226 - acc: 0.9893\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0223 - acc: 0.9895\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0223 - acc: 0.9895\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0229 - acc: 0.9894\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0216 - acc: 0.9898\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0214 - acc: 0.9899\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0208 - acc: 0.9902\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0213 - acc: 0.9899\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0209 - acc: 0.9902\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0205 - acc: 0.9905\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0210 - acc: 0.9904\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0213 - acc: 0.9902\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0197 - acc: 0.9906\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0205 - acc: 0.9904\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0196 - acc: 0.9911\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0201 - acc: 0.9907\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0198 - acc: 0.9909\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0201 - acc: 0.9908\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0197 - acc: 0.9907\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0193 - acc: 0.9912\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0199 - acc: 0.9909\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0190 - acc: 0.9912\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0196 - acc: 0.9910\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0195 - acc: 0.9910\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0190 - acc: 0.9911\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0195 - acc: 0.9911\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0185 - acc: 0.9915\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0188 - acc: 0.9914\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0187 - acc: 0.9916\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0187 - acc: 0.9915\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0186 - acc: 0.9914\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0181 - acc: 0.9917\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0183 - acc: 0.9916\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0187 - acc: 0.9914\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0180 - acc: 0.9917\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0177 - acc: 0.9918\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0182 - acc: 0.9918\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0180 - acc: 0.9917\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0181 - acc: 0.9918\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0173 - acc: 0.9923\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0183 - acc: 0.9917\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0176 - acc: 0.9920\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0182 - acc: 0.9916\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0176 - acc: 0.9920\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0176 - acc: 0.9920\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0171 - acc: 0.9922\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0172 - acc: 0.9922\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0168 - acc: 0.9925\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0177 - acc: 0.9921\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0182 - acc: 0.9919\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0172 - acc: 0.9922\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0160 - acc: 0.9928\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0177 - acc: 0.9920\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0176 - acc: 0.9921\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0168 - acc: 0.9924\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0170 - acc: 0.9923\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0166 - acc: 0.9925\n",
      "83153/83153 [==============================] - 1s 17us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 14s 41us/step - loss: 0.2145 - acc: 0.8908\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.1231 - acc: 0.9348\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.1059 - acc: 0.9449\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0911 - acc: 0.9532\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0795 - acc: 0.9603\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0698 - acc: 0.9657\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0636 - acc: 0.9689\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0569 - acc: 0.9721\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0538 - acc: 0.9739\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0500 - acc: 0.9755\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0469 - acc: 0.9774\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0448 - acc: 0.9784\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0425 - acc: 0.9798\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0410 - acc: 0.9802\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0389 - acc: 0.9812\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0383 - acc: 0.9816\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0369 - acc: 0.9824\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0352 - acc: 0.9832\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0348 - acc: 0.9835\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0345 - acc: 0.9837\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0327 - acc: 0.9843\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0318 - acc: 0.9847\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0311 - acc: 0.9853\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0307 - acc: 0.9853\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0297 - acc: 0.9857\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0294 - acc: 0.9860\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0287 - acc: 0.9863\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0286 - acc: 0.9865\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0277 - acc: 0.9866\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0274 - acc: 0.9871\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0272 - acc: 0.9869\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0263 - acc: 0.9876\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0259 - acc: 0.9875\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0264 - acc: 0.9875\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0250 - acc: 0.9881\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0250 - acc: 0.9882\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0242 - acc: 0.9886\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0246 - acc: 0.9883\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0239 - acc: 0.9889\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0233 - acc: 0.9888\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0242 - acc: 0.9885\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0226 - acc: 0.9893\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0230 - acc: 0.9889\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0226 - acc: 0.9892\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0222 - acc: 0.9894\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0227 - acc: 0.9894\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0211 - acc: 0.9901\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0218 - acc: 0.9898\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0223 - acc: 0.9893\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0211 - acc: 0.9899\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0211 - acc: 0.9899\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0211 - acc: 0.9901\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0207 - acc: 0.9902\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0203 - acc: 0.9903\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0211 - acc: 0.9902\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0206 - acc: 0.9903\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0200 - acc: 0.9906\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0197 - acc: 0.9906\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0204 - acc: 0.9904\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0196 - acc: 0.9907\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0190 - acc: 0.9911\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0197 - acc: 0.9908\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 12s 35us/step - loss: 0.0201 - acc: 0.9905\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0203 - acc: 0.9907\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0186 - acc: 0.9912\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0198 - acc: 0.9908\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0187 - acc: 0.9912\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0191 - acc: 0.9910\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0187 - acc: 0.9914\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0188 - acc: 0.9913\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0191 - acc: 0.9911\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0185 - acc: 0.9914\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0195 - acc: 0.9910\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0193 - acc: 0.9914\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0192 - acc: 0.9911\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0186 - acc: 0.9914\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0177 - acc: 0.9915\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0189 - acc: 0.9912\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0176 - acc: 0.9919\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0182 - acc: 0.9917\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0183 - acc: 0.9916\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0179 - acc: 0.9918\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0184 - acc: 0.9918\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0186 - acc: 0.9917\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0179 - acc: 0.9916\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0179 - acc: 0.9919\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0185 - acc: 0.9915\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0182 - acc: 0.9917\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0171 - acc: 0.9921\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0179 - acc: 0.9916\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0170 - acc: 0.9921\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0178 - acc: 0.9916\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0176 - acc: 0.9920\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0179 - acc: 0.9917\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0168 - acc: 0.9923\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0170 - acc: 0.9923\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0175 - acc: 0.9919\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0169 - acc: 0.9922\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0177 - acc: 0.9919\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0166 - acc: 0.9923\n",
      "83153/83153 [==============================] - 1s 16us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 210)               44310     \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 94,869\n",
      "Trainable params: 94,659\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 14s 42us/step - loss: 0.2114 - acc: 0.8915\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.1249 - acc: 0.9333\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.1098 - acc: 0.9416\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0953 - acc: 0.9506\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0832 - acc: 0.9576\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0738 - acc: 0.9626\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0684 - acc: 0.9660\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0621 - acc: 0.9692\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0590 - acc: 0.9706\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0539 - acc: 0.9734\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0508 - acc: 0.9752\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0483 - acc: 0.9765\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0459 - acc: 0.9777\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0447 - acc: 0.9782\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0423 - acc: 0.9796\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0403 - acc: 0.9804\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 12s 38us/step - loss: 0.0395 - acc: 0.9808\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0382 - acc: 0.9815\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0370 - acc: 0.9820\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0362 - acc: 0.9824\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0344 - acc: 0.9834\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0347 - acc: 0.9834\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0334 - acc: 0.9838\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0322 - acc: 0.9844\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0319 - acc: 0.9846\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0309 - acc: 0.9849\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0308 - acc: 0.9850\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0300 - acc: 0.9856\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0294 - acc: 0.9857\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0288 - acc: 0.9860\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0280 - acc: 0.9865\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0285 - acc: 0.9863\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 12s 38us/step - loss: 0.0277 - acc: 0.9867\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0276 - acc: 0.9868\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0262 - acc: 0.9873\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0267 - acc: 0.9873\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0251 - acc: 0.9880\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0264 - acc: 0.9874\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0257 - acc: 0.9878\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0248 - acc: 0.9883\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 12s 38us/step - loss: 0.0248 - acc: 0.9884\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0258 - acc: 0.9882\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0246 - acc: 0.9885\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0246 - acc: 0.9883\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0244 - acc: 0.9885\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0233 - acc: 0.9889\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0232 - acc: 0.9892\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0237 - acc: 0.9891\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 13s 38us/step - loss: 0.0231 - acc: 0.9891\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0228 - acc: 0.9894\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0223 - acc: 0.9894\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0228 - acc: 0.9892\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0220 - acc: 0.9896\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0221 - acc: 0.9894\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0222 - acc: 0.9895\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0219 - acc: 0.9898\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0213 - acc: 0.9896\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0215 - acc: 0.9900\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0209 - acc: 0.9902\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0215 - acc: 0.9899\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0209 - acc: 0.9905\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0211 - acc: 0.9902\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0210 - acc: 0.9901\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0202 - acc: 0.9905\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0214 - acc: 0.9900\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0207 - acc: 0.9904\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0208 - acc: 0.9904\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0200 - acc: 0.9906\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0199 - acc: 0.9906\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0207 - acc: 0.9905\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0194 - acc: 0.9909\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0200 - acc: 0.9906\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0205 - acc: 0.9906\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0199 - acc: 0.9910\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0194 - acc: 0.9908\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0192 - acc: 0.9912\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0202 - acc: 0.9910\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0186 - acc: 0.9915\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 13s 38us/step - loss: 0.0199 - acc: 0.9908\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 13s 39us/step - loss: 0.0191 - acc: 0.9912\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 13s 38us/step - loss: 0.0187 - acc: 0.9913\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0196 - acc: 0.9909\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0189 - acc: 0.9911\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0185 - acc: 0.9915\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 13s 38us/step - loss: 0.0185 - acc: 0.9915\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0186 - acc: 0.9914\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 13s 39us/step - loss: 0.0187 - acc: 0.9915\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 13s 38us/step - loss: 0.0185 - acc: 0.9916\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0186 - acc: 0.9917\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 13s 38us/step - loss: 0.0187 - acc: 0.9917\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0193 - acc: 0.9915\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 13s 39us/step - loss: 0.0191 - acc: 0.9914\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0195 - acc: 0.9915\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 13s 39us/step - loss: 0.0184 - acc: 0.9917\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 13s 38us/step - loss: 0.0181 - acc: 0.9918\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0176 - acc: 0.9919\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0184 - acc: 0.9916\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 12s 36us/step - loss: 0.0182 - acc: 0.9918\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0179 - acc: 0.9920\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 12s 37us/step - loss: 0.0177 - acc: 0.9920\n",
      "83153/83153 [==============================] - 1s 17us/step\n",
      "Time elapsed (hh:mm:ss.ms) 1:43:19.684747\n",
      "Overall accuracy of Neural Network model: 0.9911296471340919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 103.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9930    0.9893    0.9911    207840\n",
      "           1     0.9893    0.9930    0.9911    207927\n",
      "\n",
      "    accuracy                         0.9911    415767\n",
      "   macro avg     0.9911    0.9911    0.9911    415767\n",
      "weighted avg     0.9911    0.9911    0.9911    415767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5xV8/rA8c9MpChKOUo56ZDHRJQiuRPRhaKiEqWbEAe5dZyDkMtBLsetn3LnIPcSuaZQyeiiGg+VLpNKdSpKU02zf3981252Y2bvPdPsvfba+3m/Xr2avWettZ+9Zu/vs77f71rPygqFQhhjjDFlyfY7AGOMManNEoUxxpioLFEYY4yJyhKFMcaYqCxRGGOMicoShTHGmKgsUaQ5EblIRD7yO45UIiIbReRvPrzuQSISEpHdkv3aiSAi80Tk1AqsV+HPpIi0E5F3KrJuRYnIHiLyg4j8JZmvm0qy7DqK5BGRxcD+wHZgI/AhMERVN/oYVqUSkeOBu4BjgCJgMnCTqs73KZ5JwEuqOjpJr3coMAI4DdgdWAI8BzwCHAj8DOyuqoXJiKcsIhICmqjqggS/zkFU4nsWkW9x35lp3uMQ8AcQAjYArwE3qOr2iHU6AbcChwMFuO/dTaqaH7FMfdzntgNQA1jubevfqrpJRG4E9lfVobv6HoLIehTJd46q1gCaAy2AYT7HUyGlHRWLSBvgI+Bd4ACgMTAb+CoRR/CpdmQuIgcD04FlQDNV3QfoDrQCalbya/n23v16bRE5BtgnnCQiHOV9p04BLgT6RazTDXgFl6jr4pLFFuBLEantLbMvMBWoDrRR1ZrAmUAt4GBvU68AfURkjwS9vZSWUl+0TKKqK0VkIi5hAK6LizsavQDYA3gbuFZVN3u/7wwMB/4GrAauVNUPRWQfYCTuaKgIeBa4TVW3i0hfYICqnigiTwEbVfX6iNd8F/hCVUeKyAHAf4CTcT2eh1T1UW+524EjcEdk5wLXASWP0v8NvKCqj0Q8908RaQncDlziDVW8BDzhbWMjcIuqvhxrH0Ss+x/gWuBjEbkaeBFojfs8fwUMVtV8ERkBnAQcJyIPA8+p6pDIo2kReQ7YBBzkve/5QC9VXejF0857vXrAy7iG5sUyeijDga9V9brwE6qqQC9vW7W8py8SkTuBPb19PML7/bG4Bi0H2Ay8CVynqlu934eAIcA13nttLCKPAOcD+wA/Adeo6hRv+SrATUB/4C/Aj0AX730AzPa22V9VX/OOvO/y9sV8bz/O8ba1GHgSuMg9lL2ABbjP1ide7E8Ah3qxv+zth8nea60XEXANsHjrneht+3DgYaAlsA14RFXvLmX/tge+KOX58L5eICJf4X2nRCQLeBC4K/z5AjaLyABgDu4zdCvuc/g70FtVi7xtLQP+HrHtfBFZBxwXLYZ0ZT0Kn4hIQ9wHP7Lrfx/ui9YcOARogPsghxuRF4AbcEc6JwOLvfWeBwq9dVoA7YABpbzsK8CF3hcI74iqHfCqiGQD43A9gAZAW+AaETkrYv3OwBve678cuWER2RM4Hhhbyuu+jmsgwurhju4aAH2A/xOvFYm2DyLW3RdoBAzCfYaf9R7/FddIPQagqrcAU3BDFTVUdUgpsQH0xDXytXF/j3DDXdd7v8OAOoB677EsZ3jLx3IirrFsC9wqIjne89txjVddoI33+ytKrNsFlxSbeo9n4PbVvri/71gRqeb97jrvvXUA9sYdaf+hqid7vz/K2y+vicjRwDPAZd57HQW8V+IIuifQEahVyjDSI7gGfm/cUfjr3vPh16rlvdbUyJVEpCbwCW446ADc3/zTUvcaNMP9DUolIofhDgzC3ynBfSZ2+kx6yeBNij+TZwBvhZNEFHnAUTGWSUvWo0i+d7yjuBrAZ8BtsOPoZyBwpKr+z3vubtyXfxjuqPAZVf3Y285yb5n9cQmnltfz2CQiD+Ea0VElXnsKbiz3JNyRXjdgqqr+IiKtgf1U9Q5v2UUi8jTQA5joPTdVVcMTiZtLbHtfXKO9opT3vALX+EX6l6puAb4QkfeBC0Tkrhj7AFyP6TZv3XAcb4Y36vUiPi8lhmjeUtVvvPVfxvXOwDWw81T1Le93jwLXl74JwDWwpb3/koZ7f6vZIjIb1/jkqWpuxDKLRWQUbjjl4Yjn7wnvGwBVfSnidw+KyD9xDeRs3MHCjV6vBu+5sgwERqnqdO/x8yLyD3Y+gn7UO9IuzTbgEBGpq6prgJLDQ2XpBKxU1Qe9xwW44bvS1MId+Zf0ndd72hN4FdezgeLPXKzPZLx/t9+9GDKOJYrk6+J11U/BNYB1gfXAfrgPem7xwTVZQBXv5wOBCaVsrxFu0nRFxHrZuHHynahqSERexR0ZTsYNibwUsZ0DRGR9xCpVcMklrKxGAmAdrhGvD/xQ4nf1gTWRy6rqpojHS3BHk7H2AcBqVS0IP/B6Mg8BZ+N6BAA1RaRK5IRmDCsjfv4Dl8TxYtrxnr39l0/Z1uLea4Vez5sIH4mb09gT9/3MLbHuTn8DERmKSwgH4A4C9qa4ATwQWBhHPOD+/n1E5KqI56p62y31tUvoD9wB/CAiP+OS4fg4Xrc8Ma6j9Lmeo71tdAfuBfbCzUOEP3P1cRPqkSI/k/H+3WrivqsZxxKFT1T1C298/AHccMIa3NHx4aq6vJRVllE8sVby+S1A3TjPKvkv8JGI3IsbwjgvYjs/q2qTKOuWeYqcd2bIVNyXteQR/QXsPJxQW0T2ikgWfwXmEnsflBbDUNwRdGtv3qc5MBOXYKLGHIcVQMPwA6/X17DsxfkE6IobCquIJ3Gx91TV30XkGlyvL9KO9yMiJ+HmINriej5F3jh6+L2HPzNz43jtZcCI8HxJGaL9/X8CenpDmOcDb4hInWjrRLxuzzjiAzevcGgZrx8CXvfm8W7FzeMokI/7TP47vKwXY1cg3Dv+BDhPRIbHGH7Kwc15ZBxLFP56GDfE0FxVZ3lDPQ+JyBBV/VVEGgBHqOpEYAyugR+Pa4jrAzVV9Qdx56Q/KCL/wk0ONwYaquqfJt1UdaaIrMZNRE9U1fAR0jfAbyJyE/AosBX3xaiuqjPifD83AxNF5AdcY7kbriFvgztdNtJwb2ijNW744TavoYu2D0pTE5dc1ntnr9xW4vercJP/FfE+8JiIdAHGA4NxcyRluQ2YISL3Aw96iesQ3ER+WfMjkWoCvwEbvfH2y3EnLURbvtBbZjcRuRnXowgbDdwpIvNx4/bNgOWqupbi/RIez38aeFtEPsF9FvYETgUmq2ppwz07EZHeuM/T6ohe6XYvtiLvtX4sZdXxwEgvKT6J68U0jRgCizQBN7QUzb3AdBG519v/1wNPez3Bt3GT/nfj9tND3jojgd644bZ/quoS73M3FHcCxBzv8b7EP6SWVmwy20equho3Qf0v76mbcF/caSLyG+5IR7xlvwEuxX24N+DGjRt5612C+4LNx3XP3yB6V/q/uAm8VyJi2Q6cg5sY/Rl3dD8a98WK9/18CZyFO6JcgRtSagGc6B1xhq304vwFNyk+WFXDw1Vl7oMyPIw7rTE8Lv5hid8/AnQTkXXeHEPcvLH28NHoWtwE8re4Hlxpyy/EJcWDgHkisgE3f/ItpY+tl3Q9bjjwd1zD/VqM5ScCH+Aa4CW48f3I4aGRuEnlj3AJaAxuX4FLXs+LyHoRuUBVv8XNUzyG+9ssAPrGEXPY2bj3vBG3z3uoaoGq/oE7OeAr77WOi1zJS0Jn4j57K3Fnbp1W2guo6nfABm8+rVSq+j3uu3GD9/g14GLcSQJrcN+R6sAJXsLEm/M5HjfPMl1Efsf1gDdQnEh7Ac9HzI1lFLvgziSVeKe4qmq0IZyU5A1Z5AMXqWp5J8xNJfBOV75CVbsk8TX3wJ0IcLKq/pqs100lNvRkTBTe6cHTccNbN+DG/zNy+CEVqOpHuB5SMl9zC3BYMl8z1SQsUYjIM7ix519V9YhSfp+F66J2wJ350dfrWhqTStrghujCQ3tdvFNbjckYCRt6EpHw1b0vlJEoOgBX4RJFa9zFOmWOPRpjjPFHwiazVXUy8L8oi3TGJZGQutottcQV5jLGGJNC/JyjaMDOZ2jke89FvUIyNzc3lJ3t38laJTtgZXXI4lku9nNZUZcLhUJkZWXt9Fw8r1He16m857IqdXuRj0MhyMqKfK78+2VX/kaJfx33WuXdL9Gei75sVpzL7epz0fef2XWNWEIt1rOtaZM1LVu23K8i2/AzUWSV8lzMj0p2djYtWrRg2zZo2xaWLg03mu5fUdHOj6M9X55lw/9McGVluX/Z2cU/R3uuvM+nyrJ//LGRGjVqpGRsiVg22jZWrVpJ/fr1AvE+KvUzSMj9n51FzZcmUeV/v7KgaZMlFf3u+Jko8nGX74c1xJ1XH5e1a2HKFGjdGkRS44+X7MZmxYp8GjZsGPj3URnL/vjjD+TkHFbmspkkL28ZOTk5sRfMAHl568jJiXaNZBpavhwuvxwuvBAuughuudw9n1uyGkz8/EwU7wFDvNpDrYENqhpPYS4Atm1z/w8Y4P5lory837H2wNljjxB7ZOSdAozxhEIwejRcf71rIDt2rLRNJ/L02P/iSgDU9S6fvw1XvA5VfQp3OX4H3JWPf+CuOo5boVfVaDe7EsQYk+kWLoSBA+Hzz+G00+Dpp+Hg0krDVUzCmllVjVroyyvidWVFtx9OFLvvXtEtGGNMmvj+eze09H//54ZYKnm8NbDH49ajMMZktLlz4bvv4JJLoEsXWLQI6tRJyEsFtihgeI7CEoUxJqNs3Qq33w5HHw233AIF3u1ZEpQkIMCJwnoUxpiMM326SxDDh7uzmmbOhGrVYq+3iwLbzNochTEmoyxfDiedBPvvD+PHV+pZTbFYj8IYY1LZj979nho0gNdeg3nzkpokIMCJwuYojDFpbf16GDQIDjsMJk92z513Huy9d/T1EiCwzaz1KIwxaeu999zV1StXwg03wDEl7yScXIFtZm2OwhiTlgYMgDFjoFkzePddaNXK74iCnyisR2GMCbxwxdGsLJcYGjWCm26CqlX9jcsT2GbW5iiMMWlh2TIYPBh69ICLL3Y/p5jATmZbj8IYE2hFRfDkk3D44TBpEmzZ4ndEZQpsM2tzFMaYwPrpJzcXMXkynHGGq9HUuLHfUZUp8InCehTGmMCZPx/mzIFnnoG+fVP+pimBbWZtjsIYEyizZ8OsWdCnD3Tu7Ir41a7td1RxCfwchQ09GWNS2pYt8K9/ubOZ/vWv4iJ+AUkSkAaJwnoUxpiUNXUqtGgBd90FvXolrYhfZQtsM2tDT8aYlLZ8OZxyCtSrBxMmQPv2fkdUYdajMMaYypSX5/5v0ABef90V8QtwkoA0SBQ2R2GMSQnr1kG/ftC0KUyZ4p7r0gVq1vQ3rkoQ2ONx61EYY1LG22/DFVfA6tUwbJjvRfwqW2CbWZujMMakhH794NlnoXlzeP99dwe6NBPYZraw0F2jkh3YwTNjTGBFFvE77jho0gSuvz5tx8IDnSjS9G9ijEllS5bAZZe5010vucTdXCjNBfZ4vLDQhp2MMUlUVASPPw5HHAFfflk8/p0BAtvUbttmicIYkySqrojfl19Cu3YwahQcdJDfUSVNYJta61EYY5JG1V0P8dxzbrgpxYv4VbbANrU2R2GMSaiZM10Rv0svhXPPdUX8atXyOypf2ByFMcZEKiiAf/zDXQtx++3FRfwyNElAgBOFzVEYYyrdV1+56yHuuccNMc2aFcgifpUtsE2t9SiMMZVq+XI47TRXo2niRDdpbYAA9yhsjsIYUynmz3f/N2gAb74J339vSaKEQCcK61EYYyrsf/9ztyE9/HB372qAc86BGjV8DSsVBbaptTkKY0yFvfkmXHklrF0Lt9wCxx7rd0QpLbBNrfUojDEV0rcvPP+8K9734Ydu8tpEFdim1uYojDFxiyzid/zxkJMDQ4fa0WacErqXRORs4BGgCjBaVe8t8fu/As8DtbxlblbVCfFs23oUxpi4/PyzK9zXuzf06ZMRRfwqW8Ims0WkCvA40B5oCvQUkaYlFvsn8LqqtgB6AE/Eu32bozDGRLV9O7VffNEV8Zs2rbhXYcotkWc9HQssUNVFqroVeBXoXGKZELC39/M+wC/xbtyGnowxZcrLg5NOot4998App7g6TX37+h1VYCXymLwBsCzicT7QusQytwMfichVwF7AGbE2WlRURF5eHhs3HkTVqtvJy1sWa5W0VVBQQF74Ru4ZzvZFMdsXUOPzz6k/fz75d97J5vPPh02bXPIwFZLIRFFaecWSfb+ewHOq+qCItAFeFJEjVLWorI1mZ2eTk5PDbru50is5OTmVGXOg5OXlZfT7j2T7oljG7ovcXJg9292aNCcHevdm8/LlmbkvSpGbm1vhdRM59JQPHBjxuCF/HlrqD7wOoKpTgWpA3Xg2bnMUxhgANm+Gm2+G1q3hzjuLi/jtvXf09UzcEpkoZgBNRKSxiFTFTVa/V2KZpUBbABHJwSWK1fFs3OYojDFMngxHHQX33efmIGbOtCJ+CZCwRKGqhcAQYCKQhzu7aZ6I3CEi53qLDQUGishs4L9AX1WN69QEOz3WmAy3fDm0besag08+gdGjM7oUeCIltKn1romYUOK5WyN+ng+cUJFt29CTMRnq+++hWTNXxO/tt13F17328juqtGZFAY0xwbBmDVx8MRx5ZHERv06dLEkkQWCbWpujMCZDhEIwdiwMGQLr1sFtt7mJa5M0gU4U1qMwJgP06QMvvgitWsGnn7phJ5NUgW1qbY7CmDQWWcTvlFPccNM119iX3ic2R2GMSS2LFsEZZ8Bzz7nH/fvD9dfbF95HgU4UNkdhTBrZvh0eftgNLc2YAdmBbZ7STmBTtPUojEkj8+e70hvTp0PHjvDUU9Cwod9RGU8gm9rt290QpiUKY9LEzz/DwoXwyivQo4ebmzApI5BNbWGh+98ShTEBNmMGzJoFAwe6XsSiRVCzpt9RmVIEchAwnChsjsKYAPrjDzc5fdxxcM89xUX8LEmkrEAnCutRGBMwkya5U10ffND1JKyIXyAEsqndts39b4nCmADJz4czz4RGjeCzz1yNJhMI1qMwxiTW7Nnu/4YN4d13Yc4cSxIBE+hEYXMUxqSw1auhVy9o3hy++MI916ED7Lmnv3GZcgvkMbn1KIxJYaEQvPoqXH01bNgAw4dDmzZ+R2V2QSCbWpujMCaFXXwxvPyyq/A6ZgwcfrjfEZldFMim1oaejEkxRUXuIrmsLDf/0LKl61FUqeJ3ZKYSBHqOwnoUxqSABQvcLUmffdY97t8frr3WkkQasURhjKmYwkJ44AFXxG/mTKha1e+ITIIEsqm1OQpjfDZ3Llx6KXz7LXTuDE88AQcc4HdUJkEC2dTaHIUxPlu6FJYscWc3XXCBFfFLc4FOFNajMCaJpk93F88NGuSuh1i0CGrU8DsqkwSBnKOwoSdjkmjTJrjuOnctxL//DVu2uOctSWSMQCYK61EYkySffeaK+D30EAweDN99B3vs4XdUJskC2dTaHIUxSZCfD2edBY0buxIcJ5/sd0TGJ9ajMMbsbOZM93/DhjBunJuXsCSR0QKZKGyOwpgEWLUKLrwQjj66uIjf2WdD9er+xmV8F8hEYT0KYypRKAQvvQRNm8I778Bdd8Hxx/sdlUkhgWxqbY7CmErUq5e7HqJNG1fELyfH74hMigl0orAehTEVFFnEr107lySuvNLqM5lSBXLoyeYojNkFP/7oKrw+84x7fOmlVunVRBXIRGE9CmMqoLDQXTB31FHudqQ2SW3iFMim1uYojCmnOXOgXz/IzYXzzoPHH4f69f2OygREoBOF9SiMiVN+PixbBmPHQteuVsTPlEtCm1oRORt4BKgCjFbVe0tZ5gLgdiAEzFbVXrG2a3MUxsTh669dT2Lw4OIifnvt5XdUJoASNkchIlWAx4H2QFOgp4g0LbFME2AYcIKqHg5cE8+2bejJmLJlbdoEf/87nHgiPPhgcRE/SxKmghI5mX0ssEBVF6nqVuBVoHOJZQYCj6vqOgBV/TWeDdvQkzFl+Ogj/ta5M/znP+50VyviZypBIpvaBsCyiMf5QOsSyxwKICJf4YanblfVD6NttKioiJUrV5OVVRfVHyoz3sApKCggLy/P7zBSgu0L2G3FCg7p2JGihg1Z/MILbG7Z0s1NZDD7XFSORCaK0mbLQqW8fhPgVKAhMEVEjlDV9WVtNDs7m3322Y/ddoOcDL+CNC8vL+P3QVhG74vcXGjZ0l1RPWECi/fbj8OaN/c7qpSQ0Z+LEnJzcyu8biKHnvKBAyMeNwR+KWWZd1V1m6r+DCgucURVWGjzE8awciV07w6tWhUX8TvzTEI21GQqWSITxQygiYg0FpGqQA/gvRLLvAOcBiAidXFDUYtibbiw0OYnTAYLheD5510Rv3Hj4O67rYifSaiEJQpVLQSGABOBPOB1VZ0nIneIyLneYhOBtSIyH/gcuEFV18batiUKk9F69IC+fV2imDULhg2zLrZJqIQ2t6o6AZhQ4rlbI34OAdd5/+K2bZslCpNhIov4degAJ50EV1wB2YGswmMCJpCfMpujMBnlhx/cHebGjHGP+/SBIUMsSZikCeQnzYaeTEbYts3NPxx1FMyfDzVq+B2RyVCBbG4tUZi0N2uWK/89axZ06+YuoKtXz++oTIYKZHNrcxQm7a1c6f69+Sacf77f0ZgMF7W5FZGok8yqOrJyw4mPzVGYtPTll66I3xVXwNlnw8KFsOeefkdlTMw5ipox/vnChp5MWvn9dzc5fdJJ8PDDxUX8LEmYFBG1uVXV4ckKpDxs6MmkjYkTYdAgd6+Iv/8d7rrLiviZlBNr6OnRaL9X1asrN5z4WI/CpIVly6BTJzjkEDfsZFdXmxQVq7mteBWpBLI5ChNYoRDMmAHHHgsHHggffODuG1Gtmt+RGVOmWENPzycrkPKwHoUJpBUr3D0i3n4bJk2CU06BM87wOypjYoqruRWR/YCbcHeq23Hoo6qnJyiuqLZtswMwEyChEDz3HFx3HRQUwH33wQkn+B2VMXGL98rsl3GF/RoDw4HFuOqwvrAehQmUCy6Afv2gWTOYPRtuvNE+wCZQ4k0UdVR1DLBNVb9Q1X7AcQmMKyqbozApb/t2V8gP4Jxz4Ikn3HDToYf6GpYxFRHvYc027/8VItIRdwOihokJKTbrUZiUlpcH/fu7EhwDB8Ill/gdkTG7JN7m9i4R2QcYCvwH2Bu4NmFRxWDXUZiUtG2bm3+4805XwG+fffyOyJhKEVdzq6rjvR834N2Rzk829GRSzsyZ7mZCc+bAhRfCo4/CX/7id1TGVIq45ihE5HkRqRXxuLaIPJO4sKKzoSeTclatgjVr4J134NVXLUmYtBLvZPaRqro+/EBV1wEtEhNSbJYoTEqYPBkef9z9fPbZsGABdO7sb0zGJEC8iSJbRGqHH4jIvvhYotzmKIyvfvvNVXg95RQ3xBQu4le9ur9xGZMg8Ta3DwJfi8gbQAi4ABiRsKhisDkK45sJE+Cyy+CXX9wFdHfcYUX8TNqLq0ehqi8AXYFVwGrgfFV9MZGBRWNDT8YXy5a5oaV99oGvv4YHH4S99vI7KmMSrjz3zN4X2KSq/wFWi0jjBMUUkyUKkzShEEyb5n4+8ED46CP47jto3drfuIxJonjPeroNV+tpmPfU7sBLiQoqFpujMEnxyy/QpQu0aQNffOGeO+00qFrV37iMSbJ4exTnAecCmwBU9Rd8vsOdzVGYhAmFYPRoaNrU9SAeeMCK+JmMFm+i2KqqIdxENiLi28BsKOT+WY/CJEy3bq70RvPm8P33MHSofeBMRov30/+6iIwCaonIQKAfMDpxYZUtFHL/2/fWVKrt2yErC7Kz3XBTu3YuWWSXZxrPmPQU71lPDwBvAG8CAtyqqlFvk5polihMpZk71w0tjRnjHl98sTsF1pKEMUA5LppT1Y+BjwFEpIqIXKSqLycssjKEexQ2R2F22datcM89MGKEO+W1du3Y6xiTgaImChHZG7gSaAC8h0sUVwI3ALNwNzRKKht6MpUiN9cV8Zs7F3r1gocfhv328zsqY1JSrOb2RWAdMBUYgEsQVYHOqjorwbGVyhKFqRRr18L69TBuHHTq5Hc0xqS0WM3t31S1GYCIjAbWAH9V1d8THlkMlihMuX3+uTuL6eqr3WT1Tz/ZzdeNiUOs2brwne1Q1e3Az34niVAoC7A5ClMOGza4yenTT4cnnywu4mdJwpi4xDouP0pEfvN+zgKqe4+zgJCq7p3Q6EphQ0+mXMaNg8GDYeVKuP56GD7civgZU05Rm1tVrZKsQOJlicLEbdky6NoVDjvM3VDomGP8jsiYQArsieKWKEypQiFX2RWKi/h9+60lCWN2QUIThYicLSIqIgtE5OYoy3UTkZCItIq1TbuOwpQpPx/OPdddPBcu4nfqqVbEz5hdlLBEISJVgMeB9kBToKeINC1luZrA1cD0eLZrQ0/mT4qKqPXaa66I36efwsiRcOKJfkdlTNpIZI/iWGCBqi5S1a3Aq0BpNxS+E/g3UFCejVuiMDt07Ur94cPd8NLcuXDttVAl5abXjAmsRDa3DYBlEY/zgZ3u9iIiLYADVXW8iFwfz0aLilyX4pdflpCX90clhRpMBQUF5OXl+R2GPwoLXS2m7Gz2Pu44tjdrxqYePdypr5m6TzwZ/bkowfZF5Uhkosgq5blQ+AcRyQYeAvqWb7OuE3TwwY3IyalwbGkhLy+PnEzcCXPmQP/+MGCAuz4iJydz90UpbF8Us31RLDc3t8LrJnLoKR84MOJxQ+CXiMc1gSOASSKyGDgOeC+eCW2woaeMtGUL3HYbtGwJS5ZYbSZjkiSRze0MoIl3b+3lQA+gV/iXqroBqBt+LCKTgOtV9dtoG7XJ7Aw1Y4Yr4jd/visD/tBDUKeO31EZkxES1qNQ1UJgCDARyANeV9V5InKHiJxb0e3a6bEZat062LgRJkyAF16wJGFMEiX0uFxVJwATSjx3axnLnhrPNq1HkUE++8wV8fv7310Rvx9/tPIbxvjArsw2qWf9encb0rZtYdSo4s1MP38AABbSSURBVCJ+liSM8UXgEoX1KNLcu++6C+eeeQZuvNHdYMgShDG+Clxza2XG09jSpdC9O+TkwHvvQau4ToAzxiRY4HoUYdajSBOhEEyZ4n7+61/hk0/cGU6WJIxJGYFLFDb0lEaWLoWOHeHkk4uL+J18shXxMybFWKIwyVdUBE88AYcfDpMnw6OPWhE/Y1JY4Jpbu44iDZx/vpu0PvNM+L//g4MO8jsiY0wUgU0U1qMImIgiflx4IXTu7K60ziqtJJgxJpUEbugpzBJFgMyeDa1bu94DQM+ecOmlliSMCYjAJYrw6bGWKAKgoAD++U93BlN+PtSr53dExpgKCFxzGwq5A1G7L02K++Yb6NMHfvjB/T9yJOy7r99RGWMqIHCJAqw3EQi//QabN8OHH8JZZ/kdjTFmFwSuyQ2FLFGkrI8+gnnz3K1IzzgDVK38hjFpIIBzFHZqbMpZt85NTp91FowZY0X8jEkzgUwU1qNIIW+95Yr4vfgiDBsG335rCcKYNBPIJtcSRYpYuhR69IAjjnA3FGrRwu+IjDEJEMAeRZYlCj+FQsV1mf76V3dzoenTLUkYk8YCmChsjsI3S5ZA+/Zw6qnFyeLEE+0PYkyaC1yiABt6SrqiInjsMVfE78sv4T//gZNO8jsqY0ySBK7JtclsH3TpAuPGubOaRo2CRo38jsgYk0SBa3ItUSTJtm3u8vfsbFebqVs3uPhiq89kTAYK3NCTzVEkwXffwbHHwlNPucc9e8Ill1iSMCZDBS5RgPUoEmbzZnctxLHHwsqVcOCBfkdkjEkBgWtybegpQaZNc8X7fvwR+vWDBx6A2rX9jsoYkwIC1+TadRQJsmmTm5f4+GNXp8kYYzyBbHJtjqKSfPihK+I3dCi0betKglet6ndUxpgUE7g5Cht6qgRr17phpvbt4fnnYetW97wlCWNMKSxRZJJQCN54wxXxe+UVd/e5GTMsQRhjogpck2uJYhcsXQq9esGRR7p7Rxx1lN8RGWMCIHA9CrA5inIJhVzhPnBXVE+a5M5wsiRhjIlT4BKF9SjK4eefoV07N1EdLuJ3/PG2A40x5RLARGGnx8a0fTs88oi7T8T06fDkk1bEzxhTYYFrcq1HEYfOneH996FDB1eGw66wNsbsgkA2uTZHUYrIIn4XX+zqM/XqZfWZjDG7LKGJQkTOBh4BqgCjVfXeEr+/DhgAFAKrgX6quiTaNq1HUYpvv4X+/WHQILjySrjwQr8jMsakkYTNUYhIFeBxoD3QFOgpIk1LLDYTaKWqRwJvAP+OtV1LFMWyCgrgppugdWtYvdruE2GMSYhENrnHAgtUdRGAiLwKdAbmhxdQ1c8jlp8G9I5nwzb0BEydSuOePd3tSQcMgPvvh1q1/I7KGJOGEpkoGgDLIh7nA62jLN8f+CDWRkMh2LBhLXl5v+5ieMG25w8/UG/7dpaMGcMfbdrAihXuX4YqKCggLy/P7zBSgu2LYrYvKkciE0Vps6ih0hYUkd5AK+CUWBsNhWD//euQk1NnF8MLoAkTXBG/G26AnBzyWrYk58gj/Y4qJeTl5ZGTk+N3GCnB9kUx2xfFcnNzK7xuIq+jyAciz8tsCPxSciEROQO4BThXVbfE2mhGXkexZg307g0dO8LLLxcX8bMxOGNMEiQyUcwAmohIYxGpCvQA3otcQERaAKNwSSLusaSMaR9DIXj1VcjJgddfh9tug2++sSJ+xpikSliiUNVCYAgwEcgDXlfVeSJyh4ic6y12P1ADGCsis0TkvTI2t5OM6VEsXerKgTduDLm5cPvtliSMMUmX0CZXVScAE0o8d2vEzxW6lVpaJ4pQCD791N1lrlEjV6PpmGPcxXTGGOODwNV6gjROFAsXugJ+Z55ZXMTvuOMsSRhjfBXIRJF2cxTbt8PIkdCsmRtiGjXKivgZY1JGII/N065Hcc458MEH0KmTq/TasKHfERljzA6BbHLTIlFs3ereSHY29O3rCvn16GFF/IwxKSeQQ0+BTxTffAMtW8ITT7jHF1zgqr1akjDGpKBAJorAzlH88QcMHQpt2sC6dXDwwX5HZIwxMQXy2DyQPYovv3TXRCxaBJddBvfdB/vs43dUxhgTUxCb3GAmivCNhT7/HE491e9ojDEmbkFscoOTKMaNg7w8uPFGOO00mD8/QMEbY4xjcxSJsHq1uw3puefCf/9bXMTPkoQxJoACmShStr0NheCVV1wRvzfegDvugOnTrT6TMSbQUrXJjSplE8XSpXDppdCiBYwZA4cf7ndExhizy6xHsauKimDiRPdzo0YwZQp89ZUlCWNM2ghkokiZOYqffoLTT4ezz4bJk91zxx5rRfyMMWklkInC9x5FYSHcfz8ceSTMmuWGmayInzEmTfnd5FaI74miUyc33NS5syvDccABPgdkjL+2bdtGfn4+BQUFfoeyk23btpGXl+d3GElVrVo1GjZsyO6VOPTid5NbIb4MPW3Z4l44OxsGDIB+/aB7d6vPZAyQn59PzZo1Oeigg8hKoe/E5s2bqV69ut9hJE0oFGLt2rXk5+fTuHHjStuuDT3FY9o0OPpoePxx97hbN1fIL4W+EMb4qaCggDp16qRUkshEWVlZ1KlTp9J7dpYootm0Ca69Fo4/Hn7/HZo0SdILGxM8liRSQyL+DoEcekpKopgyxRXx+/lnuOIKuOce2HvvJLywMcaklkD2KJIyR1FY6F7oiy/ckJMlCWNS3scff4yIsHDhwh3PTZ8+ncsuu2yn5W6++WY+/PBDwE14P/DAA7Rr145OnTrRrVs3vgjfs34XjBo1ijPPPJOzzjqLKVOmlLrM1KlTOe+88+jUqRM33XQThYWFAGzYsIErr7ySc845h27duvHjjz/uWGfYsGG0adOGTp067XKM8QpkokhYj+Kdd1zPAVwRv3nz4OSTE/RixpjKNn78eFq2bMmECRPiXueRRx5h9erVjB8/nvHjx/PUU0+xadOmXYpjwYIFvP/++7z//vuMHj2a4cOHs3379p2WKSoq4uabb2bkyJGMHz+eAw44gLfffhuAp556ipycHMaNG8d9993HiBEjdqx3/vnnM3r06F2Kr7xs6Alg1Sq46ioYO9ZNWg8d6uoz+X4erjHB88IL8MwzlbvNfv3gkkuiL7Np0ya+++47XnjhBS6//HKuuuqqmNvdvHkzY8eO5dNPP6WqV5Otbt26dOjQYZfi/fTTT+nYsSNVq1blwAMPpFGjRsyZM4cWLVrsWGb9+vVUrVp1x9lJJ5xwAqNGjaJ79+4sXLiQQYMGAXDwwQezfPly1qxZQ926dTnmmGPIz8/fpfjKK5AtYaW136EQvPQSXHMNbNwII0bADTek0KXfxph4ffLJJ5x00kk0btyYWrVqMW/ePP72t79FXWfJkiXUr1+fGjVqxNz+3XffzfTp0//0fMeOHXc06mGrVq3iqKOO2vF4//33Z9WqVTstU7t2bQoLC/n+++9p1qwZH374IStXrgTgsMMO4+OPP6ZVq1bMmTOHX375hZUrV1K3bt2YcSZCIBNFpbXjS5e6ayJatXJXVx92WCVt2JjMdcklsY/+E+H999+nT58+AHTo0IHx48dz9dVXl3kWUHnPDvrHP/4R97KhUCjm62VlZTFy5Ejuuecetm7dygknnEAVr/zPoEGDGDFiBJ07d+bQQw8lJyeH3Xwc4Qhkotil/RUu4te+vSvi99VXrtqr1WcyJrDWrVvHtGnT+Omnn8jKymL79u1kZWVx1VVXUatWLTZs2LDT8uvXr6d27do0atSIFStWsHHjxpi9ivL0KOrVq7ejdwCuh/GXv/zlT+u2aNGCV155BYAvv/ySxYsXA1CjRg3u8eZLQ6EQbdu2pWHDhrF3RIJkVqL48UfXg5gyBSZNglNOcb0JY0ygTZw4kS5dunDHHXfseK53797MnDmTY445hl9//ZWFCxfuGO9XVXJycqhevTpdu3ZlxIgRDB8+nKpVq/Lrr78ydepUOnfuvNNrlKdHcfrppzN06FAuvfRSVq1axeLFiznyyCP/tNzatWupU6cOW7du5emnn2bw4MEA/Pbbb1SrVo2qVasyduxYWrVqFdfwWKIEMlFkl/dcrcJCePBBuO02qF4dnn3WzmYyJo28//77DBw4cKfn2rVrxwcffMAJJ5zA/fffz7Bhw9iyZQu77bYbd911FzVr1gTgmmuu4eGHH6Zjx47sscceVK9enauvvnqX4mnSpAnt27enQ4cOVKlShVtvvXXHsNLAgQO566672H///Rk9ejSTJk2iqKiInj170qZNGwAWLlzITTfdRHZ2NocccshOZz1dd911fPPNN6xbt46TTz6Zq666iu7du+9SvLFklTaWlspeeikv1Lt3TvlWOuss+OgjOP98d01EvXqJCS7J8vLyyMkp575IU7YvivmxL1J1/2daraew0v4eubm5uS1btqzQEErgehRZWXEmtoICN+tdpQoMGuT+de2a2OCMMSYNBfKCu5i++gqaNy8u4te1qyUJY4ypoMAliqhntG3cCFdf7W4iVFAAKdgVNiZdBW0YO10l4u+QPoniiy/giCPgscdgyBCYOxfOPDOpsRmTqapVq8batWstWfgsfD+KatWqVep2AzhHEeWXe+7pTn094YSkxWOMgYYNG5Kfn8/q1av9DmUn27Ztq9Q7vQVB+A53lSlwiWInb70FP/wA//iHuybi++/twjljfLD77rtX6h3VKkuqno0VNAlNFCJyNvAIUAUYrar3lvj9HsALQEtgLXChqi6Ots2sLGDlSje89Oab7oK56693RfwsSRhjTKVL2ByFiFQBHgfaA02BniLStMRi/YF1qnoI8BBwX6zt7lu0xk1Sjx/vSoJ//bVLEsYYYxIikZPZxwILVHWRqm4FXgU6l1imM/C89/MbQFsRiVqp64DCZW7SevZsuPlmq/RqjDEJlsihpwbAsojH+UDrspZR1UIR2QDUAdaUtdGCpoetyX344SVs3Ai5uZUccvDk2j7YwfZFMdsXxWxf7NCooismMlGU1jMoee5cPMvspGXLlvtVOCJjjDHllsihp3zgwIjHDYFfylpGRHYD9gH+l8CYjDHGlFMiexQzgCYi0hhYDvQAepVY5j2gDzAV6AZ8pqp2xY4xxqSQhPUoVLUQGAJMBPKA11V1nojcISLneouNAeqIyALgOuDmRMVjjDGmYgJXZtwYY0xyBa7WkzHGmOSyRGGMMSaqlK31lIjyH0EVx764DhgAFAKrgX6quiTpgSZBrH0RsVw3YCxwjKp+m8QQkyaefSEiFwC34047n62qJU8oSQtxfEf+iru4t5a3zM2qOiHpgSaYiDwDdAJ+VdUjSvl9Fm4/dQD+APqq6nextpuSPYpElf8Iojj3xUyglaoeibvC/d/JjTI54twXiEhN4GpgenIjTJ549oWINAGGASeo6uHANUkPNAni/Fz8E3dCTQvcGZhPJDfKpHkOODvK79sDTbx/g4An49loSiYKElT+I6Bi7gtV/VxV//AeTsNds5KO4vlcANyJS5YFyQwuyeLZFwOBx1V1HYCq/prkGJMlnn0RAvb2ft6HP1/TlRZUdTLRr0XrDLygqiFVnQbUEpH6sbabqomitPIfDcpaxjsVN1z+I93Esy8i9Qc+SGhE/om5L0SkBXCgqo5PZmA+iOdzcShwqIh8JSLTvOGZdBTPvrgd6C0i+cAE4KrkhJZyytueAKmbKBJS/iOg4n6fItIbaAXcn9CI/BN1X4hINm4YcmjSIvJPPJ+L3XBDDKcCPYHRIlIrwXH5IZ590RN4TlUb4sbnX/Q+L5mmQu1mqu4oK/9RLJ59gYicAdwCnKuqW5IUW7LF2hc1gSOASSKyGDgOeE9EWiUrwCSK9zvyrqpuU9WfAcUljnQTz77oD7wOoKpTgWpA3aREl1riak9KStWznqz8R7GY+8IbbhkFnJ3G49AQY1+o6gYivvwiMgm4Pk3PeornO/IO3pG0iNTFDUUtSmqUyRHPvlgKtMXtixxcokit+7Ymx3vAEBF5FVfNe4Oqroi1Ukr2KKz8R7E498X9QA1grIjMEpH3fAo3oeLcFxkhzn0xEVgrIvOBz4EbVHWtPxEnTpz7YigwUERmA//FnRaadgeWIvJf3MGziEi+iPQXkcEiMthbZALuYGEB8DRwRTzbtRIexhhjokrJHoUxxpjUYYnCGGNMVJYojDHGRGWJwhhjTFSWKIwxxkSVqtdRmDQmItuB7yOe6lJW5V8ROQgYr6pHiMipuOsiOlVCDKcCW1X16zJ+3wU4UlXvEJGTgYeBI4EeqvpGGesI7nqWWsAewBRVHbSrsUZs/1ygqareKyL7AeOBqrgCiMOAXqq6vox1BwN/qOoLItIX+EhVo15oJSKfAN3DtaJM5rJEYfywWVWb+xzDqcBGoNREAdwIhM/BXwr0Ba6Psc1HgYdU9V0AEWm2y1FGUNX3cBdMgbt47AdV7eM9nhJj3aciHvYF5hL7itwXcefZjyh3sCatWKIwKcHrObwI7OU9NaSso/0y1m8LPID7TM8ALlfVLV4pj1aqusYr5fEArqEcDGz36mNdpapTIrZ1KLBFVdcAhHs7IlIUI4z6uBIJeOt9763XFzgP18toDLyiqsO93/XG9Qiq4sqiX6Gq270Cfnfj7p2wRlXbettpBYzGVcetLiKzgDa4C83C7/MSXFILAXNU9WIRuR2XGBd723hZRDbjyr4MUNXzvHjO9Pbd+bikNAVLFBnP5iiMH6p7V5DPEpG3ved+Bc5U1aOBC3FH53ERkWq4OvwXqmozXLK4vKzlvYb/KdzRf/PIJOE5AYh5M5dSPAR8JiIfiMi1JQrwHQtcBDQHuotIK6+UxIW4+0U0B7YDF3nDSk8DXVX1KKB7ifhnAbcCr3nxbw7/TkQOxzX+p3vr/r3Eum8A3wIXea85AcjxXhPgUuBZb9l1wB4iko5VmU05WKIwftjsNXDNw0eywO7A0yLyPe7OdH+6IVEUAvysqj96j58HTt6F+OpTgTpAqvoskIOL/1RgmncnRoCPVXWt16i/BZyIGz5qCczwegZtgb/hihlO9gr5oarlKXZ5OvBGRG8o6rpeGYsXcSW4a+F6J5Fl6n8FDijH65s0ZENPJlVcC6wCjsIdwES96ZCITAT2xx0dPxZl0UKKD4iqxRnLZlw14qhEZATQESA85+JNED8DPCMic3HVbOHPpZxDuJLPz6vqsBLbPbeU5eOVVYF1nwXG4fb5WK92Ulg13P4wGcx6FCZV7AOsUNUi4GLc2HyZVPUsr0cyAPgBOEhEDvF+fTHwhffzYtxRO0DXiE38jitLXpo84JAyfhcZwy3hnhG4+zaLyO7ez/VwN9Ja7i1+pojsKyLVgS7AV8CnQDcR+Yu3zr4i0ghX1O0UrxoqIrJvrFgifApcEB4uKmPdnd67l9x+wd0u9Lnw894dI+vh9qHJYJYoTKp4AugjItNw5bA3xbuiqhbgxtbHekNXRbg5CIDhwCMiMgU3BxA2DjjPmyc5qcQmJwMtwrfWFZFjvDujdQdGici8MkJpB8z1KpROxFVrXen97kvcEM8s4E1V/VZV5+Ma549EZA7wMVBfVVfj7mf8lret18qxL+bhJp+/8NYdWcpizwFPee+9uvfcy8AyL6awlsC0Ej0Mk4GseqwxpRCRR4BxqvpJJWyrL+6MpCG7HFiCiMhjwExVHRPx3CPAe6r6qX+RmVRgPQpjSnc3sKffQSSDiOTiLiZ8qcSv5lqSMGA9CmOMMTFYj8IYY0xUliiMMcZEZYnCGGNMVJYojDHGRGWJwhhjTFT/D784MOMVitk5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAGDCAYAAAAVnQglAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd5xU1fnH8c/uohTpSFFAIQqPQQMiEVFjwy4KxqigBrH9iMYaO+rPFmss2PWHigIaEQuKFWuMmiAqoIj42EBYEJDey+7O7497d2ZYt8Fw987uft95zYuZc8s5d7Lus8+5556Tk0gkEBERkfjlxt0AERERCSgoi4iIZAkFZRERkSyhoCwiIpIlFJRFRESyhIKyiIhIllBQllrJzOqb2StmtszMnsvgPKeY2Vtbsm1xMLM3zGxQ3O0Qqe1y9JyyZDMzOxm4GNgFWAFMAW52948yPO9A4HxgH3cvyLihW5iZHQi8D4x19+PSyrsRfAcfuPuBlTjP9cDO7v7naFoqIluSMmXJWmZ2MXAPcAvQGtgBeAjotwVOvyPwbTYG5DS/APuYWYu0skHAt1uqAjPLMTP9HhDJEsqUJSuZWRNgDnC6u5favWxmdYHbgRPDojHAFe6+Lsw0nwKGAlcAhcBV7v6Emd0ADAFygHXAhUB70jJKM+sAzAC2cvcCMzsNuBZoCSwErnH3p8Pys9z9D+Fx+wD3Ap0JgueF7v6fcNu/gA+B3kBX4L/Aye6+sJRrK27/q8BUd3/QzPKAn4BhQO/iTNnM7gWOA5oA3wEXufuHZnYEMC7tOn9w925hOz4GDgT2AH4HPAY85e6PmdnDQEt3Pz48/+3A74FD3F2/MEQipL+QJVvtDdQDxpazz9VAL2B3oBvQE7gmbXsbgkDVFjgTeNDMmrn7dQTZ97Pu3tDdHy+vIWa2DXAfcKS7NwL2IehCLrlfc+C1cN8WwN3AayUy3ZOB04FWwNbApeXVDYwETg3fHw5MA+aW2OdTgu+gOfBP4Dkzq+fub5a4zm5pxwwEBgONCAJ9ukuArmZ2mpntR/DdDVJAFomegrJkqxbAwgq6l08BbnT3Be7+C3ADQbAptiHcvsHdXwdWAraZ7SkCdjOz+u7+s7tPK2WfPsB37j7K3Qvc/RngG+CYtH2ecPdv3X0NQWa/e3mVhll2czMzguA8spR9nnL3RWGddwF1qfg6n3T3aeExG0qcbzXwZ4I/Kp4Cznf3/ArOJyJbgIKyZKtFwLZmVqecfbZn4yzvp7AseY4SQX010HBTG+Luq4D+wNnAz2b2mpntUon2FLepbdrneZvRnlHAecBBlNJzYGaXmNn0cCT5UoLegW0rOOfs8ja6+0TgR4Ku7zGVaKOIbAEKypKt/gusBY4tZ5+5BAO2iu3Ar7t2K2sV0CDtc5v0je4+3t0PBbYjyH4frUR7its0ZzPbVGwU8Ffg9TCLTQq7l68guK/ezN2bAssIgilAWV3O5XZFm9m5BBn3XODyzW+6iGyK8rIQkdi4+zIzu5bgPnAB8BZBd/QhwEHufjnwDHCNmX1KEGSuJehu3RxTgCvMbAeCoDakeIOZtQb2At4F1hB0gxeWco7XgfvDx7jGAH8CuhAM1tps7j7DzA4gyFxLagQUEIzUrmNmVwKN07bPBw41s1x3L6pMfWbWGbiJYCDYamCimb3h7r+6jy4iW5YyZcla7n43wTPK1xAEndkE3bgvhbvcBHwGfAlMBSaFZZtT19vAs+G5PmfjQJpLMPhpLrAYOIAgcy15jkXA0eG+iwgyzKNLG129Ge37yN1L6wUYD7xBMNL7J4LehfSu6eKR64vMbFJF9YS3C54Cbnf3L9z9O+AqYFQ42l1EIqRHokRERLKEMmUREZEsoaAsIiKSJRSURUREsoSCsoiISJZQUBYREckSWfuccu65e2tYuFR7o4ctjrsJIlvEiRs8p+K9Nk/OOb0y+n2feHhCZG2ralkblEVEpHbIya0xMTVj6r4WERHJEsqURUQkVsqUUxSURUQkVgrKKQrKIiISKwXlFAVlERGp0cysPTCSYEnWImCYu99rZs0JFqLpAMwETnT3JWaWA9wLHEWwUtpp7j4pPNcggkVyAG5y9xFheQ/gSaA+wYpxF7p7oqw6ymqrBnqJiEiscnJyMnpVQgFwibv/FugFnGtmXYArgXfdvRPB0qxXhvsfCXQKX4OBhwHCAHsdwVKuPYHrzKxZeMzD4b7Fxx0RlpdVR6kUlEVEJFY5uTkZvSri7j8XZ7ruvgKYDrQF+gEjwt1GAMeG7/sBI9094e4TgKZmth1wOPC2uy8Os923gSPCbY3d/b/uniDIytPPVVodpVJQFhGRWEUdlNOZWQegO/AJ0Nrdf4YgcAOtwt3asvG65PlhWXnl+aWUU04dpdI9ZRERiVWmA73MbDBB13GxYe4+rJT9GgIvABe5+3IzK7NJpZQlNqN8kykoi4hItRYG4F8F4XRmthVBQH7a3V8Mi+eb2Xbu/nPYBb0gLM8H2qcd3g6YG5YfWKL8X2F5u1L2L6+OUqn7WkREYhV193U4mvpxYLq73522aRwwKHw/CHg5rfxUM8sxs17AsrDreTxwmJk1Cwd4HQaMD7etMLNeYV2nljhXaXWUSpmyiIjEqgqeU94XGAhMNbMpYdlVwG3AGDM7E5gFnBBue53gcajvCR6JOh3A3Reb2d+BT8P9bnT34lVnziH1SNQb4Yty6ihVTiKRnYsxaZUoqQm0SpTUFFGuEtXw2oMz+n2/8sZ3a8zsI8qURUQkVprRK0X3lEVERLKEMmUREYlVJWflqhUUlEVEJFbqvk5RUBYRkVgpKKfonrKIiEiWUKYsIiKxUqacoqAsIiKxUlBOUVAWEZFYKSinKCiLiEisFJRTNNBLREQkSyhTFhGRWClTTlFQFhGRWCkopygoi4hIrDTNZoqCsoiIxEqZcooGeomIiGQJZcoiIhIrZcopCsoiIhIrBeUUBWUREYlVrm6kJumrEBERyRLKlEVEJFZ5eiQqSUFZRERilad7ykkKyiIiEitlyikKyiIiEqs8jW5K0lchIiKSJZQpi4hIrNR9naKgLCIisVJQTlFQFhGRWGn0dYqCsoiIxCpPMTlJA71ERESyhDJlERGJlbqvUxSURUQkVhrolaKgLCIisVKmnKJ7yiIiIllCmbKIiMRKo69TFJRFRCRW6r5OUVAWEZFYaaBXioKyiIjESkE5RQO9REREsoQyZRERiZXWU05RUBYRkVip+zpFQVlERGKl0dcpCsoiIhIrZcop6skXERHJEsqURUQkVhrolaKgLCIisVL3dYqCsoiIxEoDvVLUaSAiIpIllCmLiEis1H2doqAsIiKx0kCvFAVlERGJlTLlFAVlERGJVZ5icpI6DURERLKEMmUREYlVrrqvkxSURUQkVuq+TlFQFhGRWEU9d4iZDQeOBha4+25p5ecD5wEFwGvufnlYPgQ4EygELnD38WH5EcC9QB7wmLvfFpZ3BEYDzYFJwEB3X29mdYGRQA9gEdDf3WeW11bdUxYRkVjl5WT2qoQngSPSC8zsIKAf0NXddwXuDMu7AAOAXcNjHjKzPDPLAx4EjgS6ACeF+wLcDgx1907AEoKATvjvEnffGRga7lcuBWUREanR3P3fwOISxecAt7n7unCfBWF5P2C0u69z9xnA90DP8PW9u//o7usJMuN+ZpYD9AaeD48fARybdq4R4fvngYPD/cuk7msREYlVbob912Y2GBicVjTM3YdVcFhnYD8zuxlYC1zq7p8CbYEJafvlh2UAs0uU7wW0AJa6e0Ep+7ctPsbdC8xsWbj/wrIapaAsIiKxynSgVxiAKwrCJdUBmgG9gD2BMWb2G6C01iQovWc5Uc7+VLCtVOq+FhGRWOXmZPbaTPnAi+6ecPeJQBGwbVjePm2/dsDccsoXAk3NrE6JctKPCbc34dfd6Bt/F5t9OSIiItXXSwT3gjGzzsDWBAF2HDDAzOqGo6o7AROBT4FOZtbRzLYmGAw2zt0TwPvA8eF5BwEvh+/HhZ8Jt78X7l8mdV9XQ+2atmLEoGtp07gFRYkiHv3oZe771xiaNWjM6DP+TocW2zFz0c/0f/walq5ZwQGduvPSX/7BjEXBH29jp3zA398YDkCT+g159JQh7LbdTiRIcOZTNzNhxlcc37031/U5k9+27sBed5zJ57O+SdZ/5WGncsY+x1BYVMiFzw3lremfxPI9SM1Vv10b9nriH9RrvS2JoiJ+fHwM390/kq63Xc72fQ6iaMMGVv4wi0/PGsKGZStovufv6PHw3wHIyclh2o33M+fldwBoc9h+7H731eTk5TJj+HN8c8ejcV6alCLq55TN7BngQGBbM8sHrgOGA8PN7CtgPTAoDJjTzGwM8DXBo1LnuntheJ7zgPEEj0QNd/dpYRVXAKPN7CZgMvB4WP44MMrMvifIkAdU1NacRKLcoB2b3HP3zs6GZYE2jVuwXZMWTJ79LQ3rNuCzK57gj8Ou4LRefVi8ajm3vz2KKw4dSLMGjbjy5Yc4oFN3Ljn4FPo+cumvzvXEwP/lox+m8Ph/XmGrvDo02Loey9asZJfWO1KUSPDISVdw2dj7k0H5t2068M/Tb2SvO85k+ybb8vb592E39KcoUVTVX0O1MHpYuT1VUoZ6bVpSb7uWLJ38NXUabsOhn7zAx8efS/22bVjw/gQShYV0vSX4ef7yqjvJq1+PovUbSBQWUq9NSw77/GVe2WE/SCQ48uvxfHDk6azJn88hE55nwp8vZvn0H2K+wurnxA0eWei8/pO/ZPT7/vq9/q/GTD+i7utqaN7yRUye/S0AK9etZvr8mbRt2pK+XfdjxCevAzDik9fp123/cs/TqF4D9t95dx7/zysAbCgsYNmalQB8M/8nvl0w61fH9Ou6P89+/g7rCzYwc9HPfP9LPj07dPnVfiKZWDvvF5ZO/hqAgpWrWP7Nj9TfvjXz3/mYRGEhAIs+mUL9dm0AKFyzNlmeV68uhMlG855dWfnDT6yakU/Rhg3MevY1tj/m4BiuSMpTBc8pVxuRdF+b2XHlbXf3F6OotzbasXkburfrzCczp9G6UXPmLV8EBIG7VaNmyf327rgbk4eMZO6yhVw29n6+/nkGv9m2Lb+sXMrwgdfQrW0nJs36hgufH8rq9WvLrK9t05ZMmPFV8vOcpb/QtmnL6C5Qar0GO7al6e6/ZdHELzYq73jan5j13BvJz817dmXPYbfQYMftmXja5SQKC6m/fWtW589L7rNmznya9+xaZW2Xyol6Rq/qJKpM+ZhyXkdHVGets03d+jz/P7fyt+fvYcXa1WXuN2m20+HaP9L91lN54IPnGDs4mFSmTm4ee7TvzCMfvkiP2waxav0arjzs1HLrzCllhH+23gKR6q/ONg3YZ8x9TLnkFgpWrEqW//bKsykqKGTWP8clyxZP/JLxux/NO3sfzy5X/IXcultDaQsd6OdVslgkmbK7nx7FeSWlTm4ez591C//8dDxjv/gAgPkrFtOmcQvmLV9Em8YtWLBiCcBGAfuNaf/lwf6X0WKbJuQvXUD+0l+YODPoJnx+8vtccdjAcuvNX7qA9s1aJz+3bdqSucvKfA5eZLPl1KnDPmPuY9YzrzDnpbeT5TsOPJbt+hzIB4edVupxK775kcJVa2iyW2fWzJlHg7CLG6B+29asmbug1OMkPnlaJSop8nvKZtbHzC43s2uLX1HXWRs89uer+WbeTwx9b3Sy7JWpHzFor6MAGLTXUYz78kMAWjduntxnzx27kJuTw6JVy5i/fDGzl8ync6sdADjYfs/0eTPLrXfc1A/p3+MQtq6zFR1abEenVu2TQV1kS9rz0ZtZ/s2PfHvPk8myNoftxy6X/g8f//EcCtekbrNs06EdOXl5ADTYYXsade7IqplzWPzpVBru3IFtOrQjd6ut2KF/H+a++l5VX4pUIKbnlLNSpI9EmdkjQAPgIOAxgue0JkZZZ22w705dOXWvI/lyzvdMGhJMq3r1uEe47a2RPHvmzZyxzzHMWjKfEx+7GoDju/fm7P3+SEFhIWs2rOOk4am/iy547m6eOu16tq6zFT8unMMZo24G4NhuB3DfCRfTsmFTXj3nLqbkf8uRD/6Nr3+ewXOT3mXaNf+koKiQ8569UyOvZYvbdt8edPjzsSyd6hz62UsATL3mbroPvYa8uluz/5tPALD4ky/4/Nzr2HbfHuxy2f9QVFAARUV8fv71rF8U9BRNuvBG9n/tMXLy8pjx5Ass//r72K5LSlfTBmtlItJHoszsS3fvmvZvQ4IZVA6r6Fg9EiU1gR6Jkpoiykei7pp0dka/7y/Z45EaE9ajnjxkTfjvajPbnmA9yY4R1ykiItVIrh7OTYo6KL9qZk2BOwgWfk4QdGOLiIgAGuiVLtKg7O5/D9++YGavAvXcfVmUdYqISPVS0wZrZSLqgV55QB+gQ3FdZoa73x1lvSIiUn1ooFdK1N3XrxAsHj2VYFksERERKUPUQbmdu2tOOxERKZO6r1OiHvP2hplV+PiTiIjUXnk5ORm9apKoM+UJwFgzywU2ADlAwt0bR1yviIhUE8qUU6IOyncBewNTw8WjRURENqKBXilRd19/B3ylgCwiIlKxqDPln4F/mdkbwLriQj0SJSIixXJr2H3hTEQdlGeEr63Dl4iIyEbUfZ0SWVAOJw5p6O6XRVWHiIhUf8qUUyK7p+zuhcAeUZ1fRESkpom6+3qKmY0DngNWFRe6+4sR1ysiItWEMuWUqINyc4LlGnunlSUABWUREQEUlNNFvUrU6VGeX0REqr/cHC2oXCzqVaLaAfcD+xJkyB8BF7p7fpT1iohI9aFMOSXqP0+eAMYB2wNtCVaNeiLiOkVERKqlqO8pt3T39CD8pJldFHGdIiJSjShTTok6KC80sz8Dz4SfTyIY+CUiIgIoKKeLuvv6DOBEYB7BlJvHh2UiIiIA5Gb4v5ok6tHXs4C+UdYhIiLVmzLllEiCspldW87mhLv/PYp6RUREqrOoMuVVpZRtA5wJtAAUlEVEBFCmnC6SoOzudxW/N7NGwIXA6cBo4K6yjhMRkdpHk4ekRLlKVHPgYuAUYASwh7sviao+ERGpnpQpp0R1T/kO4DhgGPA7d18ZRT0iIiI1SVSZ8iXAOuAa4GozKy7PIRjo1TiiekVEpJpRppwS1T1l3SAQEZFKUVBOiXpGLxERkXJpoFeKgrKIiMQqF2XKxfTniYiISJZQpiwiIrHSPeUUBWUREYmV7imnKCiLiEislCmnKCiLiEisFJRT1GcgIiKSJZQpi4hIrHRPOUVBWUREYqXu6xQFZRERiZUmD0lRn4GIiEiWUKYsIiKxUvd1ioKyiIjESgO9UhSURUQkVsqUUxSURUQkVjnKlJP0TYiIiGQJZcoiIhKr3IjzQzMbDhwNLHD33cKyO4BjgPXAD8Dp7r403DYEOBMoBC5w9/Fh+RHAvUAe8Ji73xaWdwRGA82BScBAd19vZnWBkUAPYBHQ391nltdWZcoiIhKrnJzcjF6V8CRwRImyt4Hd3L0r8C0wBMDMugADgF3DYx4yszwzywMeBI4EugAnhfsC3A4MdfdOwBKCgE747xJ33xkYGu5XLgVlERGJVW5Obkavirj7v4HFJcrecveC8OMEoF34vh8w2t3XufsM4HugZ/j63t1/dPf1BJlxPzPLAXoDz4fHjwCOTTvXiPD988DB4f5lUve1iIjEKifD/NDMBgOD04qGufuwTTjFGcCz4fu2BEG6WH5YBjC7RPleQAtgaVqAT9+/bfEx7l5gZsvC/ReW1RAFZRERqdbCALwpQTjJzK4GCoCnw6LSMtkEpfcsJ8rZv7xzlanCoGxmvYAv3X21mZ0EdAfud/fZFRwqIiJSobgmDzGzQQQDwA529+JgmQ+0T9utHTA3fF9a+UKgqZnVCbPl9P2Lz5VvZnWAJpToRi+pMt/EMGCNmXUFrgLmA09V4jgREZEK5ZCb0WtzhCOprwD6uvvqtE3jgAFmVjccVd0JmAh8CnQys45mtjXBYLBxYTB/Hzg+PH4Q8HLauQaF748H3ksL/qWqzNUUhCfpB9zr7ncBjSpxnIiISIWiHuhlZs8A/w3eWr6ZnQk8QBDL3jazKWb2CIC7TwPGAF8DbwLnunthmAWfB4wHpgNjwn0hCO4Xm9n3BPeMHw/LHwdahOUXA1dW1NacRKLcoI2ZfUgQ7c8CDiTIlL9w999V+E1kIPfcvctvmEg1MHpYuT1VItXGiRs8srkw5656LKPf99tvc1aNmaezMplyf4Kb1We7+88E/eV3R9oqERGpNargOeVqozKjr5cAd7p7kZntBBgwKtpmiYhIbRH1jF7VSWW+iQ+Bema2HfABcA4wPNJWiYhIraFMOaUyV5Mbjkz7E/CAux8DdIu2WSIiUltEPdCrOqlUUDazPYGTgVc34TgRERHZBJW5p3wxcAPwmrt/ZWa/IejSFhERyVgOeXE3IWtUGJTd/T3gvbTPPwJ/jbJRIiJSe9S0LuhMVGaazW2BSwiWsapXXO7uh0XYLhERqSUyXZCiJqnMN/EUMBPoTLAW5DxgSoRtEhGRWkQDvVIqczUt3f3/gPXu/i7BPJ49o22WiIhI7VOZgV4bwn/nmdnhBKtftC9nfxERkUqrac8aZ6IyQfkWM2sCXAo8CDQGLou0VSIiUmtoRq+Uyoy+Hhe+/RLYL9rmiIhIbaNMOaXMoGxmQ4EyV+5w94sjaZGIiEgtVV6m/FWVtUJERGqtmjaCOhPlBeWngIbuvii90MxaACsjbZWIiNQaek45pbxv4l6gdynlfdB6yiIisoXoOeWU8q5mf3d/rpTyUcCB0TRHRERqmxxyM3rVJOVdTU5phe6eKGubiIiIbL7ygvJCM+tRstDM9gAWR9ckERGpTdR9nVLeQK/LgBfM7DHg87Ds98AZBGsrR+qfjyjuS/XXf3CzuJsgskWcGOG59ZxySpnfhLtPAHoB9YGzw1d9YB93/2/VNE9ERGq6nERmr5qk3Bm93H0ecHUVtUVERGqjRFFmx9egUU7qMxAREckSlVmQQkREJDqZZso1SKUzZTOrG2VDRESklkoUZfaqQSoMymbW08ymAt+Fn7uZ2f2Rt0xERGoHBeWkymTK9wFHA4sA3P0L4KAoGyUiIlIbVSYo57r7TyXKCqNojIiI1EJFRZm9apDKDPSabWY9gYSZ5QHnA99G2ywREak1algXdCYqE5TPIejC3gGYD7wTlomIiGROQTmpwqDs7guAAVXQFhERqY0UlJMqDMpm9ijwq4nM3H1wJC0SERGppSrTff1O2vt6wB+B2dE0R0REap0aNlgrE5Xpvn42/bOZjQLejqxFIiJSu6j7OmlzptnsCOy4pRsiIiK1lIJyUmXuKS8hdU85F1gMXBllo0RERGqjcoOymeUA3YA5YVGRu9ew1StFRCRWypSTKlpPOWFmY929R1U1SEREapdEIrNJImvQcsqVmmZzopntEXlLRESkdtI0m0llZspmVsfdC4A/AP9jZj8Aqwj+KEm4uwK1iIhkTt3XSeV1X08E9gCOraK2iIiI1GrlBeUcAHf/oYraIiIitZEy5aTygnJLM7u4rI3ufncE7RERkdpGQTmpvKCcBzSkZg1sExGRbKOgnFReUP7Z3W+sspaIiEjtVMNGUGeivEeilCGLiIhUofIy5YOrrBUiIlJ7qfs6qcyg7O6Lq7IhIiJSSykoJ23OKlEiIiJbjoJyUmWm2RQREZEqoExZRETipdHXSQrKIiISL3VfJykoi4hIvBSUkxSURUQkXlXQfW1mfwPOAhLAVOB0YDtgNNAcmAQMdPf1ZlYXGAn0ABYB/d19ZnieIcCZQCFwgbuPD8uPAO4lmA3zMXe/bXPaqYFeIiJSo5lZW+AC4PfuvhtB4BwA3A4MdfdOwBKCYEv47xJ33xkYGu6HmXUJj9sVOAJ4yMzyzCwPeBA4EugCnBTuu8kUlEVEJF5FicxelVMHqG9mdYAGwM9Ab+D5cPsIUksV9ws/E24/2MxywvLR7r7O3WcA3wM9w9f37v6ju68nyL77bc5Xoe5rERGJV4bd12Y2GBicVjTM3YcVf3D3OWZ2JzALWAO8BXwOLHX3gnC3fKBt+L4tMDs8tsDMlgEtwvIJafWkHzO7RPlem3MtCsoiIhKvDINyGICHlbXdzJoRZK4dgaXAcwRdzSUVp92lrf2QKKe8tF7nSqfw6dR9LSIi8Yq++/oQYIa7/+LuG4AXgX2ApmF3NkA7YG74Ph9oDxBubwIsTi8vcUxZ5ZtMQVlERGq6WUAvM2sQ3hs+GPgaeB84PtxnEPBy+H5c+Jlw+3vungjLB5hZXTPrCHQCJgKfAp3MrKOZbU0wGGzc5jRUQVlEROJVVJTZqwLu/gnBgK1JBI9D5RJ0d18BXGxm3xPcM348PORxoEVYfjFwZXieacAYgoD+JnCuuxeG96XPA8YD04Ex4b6bLCeR2Kxu78iNzrPsbJjIJjhpcLO4myCyRSQenlDa/dQtc+6p12f0+z7nd9dH1raqpoFeIiISr8o/1lTjqftaREQkSyhTFhGReGmVqCQFZRERiZe6r5MUlEVEJF7KlJMUlEVEJF4Kykka6CUiIpIllCmLiEisMp0vo8Y8pIyCsoiIxE3d10kKyiIiEi8F5SQFZRERiZceiUrSQC8REZEsoUxZRETipe7rJAVlERGJl4JykoKyiIjES/eUk3RPWUREJEsoUxYRkXip+zpJQVlEROKloJykoCwiIvHSPeUkBWUREYmXMuUkDfQSERHJEsqURUQkXsqUkxSURUQkXrqnnKSgLCIi8VKmnKSgLCIisUoUKlMupoFeIiIiWUKZsoiIxEv3lJMUlEVEJF7qvk5SUBYRkVgllCkn6Z6yiIhIllCmLCIi8VL3dZKCsoiIxKtQzykXU1AWEZFY6Z5yioKyiIjES93XSRroJSIikiWUKdcwPR+7he37HMjaBYt4s9sxG22zi8+g+x1X8GKrXqxftIRWB/TkD2MfYtWMfADyx77NtJseBKDzhYPY6cwTSCQSLPvqWz45YwhF69ZX+fVIzdauWStGDrqONo1bUJQoYthHL3Hf+2No1qAxz8+lXRgAABOGSURBVJ51Ex1abMfMRT9z4mNXs3T1CgAO6LQH95xwEVvl1WHhyqUcOPSvyfPl5uTy2ZAnmLP0F4556NJk+U19z+aEPXpTWFTEwx++yP3vj6Fpg0YMH3g1O23bjrUF6zhj1M1Mm/tjlX8HgiYPSaOgXMPMGPEi3z34FHs9eftG5Q3ataHNofuw6qc5G5X/8tFnfNj37I3K6m/fis7nn8obux1F4dp17DP6HnYc0IcZI8ZG3n6pXQoKC7nkhfuYPNtpWLcBnw95krenT+S0vY/m3W8+5fa3RnHFYQO58rBTufKlB2lSvyEPnXQZR9x/EbOXzKdlo2Ybne/C3v2ZPm8mjettkyw7be8+tG/Wil1u6E8ikUgec9URg5iS/x3H/d+VWOsdeXDApRxy7/lVev0S0NzXKeq+rmF++fAz1i9e9qvy7ncP4Ysr7oBE5X74c+vkkVe/Hjl5eeQ1qMeauQu2dFNFmLd8EZNnOwAr161m+ryZtG3ain7d9mPEhNcBGDHhdY7dfX8ATt7zcF6c8i9mL5kPwC8rliTP1bZpS/rstg+PfTxuozrO2f84bnx9OInwZ7/4mC5tOvLuN58B4PN/okOL7WjVqHmEVytlKirK7FWDRBqUzewfZtbYzLYys3fNbKGZ/TnKOuXXtj+mN6vnLGDpl/6rbdv22p3DJ73M/q89SuMuOwOwZu4CvrlrOMfMfJ9+cz5iw7KVzHv746puttQyOzbfju7tO/PJzK9o3ag585YvAoLA3SrMbju3bk+zBo14/28P8dmQJxm415HJ4+854W9cPvYBikp0he60bTv69ziET698gtfPG8rOLdsD8MWc7zhu9wMB2HPHLuzYvA3tmrWsgiuVXylMZPaqQaLOlA9z9+XA0UA+0Bm4LOI6JU1e/XrsOuRsvrru3l9tWzxpGq907M34Pfrx3QOj2O/F4H7yVk0b07bvwby608G83G4/6mxTnx1P6VvVTZdaZJu69XnhL7dy0XP3sGLt6jL3q5ObR48ddqHPgxdz+H0X8r9HnUGnVu3ps9u+LFixhEmzfv2HZ906W7F2w3r2vO10Hv3oZYafejUAt40fSbMGjZh81UjOP+gEJs/+loLCwsiuUaQyor6nvFX471HAM+6+2MwirlLSNdxpB7bp2I4jJr8MQP12bTj8sxd5u9cJrJ2/MLnfz2/8m9wHrmPrFs1ofdBerJqZz7qFQTdf/ti32Hbv7vz09LhS6xDJRJ3cPF4YfCtPTxzP2Cn/AmD+isW0adyCecsX0aZxCxaEXc75SxawcOUyVq9fy+r1a/n3d5Pp1q4Te7Q3+nbdj6N224d6dbamcf1tGHXa9Qx88nryly7ghcnvAzB2yr944tRrAFixdjVnjLop2Y4ZN41lxqK5VXvxAug55XRRZ8qvmNk3wO+Bd82sJbA24jolzbKvvuWl7fbhlZ0O5pWdDmZN/jzG//441s5fSL3W2yb3a77n7yA3l/WLlrBq1lxa7NWNvPr1AGjde2+WT/8hrkuQGu7xgVczfd5Mhr77TLJs3JcfMqjXUQAM6nUUL3/xIQAvf/kh++3cjbzcPOpvVZe9Ou7K9Hkzuerlh2l/VV86XvNHBjz+v7znnzHwyesBeOmLf9PbegDByO1v588CoEn9hmyVF+QlZ+3bj39/N7ncLF0ipO7rpEgzZXe/0sxuB5a7e6GZrQL6RVlnbbf303fR6oCe1N22GX1/+oCvbrifH4c/X+q+7f90ODuffRJFBYUUrlnLf06+GIDFE79k9gvjOfyzsRQVFLB0ynR+ePTZqrwMqSX23akbp/Y6ii/zv2fyVSMBuOrlh7lt/EjGnHUzZ+7bl1mL53HCo0GX8zfzZvLm1xP48pqnKEoU8djH4yp8jOm28SN5+vQb+NvBA1i5bg1nPXULAL9t04GRp11HYVEhX/88kzOfujnSa5Vy1LDAmomcRCVH424OMzu1tHJ3H1nRsaPzTP8vSbV30uBmFe8kUg0kHp6QE9W51916bEa/7+sOeSmytlW1qO8p75n2vh5wMDAJqDAoi4hI7aB7yilRd19v9CS+mTUBRkVZp4iIVDNaJSqpqmf0Wg10quI6RUQkiylTTok0KJvZK0Dxt50H/BYYE2WdIiJSzWigV1LUmfKdae8LgJ/cPT/iOkVERKqlSJ9TdvcPgG+ARkAzQMsMiYjIxooSmb1qkKjnvj4RmAicAJwIfGJmx0dZp4iIVC+JwkRGr5ok6u7rq4E93X0BQDij1ztA6bNZiIhI7VPDst1MRB2Uc4sDcmgRWi5SRETS6ZGopKiD8ptmNh4ontS2P/B6xHWKiIhUS1FPHnKZmf0J2BfIAYa5+9go6xQRkeqlKp5TNrM84DNgjrsfbWYdgdFAc4KZJge6+3ozq0sw62QPgt7d/u4+MzzHEOBMoBC4wN3Hh+VHAPcSPPr7mLvftrntjHzyEHd/AXgh6npERKSaqprBWhcC04HG4efbgaHuPtrMHiEItg+H/y5x953NbEC4X38z6wIMAHYFtgfeMbPO4bkeBA4F8oFPzWycu3+9OY2MJCib2Ufu/gczW0Fq8hAIsuWEuzcu41AREallos6Uzawd0Ae4GbjYzHKA3sDJ4S4jgOsJgnK/8D0Eg5IfCPfvB4x293XADDP7HugZ7ve9u/8Y1jU63Dd7grK7/yH8t1EU5xcREdkE9wCXE8yZAdACWOruBeHnfKBt+L4tMBvA3QvMbFm4f1tgQto504+ZXaJ8r81taNTTbPYCprn7ivBzQ2BXd/8kynpFRKT6yPRZYzMbDAxOKxrm7sPCbUcDC9z9czM7MNxe2lKPiQq2lVVe2hNFm31BUd9TfhjYI+3z6lLKRESkFsu0+zoMwMPK2Lwv0NfMjiJYQrgxQebc1MzqhNlyO2BuuH8+0B7IN7M6QBNgcVp5sfRjyirfZFE/M5zj7slv292LqPqVqUREJIsVFSYyepXH3Ye4ezt370AwUOs9dz8FeB8onmFyEPBy+H5c+Jlw+3thHBsHDDCzuuHI7U4EM1Z+CnQys45mtnVYx7jN/S6iDpA/mtkFBNkxwF+BHyOuU0REqpGYlm68AhhtZjcBk4HHw/LHgVHhQK7FBEEWd59mZmMIBnAVAOe6eyGAmZ0HjCd4JGq4u0/b3EblJBLRfRlm1gq4j2CUWwJ4F7ioxCxfpRqdZ5p3Taq9kwY3i7sJIltE4uEJpd1T3SKWnHFARr/vmw3/ILK2VbWoJw9ZQPhXhoiISGkSRZpms1hUzylf7u7/MLP7KWUUmrtfEEW9IiJS/dS0lZ4yEVWmPD3897OIzi8iIjVETPeUs1JUk4e8Ev47Iorzi4iI1ERRdV+/QjkPT7t73yjqFRGR6kfd1ylRdV/fGdF5RUSkhlH3dUpU3dcfRHFeERGpeYoUlJOinvu6E3Ar0IVgejMA3P03UdYrIiLVh7qvU6KeZvMJgtm8CoCDCBaOHhVxnSIiItVS1EG5vru/SzAH9k/ufj3B7F4iIiJAcE85k1dNEvXc12vNLBf4LpwbdA7QKuI6RUSkGqlpgTUTUQfli4AGwAXA3wmy5EHlHiEiIrWK7imnRD339afh25XA6VHWJSIi1ZPmvk6JavKQcteS1OQhIiIivxZVprw3MBt4BvgEqDHLaomIyJal7uuUqIJyG+BQ4CTgZOA14JlMFn4WEZGaSQO9UiJ5JMrdC939TXcfBPQCvgf+ZWbnR1GfiIhUX0VFiYxeNUlkA73MrC7QhyBb7gDcB7wYVX0iIiLVXVQDvUYAuwFvADe4+1dR1CMiItWf7imnRJUpDwRWAZ2BC8ysuDwHSLh744jqFRGRakb3lFOiWiUq6uk7RUSkhlCmnBL1jF4iIiLlUqacooxWREQkSyhTFhGRWClTTlFQFhGRWOmecoqCsoiIxKqmTQCSCQVlERGJlRaJStFALxERkSyhTFlERGKlTDlFQVlERGKloJyioCwiIrHSOK8U3VMWERHJEsqURUQkVuq+TlFQFhGRWCkopygoi4hIrBSUUxSURUQkVgrKKRroJSIikiWUKYuISKyUKacoKIuISKwUlFMUlEVEJFYKyikKyiIiEisF5RQN9BIREckSypRFRCRWiYQmvy6moCwiIrFS93WKgrKIiMRKQTlF95RFRESyhDJlERGJlTLlFAVlERGJlYJyioKyiIjESkE5RUFZRERipaCcooFeIiIiWUKZsoiIxEqZcoqCsoiIxKpIE3olKSiLiEislCmnKCiLiEisFJRTNNBLREQkSyhTFhGRWClTTlFQFhGRWCkop+RoHUsREZHsoHvKIiIiWUJBWUREJEsoKIuIiGQJBWUREZEsoaAsIiKSJRSURUREsoSCcjVlZgkzuyvt86Vmdn0Vt+FJMzu+KuuU6i38uR2V9rmOmf1iZq9WcNyBxfuYWV8zu7KC/f+zZVosUrUUlKuvdcBxZrbt5hxsZpo4RuKwCtjNzOqHnw8F5mzKCdx9nLvfVsE++2xm+0RipV/M1VcBMAz4G3B1+gYz2xEYDrQEfgFOd/dZZvYksBjoDkwysxVAR2A7oDNwMdALOJLgF+Ux7r7BzK4FjgHqA/8B/uLumnVGNtcbQB/geeAk4BlgPwAz6wncQ/CztobgZ9fTDzaz04Dfu/t5ZtYaeAT4Tbj5HHf/j5mtdPeGZpYD/IPgZzoB3OTuz5rZgcCl7n50eM4HgM/c/Ukzuw3oS/Df2FvufmlUX4RIScqUq7cHgVPMrEmJ8geAke7eFXgauC9tW2fgEHe/JPy8E8EvyH7AU8D77v47gl+IfYrP5+57uvtuBL8sj47kaqS2GA0MMLN6QFfgk7Rt3wD7u3t34FrglgrOdR/wgbt3A/YAppXYfhywO9ANOAS4w8y2K+tkZtYc+COwa/jfz02VviqRLUBBuRpz9+XASOCCEpv2Bv4Zvh8F/CFt23PuXpj2+Q133wBMBfKAN8PyqUCH8P1BZvaJmU0FegO7brGLkFrH3b8k+Nk6CXi9xOYmwHNm9hUwlIp/1noDD4fnLXT3ZSW2/wF4Jtw2H/gA2LOc8y0H1gKPmdlxwOqKr0hky1FQrv7uAc4Etilnn/Su5lUltq0DcPciYENat3QRUCfMZh4Cjg8z6EeBelui4VKrjQPuJOi6Tvd3gt6a3QhumWT6s5ZTRnkBG//+qwfg7gVAT+AF4FhSf6SKVAkF5WrO3RcDYwgCc7H/AAPC96cAH2VQRfEvxYVm1hDQaGvZEoYDN7r71BLlTUgN/DqtEud5FzgHwMzyzKxxie3/BvqH21oC+wMTgZ+ALmZWN7z9c3B4joZAE3d/HbiIoOtbpMooKNcMdwHpo7AvAE43sy+BgcCFm3tid19KkB1PBV4CPs2gnSIAuHu+u99byqZ/ALea2ccEt1MqciHB7ZWpwOf8urt7LPAl8AXwHnC5u89z99kEf8x+STDuYnK4fyPg1fC/nQ8IBlKKVBkt3SgiIpIllCmLiIhkCQVlERGRLKGgLCIikiUUlEVERLKEgrKIiEiW0NzXUmOYWSHBo1t1gOnAIHffrBmZ0udGNrO+QJeyFkEws6bAye7+0CbWcT2w0t3vLGXbqcDlBJNf5ADD3f3OcP7yV939+U2pS0SqB2XKUpOscffdw9mg1gNnp280sxwz2+Sf+UqsStQU+OumnrcsZnYkwcQVh7n7rgRzOpecPlJEaiBlylJTfQh0NbMOBKsSvU8wJ/ixZmbADUBd4AeClYhWmtkRBNOWLgQmFZ+oolWJCCZr2cnMpgBvu/tlZnYZcGJYx1h3vy4819XAqcBsghW8Pi+l7UMIsvS5AO6+lmACl42UtXqXmV1A8AdJAfC1uw8wswOA4sk6EgSLPqyo9LcpIlVCmbLUOOFa0UcSdGUDGMGqWd0J5v6+hmClrD2Az4CLwzm+HyUIcvsBbco4fWmrEl0J/BBm6ZeZ2WFAJ4I5lHcHepjZ/mbWg2D60+4EqxeVtTDCbpQerEsqa/WuK4Hu4SpHxb0FlwLnuvvu4fWtqcT5RaSKKVOWmqR+mK1CkCk/DmwP/OTuE8LyXkAX4OMgYWZr4L/ALsAMd/8OwMyeAgaXUkdvgkyXcLWtZWbWrMQ+h4Wv4qkbGxIE6UYEWfPqsI5xGV1tML3k5UADoDnBHwivEE4daWYvEUyNCvAxcLeZPQ286O75GdYtIhFQUJaaZE2YCSaFgTd9Zawcgi7mk0rstzsbr6aViRzgVnf/vxJ1XFTJOqYBPQjmai5V2updv3f32eGgseLFQ/oQLLzQF/hfM9vV3W8zs9eAo4AJZnaIu3+zidclIhFT97XUNhOAfc1sZwAza2BmnYFvgI5mtlO430llHF/aqkQrCLLgYuOBM8IVhzCztmbWimDFoj+aWX0za0TQVV6aW4F/mFmb8Pi64X3idKWu3hUOZGvv7u8TjN5uCjQ0s53cfaq7307QZb9LeV+SiMRDQVlqFXf/hWBJwGfClYAmALuEg6kGA6+Z2UcES/uV5lerErn7IoLu8K/M7A53fwv4J/DfcL/ngUbuPgl4FphCsF7vh2W08XXgQeAdM5sW1lOnxD5lrd6VBzwV1jsZGBrue1HYvi8I7ie/UflvTUSqilaJEhERyRLKlEVERLKEgrKIiEiWUFAWERHJEgrKIiIiWUJBWUREJEsoKIuIiGQJBWUREZEsoaAsIiKSJf4fi9YOWKbquxIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Fitting the PCA algorithm with our Data\n",
    "# pca = PCA().fit(data_rescaled)\n",
    "pca_ = PCA(n_components = 0.95, svd_solver = 'full').fit(train_x)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "n_coml = [pca_.n_components_]\n",
    "\n",
    "plt.plot(np.cumsum(pca_.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Variance (%)', fontsize=14) #for each component\n",
    "plt.title('Pulsar Dataset Explained Variance '+str(dsnum)+' node DS', fontsize=14)\n",
    "\n",
    "n_coml = [*n_coml]\n",
    "\n",
    "for i, v in enumerate(n_coml):\n",
    "    plt.text(v-0.8, i+0.94, '{:.0f}'.format(v), color='navy', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/PCA_components_ds'+str(dsnum)+'bal.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300, \n",
    "                             criterion='gini', \n",
    "                             max_depth=16, \n",
    "#                              min_samples_split=2, \n",
    "                             #min_samples_leaf=1, \n",
    "                             max_features=0.3, \n",
    "                             #bootstrap=True,\n",
    "                             oob_score=True,\n",
    "                             random_state=23)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 26 21:17:20 2019\n",
      "Time elapsed (hh:mm:ss.ms) 7:13:25.082207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9401    0.9918    0.9653    207840\n",
      "           1     0.9914    0.9369    0.9633    207927\n",
      "\n",
      "    accuracy                         0.9643    415767\n",
      "   macro avg     0.9657    0.9643    0.9643    415767\n",
      "weighted avg     0.9657    0.9643    0.9643    415767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(enc_train_x_asam, train_y)\n",
    "\n",
    "pred_y_ae_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(enc_test_x_asam),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=2)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_ae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cm(pred_y_ae_RF, pred_y_ae_RF, './Figures/ROC_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 27 04:30:51 2019\n",
      "Time elapsed (hh:mm:ss.ms) 6:59:41.034169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9483    0.9886    0.9680    207840\n",
      "           1     0.9881    0.9462    0.9667    207927\n",
      "\n",
      "    accuracy                         0.9674    415767\n",
      "   macro avg     0.9682    0.9674    0.9673    415767\n",
      "weighted avg     0.9682    0.9674    0.9673    415767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(enc_train_x_spsam, train_y)\n",
    "\n",
    "pred_y_spae_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(enc_test_x_spsam),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=2)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_spae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cm(pred_y_spae_RF, pred_y_spae_RF, './Figures/ROC_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with pca DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 27 11:30:35 2019\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(train_x_pca, train_y)\n",
    "\n",
    "pred_y_pca_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(test_x_pca),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=2)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_pca_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_y_pca_RF, pred_y_pca_RF, './Figures/ROC_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_ae_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_ae_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_y_ae_RF.shape)\n",
    "print(pred_y_spae_RF.shape)\n",
    "print(pred_y_pca_RF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate_ae_ann, recall_ae_ann, thresholds_ae_ann = roc_curve(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_ae_ann = auc(false_positive_rate_ae_ann, recall_ae_ann)\n",
    "false_positive_rate_sp_ann, recall_sp_ann, thresholds_sp_ann = roc_curve(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_sp_ann = auc(false_positive_rate_sp_ann, recall_sp_ann)\n",
    "false_positive_rate_nodr_ann, recall_nodr_ann, thresholds_nodr_ann = roc_curve(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_nodr_ann = auc(false_positive_rate_nodr_ann, recall_nodr_ann)\n",
    "\n",
    "false_positive_rate_ae_RF, recall_ae_RF, thresholds_ae_RF = roc_curve(test_y, pred_y_ae_RF)\n",
    "roc_auc_ae_RF = auc(false_positive_rate_ae_RF, recall_ae_RF)\n",
    "false_positive_rate_spae_RF, recall_spae_RF, thresholds_spae_RF = roc_curve(test_y, pred_y_spae_RF)\n",
    "roc_auc_spae_RF = auc(false_positive_rate_spae_RF, recall_spae_RF)\n",
    "false_positive_rate_pca_RF, recall_pca_RF, thresholds_pca_RF = roc_curve(test_y, pred_y_pca_RF)\n",
    "roc_auc_pca_RF = auc(false_positive_rate_pca_RF, recall_pca_RF)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC)', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "# plt.ylim([0.97,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC) Zoom in', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "# plt.ylim([0.0,1.0])\n",
    "plt.ylim([0.955,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal_zoom.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "# labels = ['Normal', 'Malicious']\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=sns.light_palette(\"purple\"), vmin = 0.2);\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('True Class')\n",
    "# plt.xlabel('Predicted Class')\n",
    "# plt.savefig('./Figures/CM_ae_ann_thirdds'+str(dsnum)+'bal_TEST.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ae_ann = \"AE+DNN\"\n",
    "acc_ae_ann = (sm.accuracy_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_ae_ann = (sm.precision_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_ae_ann = (sm.recall_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_ae_ann = (sm.f1_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_sp_ann = \"SAE+DNN\"\n",
    "acc_sp_ann = (sm.accuracy_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_sp_ann = (sm.precision_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_sp_ann = (sm.recall_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_sp_ann = (sm.f1_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_nodr_ann = \"DNN\"\n",
    "acc_nodr_ann = (sm.accuracy_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_nodr_ann = (sm.precision_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_nodr_ann = (sm.recall_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_nodr_ann = (sm.f1_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_ae_RF = \"AE+RF\"\n",
    "acc_ae_RF = (sm.accuracy_score(test_y, pred_y_ae_RF)*100) \n",
    "pre_ae_RF = (sm.precision_score(test_y, pred_y_ae_RF)*100) \n",
    "recall_ae_RF = (sm.recall_score(test_y, pred_y_ae_RF)*100) \n",
    "f1score_ae_RF = (sm.f1_score(test_y, pred_y_ae_RF)*100)\n",
    "\n",
    "classi_spae_RF = \"SAE+RF\"\n",
    "acc_spae_RF = (sm.accuracy_score(test_y, pred_y_spae_RF)*100) \n",
    "pre_spae_RF = (sm.precision_score(test_y, pred_y_spae_RF)*100) \n",
    "recall_spae_RF = (sm.recall_score(test_y, pred_y_spae_RF)*100) \n",
    "f1score_spae_RF = (sm.f1_score(test_y, pred_y_spae_RF)*100)\n",
    "\n",
    "classi_pca_RF = \"PCA+RF\"\n",
    "acc_pca_RF = (sm.accuracy_score(test_y, pred_y_pca_RF)*100) \n",
    "pre_pca_RF = (sm.precision_score(test_y, pred_y_pca_RF)*100) \n",
    "recall_pca_RF = (sm.recall_score(test_y, pred_y_pca_RF)*100) \n",
    "f1score_pca_RF = (sm.f1_score(test_y, pred_y_pca_RF)*100)\n",
    "\n",
    "\n",
    "print('Classifier\\tAcc\\tPreci\\tRecall\\tF1Score')\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_ann, acc_ae_ann, pre_ae_ann, recall_ae_ann, f1score_ae_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_sp_ann, acc_sp_ann, pre_sp_ann, recall_sp_ann, f1score_sp_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_nodr_ann, acc_nodr_ann, pre_nodr_ann, recall_nodr_ann, f1score_nodr_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_RF, acc_ae_RF, pre_ae_RF, recall_ae_RF, f1score_ae_RF))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_spae_RF, acc_spae_RF, pre_spae_RF, recall_spae_RF, f1score_spae_RF))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_pca_RF, acc_pca_RF, pre_pca_RF, recall_pca_RF, f1score_pca_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1list = [[\"AE+DNN\",f1score_ae_ann],[\"SAE+DNN\",f1score_sp_ann],[\"DNN\",f1score_nodr_ann],\n",
    "          [\"AE+RF\",f1score_ae_RF],[\"SAE+RF\",f1score_spae_RF],[\"PCA+RF\",f1score_pca_RF]]\n",
    "\n",
    "xs, ys = [*zip(*f1list)]\n",
    "\n",
    "'{:.2f}'.format(f1score_ae_ann)\n",
    "\n",
    "plt.figure(figsize=(8,6), )\n",
    "plt.barh(xs, ys, color = \"purple\")\n",
    "plt.title(\"F1 score vs Classifier\", fontsize=16)\n",
    "plt.xlabel(\"Classifier\", fontsize=14)\n",
    "plt.ylabel(\"F1 score\", fontsize=14)\n",
    "plt.xticks(np.arange(0, 101, 10), fontsize=12)\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "for i, v in enumerate(ys):\n",
    "    plt.text(v+1, i+0.1, '{:.2f}'.format(v), color='purple', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/F1scoreplot_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
