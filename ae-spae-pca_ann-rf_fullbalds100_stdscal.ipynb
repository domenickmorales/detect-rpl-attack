{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Donâ€™t pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "from keras import optimizers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Activation, Dense, Dropout, Input\n",
    "from keras import backend as k\n",
    "\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "#k.tensorflow_backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------Import modules------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(23)\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from datetime import datetime \n",
    "import os.path\n",
    "\n",
    "dsnum=100\n",
    "verbose_level=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/01Code/00Datasets_final/00BalancedDS/FullCloneID100bal_stdscal.csv\n"
     ]
    }
   ],
   "source": [
    "pathds = os.path.abspath('/home/user/01Code/00Datasets_final/00BalancedDS')\n",
    "file_name = \"FullCloneID\"+str(dsnum)+\"bal_stdscal.csv\"\n",
    "full_path = os.path.join(pathds,file_name)\n",
    "print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2078832, 211)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "neurons=df.shape[1]-1\n",
    "batch_size=df.shape[1]-1\n",
    "print(neurons)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Explaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1039416\n",
      "Class 1: 1039416\n",
      "Proportion: 1.0 : 1\n"
     ]
    }
   ],
   "source": [
    "#if you don't have an intuitive sense of how imbalanced these two classes are, let's go visual\n",
    "count_classes = pd.value_counts(df['class'], sort = True)\n",
    "print('Class 0:', count_classes[0])\n",
    "print('Class 1:', count_classes[1])\n",
    "print('Proportion:', round(count_classes[0] / count_classes[1], 3), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEWCAYAAADPZygPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYXFWd//F3k2DAYUnYMUECEj8GkC0IcRl2IaxBRvYlLIqDKBHcAoOgwDhxYYmj4g8hkCASAihkIAgRWWaGRWgEkWm/EiGQQGRLgECggdC/P84pU7TV1ZXurr7dXZ/X8/RT95577j3nVt2ub51zz723qa2tDTMzsyKsVHQFzMyscTkImZlZYRyEzMysMA5CZmZWGAchMzMrjIOQmZkVxkHIbAVIapO0WZ3LGJnLGVzPcnqapCMl3VZ0PbpD0i6SFhRdj0bSrw5yK4akecD6wLKy5A9HxLOFVMgKJ2kk8CSwckS8AxARVwFXFVkv638chKxW+0fEb6tlkDS49IVkfZukJqApIt4tui4Dlf8fauMgZF1W9mv4c8DZwDxgJ0ljgQuAzYGngIkRcWdeZxPgCmA74D4ggKERcZSkXYBfRMSIsjLmAZ+LiN9KWgn4BvB5YChwO/CvEbGorC7HAucC7wcujIh/z9sZBHwTOAFYD/gLcCAwCXgzIr5aVuZ/AbdHxEUd7Po+kr4CrAFcnre7MrAQ2DkiHs3bWS/v/wcj4oV2791KwBl5X1YFfgN8OSJeKct2vKRvA03ADyPi/LzuDsBPgQ8DbwBXRcRpeVm19/5O4H+BXfL7/++SDoyI7cvqdSqwa0QcIGlf4DzgQ8ArwGUR8e2c9e78+rIkgE8DIn1Wn8rb+gQwJdfzL7ku95TV5b+B3YCtgHuBIyLixfZvdum4AC7M7/Uy4IyIuLxsW7+IiEvz/LHt6tEGnAycCmwAXEQ6Bn8BbJHf+6Mi4q2yMs8ATgNeA/4tt/KQNAT4d+AQYAjwa+DUiHijrJ7/mcuaAxzdfn/svXxOyHrCzsBoYC9Jw4GbSV9eawFfA66XtG7O+0ugGViHFCwmrEA5p5ACx87AB4DFwE/a5fkU6ctwd+AsSaNz+mnA4cA+pOBxPLAUmAYcnoMCktbJ615dpR6fAbYnfZGPB46PiFZgBnBUWb7Dgd+2D0DZsflvV2BTYDXgx+3y7AqMAvYEJknaI6dPAaZExBqkADEz172z9x7Sl+KJwOqkL0tJGlW2/AjSZwTwOnAMKeDvC5wk6cC8bKf8OjQiVouIe8srLmmtXJcfAWuTAuPNktZuV9ZxpB8F78v17cgGwJrAcNIPiZ9IGlYlf3vjgDHAWNIPmUuAI4GNgC1Jn1V5WevksiYAlyhHWuB7pKC6DbBZznNWu3XXAjYmvc/WCbeErFY3SCp1LdwZEQeWLft2RLwOIOkoYHZEzM7L5kh6kNR6uAP4GLBH/tK+O7c6avUF4EsRsSCX9W3gaUnlvza/ExFvAI9IegTYGmghtda+ERGR8z2SX1+S9Aop8MwBDsv791yVenwvIhYBiyRdRPoCu5QU0K6TdHru5joa+H4H2zgSuCAinsj7cjrwJ0nHtduX14FHJV2ey/kt8DawmaR1csvhvpy/w/c+1w3gioh4LE+/IunGvN1zcjD6CDALoNSCyv4o6WrSD4Abqrw3JfsCj0fElXn+akmnAPuTWiEAl0fEX/L+zwQOqLK9t4FzcvfWbEmvkX5s3FdlnXLfi4hXgcck/Qm4rey9vwXYluXvEcC38jF6l6SbgUMknUdquW6VP38kfZcUtE/P670LnJ3XtRo4CFmtDqxyTmh+2fTGwMGS9i9LWxm4g9x6KQWs7CnSr9FabAz8WlL5eYxlpEETJX8rm15KamGQy/hrB9udRvoCn5Nfp3RSj/L9fYq0X0TE/ZJeB3aWtJD0S3lWB9v4QF63fDuD2+1L+3I+mqdPAM4B/izpSVKwuonq732lbUL6Aj0/b+8I4IaIWAogaUdgMqml8D5S99O1HexPZ/tX2ofhZfMdfVaVvNTu/Epn+dsr/1HxRoX5DcrmKx2jHwDWJXXzNi9vGNEEDCrL+0JEvLkC9Wp4DkLWE8pvxT4fuDIiPt8+k6SNgWGS/qnsn/yDZeu/TvonL+UfRPrHL9/28RHxvxW2PbKTOs4ndV39qcKyX5BaIVuTuhU7+6W/EVBqTXwQKB8lWApofwOuq/KF9CwpaJR8EHiH9OVYOie2EfDn9uVExOMs70I8iNT6Wpsq732Z9rfNvw1YR9I2pBbRqWXLfknqItw7It7Mrb51OthOZ/tX2offdLJeV7znuOG9AaUrKh2jfwJeJAWsLSLimQ7W9WMJVpDPCVlP+wWwv6S9JA2StEq+9mJERDwFPAh8R9L7JH2K1D1T8hdgFUn7SloZOJP067vkZ6ST6RsDSFpX0vga63UpcK6kUZKaJG1VOj+Ru/ceAK4Ers/dedV8XdIwSRsBE4FrypZdSTpndBQwvco2rgZOlbSJpNWA7wLXtPu1/y1J75e0BencyTV5v4+StG7u8ns5511Glfe+o0rk8q4DfkA6lzGnbPHqwKIcgHYgtZRKXiB1PW3awaZnAx+WdISkwZIOJQ2WuKnKe9JVDwMH5fdqM1JLsbtKx+g/A/sB1+b3++fAhXnQCZKGS9qrB8prWA5C1qMiYj7pZP0ZpC+q+cDXWX6sHQHsCCwijaibXrbuK8AXSQHjGdIv3PILB6eQurduk7SEdD5gxxqrdgHpBP5twKvAZaRRaSXTSN1dV/7jqv/gRtLgiodJJ98vK9uHBcBDpF/E/11lG1NzWXeTRvW9CXy5XZ67gLmkUYA/jIjShaDjSOc2XiO9J4dFxJs1vPcd+SWwB+mLtjwIfpF0rmgJ6eT7zLL9XEoaJfa/kl7Oo/IoW/4S6cv7q8BLpMEA+1Ua/dYDLgTeIrUip9H9a5X+Rhr08mze1r9GRKlF+k3SZ3KfpFdJ5+hUcStWkyY/1M6KlAcXbBYRR3WWt8712InUkhjZ3WtnJE0Fno2IM3ukcmYDmM8JWcPLXX8TgUt7IACNJJ2n2bYHqmY24Lk7zhpavo7oZWBD0kWM3dnWuaQT2D+IiCd7oHpmA56748zMrDBuCZmZWWF8TqgTDz/8cNuQIUM6z2g1aW1txe+n9UU+NnvW0qVLXxwzZsy6neVzEOrEkCFDGD16dOcZrSYtLS1+P61P8rHZs5qbm9vfMaMid8eZmVlhHITMzKwwDkJmZlYYByEzMyuMg5CZmRXGQcjMzArjIGRmZoVxEDIzs8I4CJmZWWEchAaIN99eVnQVatJfrkjvL+9nf9Bf3ksfm8XwbXsGiFVWHsTISTcXXY0BY97kfYuuwoDhY7NnDbRj0y0hMzMrjIOQmZkVpm7dcZKmAvsBz0fEljltLeAaYCQwDzgkIhZLagKmAPsAS4FjI+KhvM4E4My82fMiYlpOHwNcAawKzAYmRkRbV8owM7Ni1LMldAUwrl3aJOD2iBgF3J7nAfYGRuW/E4GL4e9B62xgR2AH4GxJw/I6F+e8pfXGdaUMMzMrTt2CUETcDSxqlzwemJanpwEHlqVPj4i2iLgPGCppQ2AvYE5ELIqIxcAcYFxetkZE3BsRbcD0dttakTLMzKwgvX1OaP2IWAiQX9fL6cOB+WX5FuS0aukLKqR3pQwzMytIXxmi3VQhra0L6V0po6rW1lZaWlo6y1a4/nKNQ3/SHz73/sDHZs8bSMdmbweh5yRtGBELc1fY8zl9AbBRWb4RwLM5fZd26Xfm9BEV8neljKr8eO/G5c/d+qr+cGw2NzfXlK+3u+NmARPy9ATgxrL0YyQ1SRoLvJK70m4F9pQ0LA9I2BO4NS9bImlsHvV2TLttrUgZZmZWkHoO0b6a1IpZR9IC0ii3ycBMSScATwMH5+yzSUOn55KGTx8HEBGLJJ0LPJDznRMRpcEOJ7F8iPYt+Y8VLcPMzIpTtyAUEYd3sGj3CnnbgJM72M5UYGqF9AeBLSukv7SiZZiZWTF8xwQzMyuMg5CZmRXGQcjMzArjIGRmZoVxEDIzs8I4CJmZWWEchMzMrDAOQmZmVhgHITMzK4yDkJmZFcZByMzMCuMgZGZmhXEQMjOzwjgImZlZYRyEzMysMA5CZmZWGAchMzMrjIOQmZkVxkHIzMwK4yBkZmaFcRAyM7PCOAiZmVlhHITMzKwwDkJmZlYYByEzMyuMg5CZmRVmhYKQpGGStqpXZczMrLEM7iyDpDuBA3Leh4EXJN0VEafVuW5mZjbA1dISWjMiXgUOAi6PiDHAHvWtlpmZNYJagtBgSRsChwA31bk+ZmbWQGoJQucAtwJzI+IBSZsCj9e3WmZm1gg6PScUEdcC15bNPwH8S3cKlXQq8DmgDXgUOA7YEJgBrAU8BBwdEW9JGgJMB8YALwGHRsS8vJ3TgROAZcApEXFrTh8HTAEGAZdGxOScvkmlMrqzL2Zm1nWdtoQkrSvpDEmXSJpa+utqgZKGA6cA20fElqRAcRjwPeDCiBgFLCYFF/Lr4ojYDLgw50PS5nm9LYBxwE8lDZI0CPgJsDewOXB4zkuVMszMrAC1dMfdCKwJ/Ba4ueyvOwYDq0oaDLwfWAjsBlyXl08DDszT4/M8efnukppy+oyIaI2IJ4G5wA75b25EPJFbOTOA8XmdjsowM7MCdNodB7w/Ir7ZUwVGxDOSfgg8DbwB3AY0Ay9HxDs52wJgeJ4eDszP674j6RVg7Zx+X9mmy9eZ3y59x7xOR2V0qLW1lZaWlhXaxyKMHj266CoMOP3hc+8PfGz2vIF0bNYShG6StE9EzO6JAiUNI7ViNgFeJp1v2rtC1rb82tTBso7SK7XuquWvasiQIf4nalD+3K2v6g/HZnNzc035aumOm0gKRG9KWpL/Xu1G3fYAnoyIFyLibeBXwCeAobl7DmAE8GyeXgBsBJCXrwksKk9vt05H6S9WKcPMzApQy+i41Xu4zKeBsZLeT+qO2x14ELgD+CzpHM4E0rkogFl5/t68/HcR0SZpFvBLSRcAHwBGAb8ntXhG5ZFwz5AGLxyR1+moDDMzK0At3XFIOgDYKc/eGRFdvmg1Iu6XdB1piPQ7wB+AS0iDHWZIOi+nXZZXuQy4UtJcUgvosLydxyTNBP4vb+fkiFiW6/sl0rVNg4CpEfFY3tY3OyjDzMwK0NTWVv20iKTJwMeAq3LS4UBzREyqc936hJaWlrb+0P8KMHJSdwctWsm8yfsWXYUBxcdmz+kvx2Zzc3PzmDFjtu8sXy0toX2AbSLiXQBJ00itiIYIQmZmVj+1PsphaNn0mvWoiJmZNZ5aWkL/Afwhn9RvIp0bOr2utTIzs4bQaUsoIq4GxpKGUv8K+HhEzKh3xczMbODrMAhJ+kh+3Y50c9EFpDsRfCCnmZmZdUu17rjTgBOB8yssayPdh83MzKzLOgxCEXFintw7It4sXyZplbrWyszMGkIto+PuqTHNzMxshXTYEpK0Aeku06tK2pblNwBdg/T4BTMzs26pdk5oL+BY0o0+LyhLXwKcUcc6mZlZg6h2TmgaME3Sv0TE9b1YJzMzaxC13EX7ekn7kh6jvUpZ+jn1rJiZmQ18nQ5MkPQz4FDgy6TzQgcDG9e5XmZm1gBqGR33iYg4BlgcEd8BPs57HxpnZmbWJbUEoTfy61JJHwDeJj2a28zMrFtquYHpTZKGAj8gPYiuDfh5XWtlZmYNoZaBCefmyesl3QSsEhGv1LdaZmbWCDoNQpIeAa4BromIvwKtda+VmZk1hFq64w4gjY6bKeldUkCaGRFP17VmZmY24NXyPKGnIuL7ETEGOALYCniy7jUzM7MBr5aWEJJGAoeQWkTLgG/UsU5mZtYgajkndD+wMjATODginqh7rczMrCFUDUKSVgJ+HRGTe6k+ZmbWQKqeE4qId4F9eqkuZmbWYGo5JzRH0tdIo+JeLyVGxKK61crMzBpCLUHo+Px6cllaG7Bpz1fHzMwaSS13TPB94szMrC5qGR33fuA04IMRcaKkUYAi4qa6187MzAa0Wu6ifTnwFvCJPL8AOK9uNTIzs4ZRSxD6UER8n/QIByLiDdLD7czMzLqlliD0lqRVSYMRkPQhfBNTMzPrAbWMjjsb+A2wkaSrgE8Cx3an0Px8okuBLUnB7XggSMPARwLzgEMiYrGkJmAK6XqlpcCxEfFQ3s4E4My82fMiYlpOHwNcAawKzAYmRkSbpLUqldGdfTEzs66r5Qamc4CDSIHnamD7iLizm+VOAX4TER8BtgZagEnA7RExCrg9zwPsDYzKfycCFwPkgHI2sCOwA3C2pGF5nYtz3tJ643J6R2WYmVkBOg1Ckj4JvBkRNwNDgTMkbdzVAiWtAewEXAYQEW9FxMvAeGBazjYNODBPjwemR0RbRNwHDJW0IbAXMCciFuXWzBxgXF62RkTcGxFtwPR226pUhpmZFaCW7riLga0lbQ18HZhK+mLfuYtlbgq8AFyet9kMTATWj4iFABGxUNJ6Of9wYH7Z+gtyWrX0BRXSqVJGh1pbW2lpaVmxPSzA6NGji67CgNMfPvf+wMdmzxtIx2YtQeidfD5lPPCjiLgsn4vpTpnbAV+OiPslTaF6t1ilkXhtXUjvkiFDhvifqEH5c7e+qj8cm83NzTXlq2V03BJJpwNHAzdLGkR6tENXLQAWRMT9ef46UlB6LnelkV+fL8u/Udn6I4BnO0kfUSGdKmWYmVkBaglCh5KGZB8fEX8jdW39oKsF5m3Ml6SctDvwf8AsoNTCmgDcmKdnAcdIapI0Fngld6ndCuwpaVgekLAncGtetkTS2Dyy7ph226pUhpmZFaCW0XF/A34JDJO0P/BWREzvZrlfBq6S9EdgG+C7wGTg05IeBz6d5yENsX4CmAv8HPhirtci4Fzggfx3TtmdvU8iDQGfC/wVuCWnd1SGmZkVoJZ7x30OOAv4Hel8y39KOicipna10Ih4GNi+wqLdK+Rt47138C5fNpU0UKJ9+oOka5Dap79UqQwzMytGLQMTvg5sm7/AkbQ2cA8VvvzNzMxWRC3nhBYAS8rml/DeodFmZmZd0mFLSNJpefIZ4H5JN5KGOo8Hft8LdTMzswGuWnfc6vn1r/mvxCPKzMysR3QYhCLiO6VpSasBbRHxeq/UyszMGkLVc0KSTpL0NPAU8LSkpyR9sXeqZmZmA12HQUjSmcD+wC4RsXZErA3sCuydl5mZmXVLtZbQ0cBBEfFEKSFPH0K6C4GZmVm3VO2Oi4g3K6S9AbxbtxqZmVnDqBaEFkj6h7sLSNoNWFi/KpmZWaOoNkT7FOBGSf9DeuZPG/Ax0uO9x/dC3czMbIDrsCUUEY+R7r92NzCS9DC6u4Et8zIzM7NuqXrvuHxOyPeIMzOzuqjl3nFmZmZ14SBkZmaFqXax6u359Xu9Vx0zM2sk1c4JbShpZ+AASTNID7T7u4h4qK41MzOzAa9aEDoLmASMAC5ot6wN2K1elTIzs8ZQ7S7a1wHXSfpWRJzbi3UyM7MG0enjvSPiXEkHADvlpDsj4qb6VsvMzBpBp6PjJP0HMBH4v/w3MaeZmZl1S6ctIWBfYJuIeBdA0jTgD8Dp9ayYmZkNfLVeJzS0bHrNelTEzMwaTy0tof8A/iDpDtIw7Z1wK8jMzHpApy2hiLgaGAv8Kv99PCJm1LtiZmY28NXSEiIiFgKz6lwXMzNrML53nJmZFcZByMzMClM1CElaSdKfeqsyZmbWWKoGoXxt0COSPthL9TEzswZSy8CEDYHHJP0eeL2UGBEH1K1WZmbWEGoJQt+pR8GSBgEPAs9ExH6SNgFmAGsBDwFHR8RbkoYA04ExwEvAoRExL2/jdOAEYBlwSkTcmtPHAVOAQcClETE5p1csox77Z2ZmnavlOqG7gHnAynn6AdIXeHdNBFrK5r8HXBgRo4DFpOBCfl0cEZsBF+Z8SNocOAzYAhgH/FTSoBzcfgLsDWwOHJ7zVivDzMwKUMsNTD8PXAf8v5w0HLihO4VKGkG6J92leb6J9Hyi63KWacCBeXp8nicv3z3nHw/MiIjWiHgSmAvskP/mRsQTuZUzAxjfSRlmZlaAWrrjTiZ9sd8PEBGPS1qvm+VeBHwDWD3Prw28HBHv5PkFpGBHfp2fy35H0is5/3DgvrJtlq8zv136jp2U0aHW1lZaWlo6y1a40aNHF12FAac/fO79gY/NnjeQjs1aglBrPjcDgKTBpCerdomk/YDnI6JZ0i45ualC1rZOlnWUXql1Vy1/VUOGDPE/UYPy5259VX84Npubm2vKV8vFqndJOgNYVdKngWuB/+pG3T4JHCBpHqmrbDdSy2hoDnCQHin+bJ5eAGwEfw+AawKLytPbrdNR+otVyjAzswLUEoQmAS8AjwJfAGYDZ3a1wIg4PSJGRMRI0sCC30XEkcAdwGdztgnAjXl6Vp4nL/9dRLTl9MMkDcmj3kYBvycNnBglaRNJ78tlzMrrdFSGmZkVoJbRce+STuKfSxquPS1/ofe0bwKnSZpLOn9zWU6/DFg7p59GCopExGPATNLTXn8DnBwRy/I5ny8Bt5JG383MeauVYWZmBWhqa6seTyTtC/wM+CvpvMomwBci4pb6V694LS0tbf2h/xVg5KSbi67CgDFv8r5FV2FA8bHZc/rLsdnc3Nw8ZsyY7TvLV8vAhPOBXSNiLoCkDwE3Aw0RhMzMrH5qOSf0fCkAZU8Az9epPmZm1kA6bAlJOihPPiZpNun8SxtwMOnkv5mZWbdU647bv2z6OWDnPP0CMKxuNTIzs4bRYRCKiON6syJmZtZ4Oh2YkK/B+TIwsjy/H+VgZmbdVcvouBtI19P8F/BufatjZmaNpJYg9GZE/KjuNTEzs4ZTSxCaIuls4DagtZQYET3xTCEzM2tgtQShjwJHk240WuqOa8vzZmZmXVZLEPoMsKkfg21mZj2tljsmPAIMrXdFzMys8dTSElof+LOkB3jvOSEP0TYzs26pJQidXfdamJlZQ+o0CEXEXb1RETMzazy13DFhCWk0HMD7gJWB1yNijXpWzMzMBr5aWkKrl89LOhDYoW41MjOzhlHL6Lj3iIgb8DVCZmbWA2rpjjuobHYlYHuWd8+ZmZl1WS2j48qfK/QOMA8YX5famJlZQ6nlnJCfK2RmZnVR7fHeZ1VZry0izq1DfczMrIFUawm9XiHtn4ATgLUBByEzM+uWao/3Pr80LWl1YCJwHDADOL+j9czMzGpV9ZyQpLWA04AjgWnAdhGxuDcqZmZmA1+1c0I/AA4CLgE+GhGv9VqtzMysIVRrCX2VdNfsM4F/k1RKbyINTPBte8zMrFuqnRNa4bspmJmZrQgHGjMzK4yDkJmZFcZByMzMClPLveN6lKSNgOnABsC7wCURMSUPB78GGEm6P90hEbFYUhMwBdgHWAocGxEP5W1NIA2cADgvIqbl9DHAFcCqwGxgYkS0dVRGnXfZzMw6UERL6B3gqxExGhgLnCxpc2AScHtEjAJuz/MAewOj8t+JwMXw92uYzgZ2JD3f6GxJw/I6F+e8pfXG5fSOyjAzswL0ehCKiIWllkxELAFagOGkO3NPy9mmAQfm6fHA9Ihoi4j7gKGSNgT2AuZExKLcmpkDjMvL1oiIeyOijdTqKt9WpTLMzKwAvd4dV07SSGBb4H5g/YhYCClQSVovZxsOzC9bbUFOq5a+oEI6VcroUGtrKy0tLSu4Z71v9OjRRVdhwOkPn3t/4GOz5w2kY7OwICRpNeB64CsR8WrZxbDtNVVIa+tCepcMGTLE/0QNyp+79VX94dhsbm6uKV8ho+MkrUwKQFdFxK9y8nO5K438+nxOXwBsVLb6CODZTtJHVEivVoaZmRWg14NQHu12GdASEReULZoFTMjTE4Aby9KPkdQkaSzwSu5SuxXYU9KwPCBhT+DWvGyJpLG5rGPabatSGWZmVoAiuuM+CRwNPCrp4Zx2BjAZmCnpBOBp4OC8bDZpePZc0hDt4wAiYpGkc4EHcr5zImJRnj6J5UO0b8l/VCnDzMwK0OtBKCL+h8rnbQB2r5C/DTi5g21NBaZWSH8Q2LJC+kuVyjAzs2L4jglmZlYYByEzMyuMg5CZmRXGQcjMzArjIGRmZoVxEDIzs8I4CJmZWWEchMzMrDAOQmZmVhgHITMzK4yDkJmZFcZByMzMCuMgZGZmhXEQMjOzwjgImZlZYRyEzMysMA5CZmZWGAchMzMrjIOQmZkVxkHIzMwK4yBkZmaFcRAyM7PCOAiZmVlhHITMzKwwDkJmZlYYByEzMyuMg5CZmRXGQcjMzArjIGRmZoVxEDIzs8I4CJmZWWEGF12B3iZpHDAFGARcGhGTC66SmVnDaqiWkKRBwE+AvYHNgcMlbV5srczMGldDBSFgB2BuRDwREW8BM4DxBdfJzKxhNVp33HBgftn8AmDHaissXbr0xebm5qfqWqsecv3BGxRdhQGjubm56CoMKD42e04/OjY3riVTowWhpgppbdVWGDNmzLp1qouZWcNrtO64BcBGZfMjgGcLqouZWcNrtJbQA8AoSZsAzwCHAUcUWyUzs8bVUC2hiHgH+BJwK9ACzIyIx4qtlZlZ42pqa6t6SsTMzKxuGqolZGZmfYuDkJmZFcZByMzMCuMgZABIapN0ftn81yR9u5frcIWkz1ZIv1NSSDogz68laY6kx/PrsJx+qKS5km7qzXpbz8nH4ZVl84MlvdDZZyppl1IeSQdImtRJ/nt6psYVt/1tSc9IOifPf0TSvZJaJX2tLN+qkh6W9JakdepVn77OQchKWoGDuvrPIKnew/2PjIhZeXoScHtEjAJuz/NExDXA5+pcD6uv14EtJa2a5z9NupyiZhExq7MbE0fEJ7pYv1pdGBFn5elFwCnAD9vV4Y2I2IYGv1ax0a4Tso69A1wCnAr8W/kCSRsDU4F1gReA4yLiaUlXkP7BtgUekrQE2ATYEPgwcBowlnTD2GeA/SPibUlnAfsDqwL3AF+IiBUZpjke2CVPTwPuBL65YrtrfdgtwL7AdcDhwNXAPwNI2gG4iHTsvEE6FqN8ZUnHAttHxJckrQ/8DNg0Lz4pIu6R9FpErCapCfg+6RhtA86LiGsk7QKQChl/AAAEjklEQVR8LSL2y9v8MfBgRFwhaTJwAOl/5raI+BpVRMTzwPOS9u3WuzJAuSVk5X4CHClpzXbpPwamR8RWwFXAj8qWfRjYIyK+muc/RPoCGQ/8ArgjIj5K+sIo/RP+OCI+FhFbkr5M9lvBeq4fEQsB8ut6K7i+9W0zgMMkrQJsBdxftuzPwE4RsS1wFvDdTrb1I+CuiNga2A5of13gQcA2wNbAHsAPJG3Y0cYkrQV8Btgi/z+cV/NeWUUOQvZ3EfEqMJ3UdVDu48Av8/SVwKfKll0bEcvK5m+JiLeBR0nPbPpNTn8UGJmnd5V0v6RHgd2ALXpsJ6zfi4g/ko6Vw4HZ7RavCVwr6U/AhXR+7OwGXJy3uywiXmm3/FPA1XnZc8BdwMeqbO9V4E3gUkkHAUs73yOrxkHI2rsIOAH4pyp5yrvOXm+3rBUgIt4F3i7rZnsXGJx/3f4U+GxuIf0cWGUF6/hc6ddqfn1+Bde3vm8W6RzK1e3SzyW1rrckdemu6LHTXqWbGkPqaiv/flwF/n7XlR2A64EDWf4jy7rIQcjeIyIWATNJgajkHtJ99gCOBP6nG0WUvjRelLQa8A+j4WowC5iQpycAN3ajPtY3TQXOiYhH26WvyfKBCsfWsJ3bgZMgPdRS0hrtlt8NHJqXrQvsBPweeArYXNKQ3D29e97GasCaETEb+AqpK8+6wQMTrJLzSffYKzkFmCrp6+SBCV3dcES8LOnnpO65eaSbyq6oycBMSScATwMHd7U+1jdFxAJgSoVF3wemSToN+F0Nm5oIXJKPlWWkgHRv2fJfk7qbHyG18L8REX8DkDQT+CPwOPCHnH914Mbcom8iDeSpStIGwIPAGsC7kr4CbJ67vxue7x1nfZ6kO0kjlR6sIe8ulI1qMutt+fq61yLih53lzfnnkUbzvVjHavVZ7o6z/mARcEXpYtWOSDqUdL5pca/Uyqyy14ATSxerdqR0sSqwMumcaUNyS8jMzArjlpCZmRXGQcjMzArj0XFmfUgeSXUR6YLJVtIIwq8Av8rXxpgNKA5CZn1Evo/Zr4FpEXFYTtsGWL/QipnVkYOQWd+xK+kuEz8rJUTEw5JGlubz9JUsv6PFl/INOTcEriFdizKYdD3MPcBlwPaka2CmRsSFvbAfZjXzOSGzvmNLoLmTPM8Dn46I7YBDWX4z2SOAW/OjAbYGHiZdzT88IrbMt0i6vD7VNus6t4TM+peVgR/nbrplpLuYQ7rzxFRJKwM35BbUE8Cmkv4TuBm4rZAam1XhlpBZ3/EYMKaTPKcCz5FaO9sD7wOIiLtJ9z17BrhS0jERsTjnuxM4Gbi0PtU26zoHIbO+43fAEEmfLyVI+hiwcVmeNYGF+S7lR5Mel1F68ODzEfFz0nmg7fJTcleKiOuBb5Gep2PWp7g7zqyPiIg2SZ8BLpI0ifTcmnmkIdolPwWul3QwcAfLH6WxC/B1SW+TbhtzDDAcuFxS6cfm6XXfCbMV5Nv2mJlZYdwdZ2ZmhXEQMjOzwjgImZlZYRyEzMysMA5CZmZWGAchMzMrjIOQmZkV5v8DbFkups4ZOfgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.xticks(range(2), ['Normal [0]','Malicious [1]'])\n",
    "plt.title(\"Frequency by observation number\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Observations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed and percentage of test data\n",
    "RANDOM_SEED = 23 #used to help randomly select the data points\n",
    "TEST_PCT = 0.20 # 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_df = train_test_split(df, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ df -> original dataset \n",
    "+ train -> subset of 80% from original dataset \n",
    "+ test_df -> subset of 20% from original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(train, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train -> subset of 80% from original dataset \n",
    "+ train_df -> subset of 80% from train\n",
    "+ dev_df -> subset of 20% from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49990228884619664\n",
      "0.500260061993969\n",
      "0.5001046259082611\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of mal samples in train and test set\n",
    "print(train_df.iloc[:, batch_size].sum()/train_df.shape[0]) \n",
    "print(dev_df.iloc[:, batch_size].sum()/dev_df.shape[0]) \n",
    "print(test_df.iloc[:, batch_size].sum()/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.iloc[:, :batch_size] \n",
    "dev_x = dev_df.iloc[:, :batch_size] \n",
    "test_x = test_df.iloc[:, :batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_x -> features of train_df **Training subset for AE**\n",
    "+ dev_x -> features of dev_df **Validation subset for AE**\n",
    "+ test_x -> features of test_df **Testing subset for ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final train and test sets\n",
    "train_y = train_df.iloc[:,batch_size]\n",
    "dev_y = dev_df.iloc[:,batch_size]\n",
    "test_y = test_df.iloc[:,batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ train_y -> **Labels for supervised training of ANN**\n",
    "+ dev_y -> labels of dev_df  *not used for AE neither ANN*\n",
    "+ test_y -> labels of test_df  **Ground Truth for predictions of supervised ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "train_x =np.array(train_x)\n",
    "dev_x =np.array(dev_x)\n",
    "test_x = np.array(test_x)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "dev_y = np.array(dev_y)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "print(train_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(factor_enc_dim, enc_activation, dec_activation, \n",
    "                optimizer, loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer #RELU\n",
    "    encoded = Dense(encoding_dim, activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer #SIMOID\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spae(factor_enc_dim,dec_activation,enc_activation,\n",
    "         optimizer,loss):\n",
    "\n",
    "    encoding_dim = int(int(train_x.shape[1])/factor_enc_dim)\n",
    "    ### Define input layer\n",
    "    input_data = Input(shape=(train_x.shape[1],))\n",
    "    ### Define encoding layer\n",
    "    encoded = Dense(encoding_dim, activity_regularizer=regularizers.l1(1e-4), activation=enc_activation, name='encoded_bottle_neck')(input_data)\n",
    "    ### Define decoding layer\n",
    "    decoded = Dense(train_x.shape[1], activation=dec_activation)(encoded)\n",
    "    ### Create the autoencoder model\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    encoded_train_x = encoder.predict(train_x)\n",
    "    encoded_test_x = encoder.predict(test_x)\n",
    "    \n",
    "    return autoencoder,encoded_train_x,encoded_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pca(thr):\n",
    "    #train_x_pca,test_x_pca = to_pca(0.95)\n",
    "    pca = PCA(n_components = thr, svd_solver = 'full')\n",
    "    train_x_ = np.array(train_x)\n",
    "    print(type(train_x_))\n",
    "\n",
    "    test_x_ = np.array(test_x)\n",
    "    print(type(test_x_))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(time.ctime(start_time))\n",
    "\n",
    "    train_x_pca = pca.fit_transform(train_x_)\n",
    "    print(train_x_pca.shape)\n",
    "\n",
    "    test_x_pca = pca.fit_transform(test_x_)\n",
    "    print(test_x_pca.shape)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "\n",
    "    print(\"--- PCA spent %s seconds ---\" %elapsed_time )\n",
    "    \n",
    "    return  train_x_pca,test_x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ae(checkpoint_file, autoencoder,\n",
    "           epochs, batch_size, shuffle):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    hist_auto = autoencoder.fit(train_x, train_x,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    verbose=verbose_level,\n",
    "                    callbacks=[early_stopping, cp, tb],\n",
    "                    validation_data=(dev_x, dev_x))\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "    \n",
    "    return hist_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_auto(hist_auto, fig_file):\n",
    "    best_loss_value = hist_auto.history['loss'][-1]\n",
    "    print('Best loss value:', best_loss_value)\n",
    "\n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist_auto.history['loss'])\n",
    "    plt.plot(hist_auto.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.savefig(fig_file)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h(neurons,encoded_train_x,init_mode,activation_input,\n",
    "               weight_constraint,dropout_rate,activation_output,\n",
    "               loss,optimizer):\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=encoded_train_x.shape[1],\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_2h_():\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, input_dim=input_dim,\n",
    "                    kernel_initializer=init_mode,\n",
    "                    #kernel_regularizer=regularizers.l2(0.02), #from example\n",
    "                    activation=activation_input,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)\n",
    "                    )\n",
    "              )\n",
    "\n",
    "#     #Hidden Layer\n",
    "    model.add(Dense(int(neurons-int(neurons/4)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "\n",
    "    model.add(Dense(int(neurons-int((neurons/4)*2)), activation=\"relu\", kernel_initializer=init_mode)) #rezvy\n",
    "    model.add(BatchNormalization()) #commented for ex\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation=activation_output)) #example\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(checkpoint_file,ann,enc_train_x,train_y,epochs,shuffle,batch_size):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=verbose_level)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))\n",
    "\n",
    "    history = ann.fit(enc_train_x,\n",
    "                      train_y,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[early_stopping, cp, tb],\n",
    "                      epochs=epochs,\n",
    "                      shuffle=shuffle,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=verbose_level)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict(ann,enc_test_x):\n",
    "    pred_ann_prob = ann.predict(enc_test_x)\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "    \n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob, pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_predict_():\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(datetime.ctime(start_time))  \n",
    "\n",
    "    modelk = KerasClassifier(build_fn=ann_2h_,\n",
    "                             epochs=epochs, \n",
    "                             batch_size=batch_size, \n",
    "                             verbose=verbose_level\n",
    "                            )\n",
    "\n",
    "    pred_ann_prob = cross_val_predict(modelk,\n",
    "                                      enc_test_x,\n",
    "                                      test_y,\n",
    "                                      cv=KFold(n_splits=5, random_state=23),\n",
    "                                      verbose=1)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "    print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "    pred_ann_prob.shape\n",
    "    pred_ann_prob = pred_ann_prob[:,0]\n",
    "    pred_ann_01 = np.where(pred_ann_prob > 0.5, 1, 0)\n",
    "\n",
    "    #Print accuracy\n",
    "    acc_ann = accuracy_score(test_y, pred_ann_01)\n",
    "    print('Overall accuracy of Neural Network model:', acc_ann)\n",
    "\n",
    "    classiBM = \"NN\"\n",
    "    preBM = (sm.precision_score(test_y, pred_ann_01)*100) \n",
    "    recallBM = (sm.recall_score(test_y, pred_ann_01)*100) \n",
    "    f1scoreBM = (sm.f1_score(test_y, pred_ann_01)*100)\n",
    "    print(sm.classification_report(test_y, pred_ann_01,digits=4))\n",
    "    \n",
    "    return pred_ann_prob,pred_ann_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cm(pred_ann_prob, pred_ann_01, roc_file, cm_file):\n",
    "    false_positive_rate, recall, thresholds = roc_curve(test_y, pred_ann_prob)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.savefig(roc_file)\n",
    "    plt.show()\n",
    "    \n",
    "    cm = confusion_matrix(test_y, pred_ann_01)\n",
    "    labels = ['Normal', 'Malicious']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=\"RdYlGn\", vmin = 0.2);\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.savefig(cm_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- PCA Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Mon Jul  1 12:40:23 2019\n",
      "(1330452, 55)\n",
      "(415767, 55)\n",
      "--- PCA spent 88.86616444587708 seconds ---\n"
     ]
    }
   ],
   "source": [
    "train_x_pca,test_x_pca = to_pca(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- AE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 140)               29540     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 210)               29610     \n",
      "=================================================================\n",
      "Total params: 59,150\n",
      "Trainable params: 59,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_sigmoid_adam_mse,enc_train_x_asam,enc_test_x_asam = ae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ae_sigmoid_adam_mse = load_model('ae_sigmoid_adam_mse_redds10bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 12:42:16 2019\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1330452 samples, validate on 332613 samples\n",
      "Epoch 1/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0533 - acc: 0.3241 - val_loss: 0.0512 - val_acc: 0.2167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05117, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0493 - acc: 0.2521 - val_loss: 0.0510 - val_acc: 0.2283\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05117 to 0.05102, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1914 - val_loss: 0.0510 - val_acc: 0.1619\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05102 to 0.05100, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1038 - val_loss: 0.0510 - val_acc: 0.0641\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05100 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.0924 - val_loss: 0.0510 - val_acc: 0.1414\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1320 - val_loss: 0.0510 - val_acc: 0.0307\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.0862 - val_loss: 0.0510 - val_acc: 0.1003\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.0915 - val_loss: 0.0510 - val_acc: 0.0715\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1034 - val_loss: 0.0510 - val_acc: 0.1090\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.1040 - val_loss: 0.0510 - val_acc: 0.0853\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.05098\n",
      "Epoch 11/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.0861 - val_loss: 0.0510 - val_acc: 0.0901\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.0868 - val_loss: 0.0510 - val_acc: 0.0869\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.0909 - val_loss: 0.0510 - val_acc: 0.0856\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.0858 - val_loss: 0.0510 - val_acc: 0.0857\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.05098\n",
      "Epoch 15/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0855 - val_loss: 0.0510 - val_acc: 0.0854\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 16/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0851 - val_loss: 0.0510 - val_acc: 0.0851\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0492 - acc: 0.0853 - val_loss: 0.0510 - val_acc: 0.0882\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0873 - val_loss: 0.0510 - val_acc: 0.0841\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 19/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0851 - val_loss: 0.0510 - val_acc: 0.0854\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0853 - val_loss: 0.0510 - val_acc: 0.0895\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05098 to 0.05098, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0858 - val_loss: 0.0510 - val_acc: 0.0854\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05098 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 22/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0861 - val_loss: 0.0510 - val_acc: 0.0867\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.05097\n",
      "Epoch 23/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0856 - val_loss: 0.0510 - val_acc: 0.0862\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 24/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0862 - val_loss: 0.0510 - val_acc: 0.0866\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 25/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0866 - val_loss: 0.0510 - val_acc: 0.0867\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 26/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0864 - val_loss: 0.0510 - val_acc: 0.0866\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 27/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0869 - val_loss: 0.0510 - val_acc: 0.0857\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 28/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0872 - val_loss: 0.0510 - val_acc: 0.0879\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.05097\n",
      "Epoch 29/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0880 - val_loss: 0.0510 - val_acc: 0.0849\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 30/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0858 - val_loss: 0.0510 - val_acc: 0.0859\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 31/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0866 - val_loss: 0.0510 - val_acc: 0.0869\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 32/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0864 - val_loss: 0.0510 - val_acc: 0.0873\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 33/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0846 - val_loss: 0.0510 - val_acc: 0.0837\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.05097\n",
      "Epoch 34/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0845 - val_loss: 0.0510 - val_acc: 0.0836\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.05097\n",
      "Epoch 35/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0831 - val_loss: 0.0510 - val_acc: 0.0839\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 36/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0846 - val_loss: 0.0510 - val_acc: 0.0852\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05097\n",
      "Epoch 37/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0851 - val_loss: 0.0510 - val_acc: 0.0826\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 38/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0874 - val_loss: 0.0510 - val_acc: 0.0879\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.05097\n",
      "Epoch 39/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0893 - val_loss: 0.0510 - val_acc: 0.0897\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 40/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0881 - val_loss: 0.0510 - val_acc: 0.0891\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.05097\n",
      "Epoch 41/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0868 - val_loss: 0.0510 - val_acc: 0.0863\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 42/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0856 - val_loss: 0.0510 - val_acc: 0.0882\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 43/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0875 - val_loss: 0.0510 - val_acc: 0.0852\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.05097\n",
      "Epoch 44/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0839 - val_loss: 0.0510 - val_acc: 0.0862\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 45/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0825 - val_loss: 0.0510 - val_acc: 0.0889\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 46/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0931 - val_loss: 0.0510 - val_acc: 0.0924\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 47/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0886 - val_loss: 0.0510 - val_acc: 0.0919\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.05097\n",
      "Epoch 48/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0877 - val_loss: 0.0510 - val_acc: 0.0905\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.05097\n",
      "Epoch 49/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0914 - val_loss: 0.0510 - val_acc: 0.0916\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 50/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0914 - val_loss: 0.0510 - val_acc: 0.0912\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.05097\n",
      "Epoch 51/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0908 - val_loss: 0.0510 - val_acc: 0.0948\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.05097\n",
      "Epoch 52/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.0962 - val_loss: 0.0510 - val_acc: 0.0940\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.05097\n",
      "Epoch 53/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0492 - acc: 0.1023 - val_loss: 0.0510 - val_acc: 0.1019\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.05097 to 0.05097, saving model to ./H5files/ae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 54/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0492 - acc: 0.0907 - val_loss: 0.0510 - val_acc: 0.0877\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.05097\n",
      "Epoch 55/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.0903 - val_loss: 0.0510 - val_acc: 0.0913\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.05097\n",
      "Epoch 56/200\n",
      "1330452/1330452 [==============================] - 13s 10us/step - loss: 0.0492 - acc: 0.0909 - val_loss: 0.0510 - val_acc: 0.0925\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.05097\n",
      "Epoch 57/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.0949 - val_loss: 0.0510 - val_acc: 0.0961\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.05097\n",
      "Epoch 58/200\n",
      "1330452/1330452 [==============================] - 14s 10us/step - loss: 0.0492 - acc: 0.0972 - val_loss: 0.0510 - val_acc: 0.0980\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.05097\n",
      "Time elapsed (hh:mm:ss.ms) 0:14:07.390776\n"
     ]
    }
   ],
   "source": [
    "hist_ae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/ae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = ae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.04916937082501633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_ae_sigmoid_adam_mse  = plot_hist_auto(hist_ae_sigmoid_adam_mse, './Figures/hist_ae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- SPAE Dimensionality reduction ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "encoded_bottle_neck (Dense)  (None, 140)               29540     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 210)               29610     \n",
      "=================================================================\n",
      "Total params: 59,150\n",
      "Trainable params: 59,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "spae_sigmoid_adam_mse,enc_train_x_spsam,enc_test_x_spsam = spae(factor_enc_dim = 1.5,\n",
    "                                                          enc_activation = 'relu',\n",
    "                                                          dec_activation = 'sigmoid',\n",
    "                                                          optimizer='Adam',\n",
    "                                                          loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spae_sigmoid_adam_mse = load_model('spae_sigmoid_adam_mse_redds20bal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 12:56:40 2019\n",
      "Train on 1330452 samples, validate on 332613 samples\n",
      "Epoch 1/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.1342 - acc: 7.9672e-05 - val_loss: 0.0994 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09935, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0962 - acc: 3.5326e-05 - val_loss: 0.0976 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09935 to 0.09765, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0954 - acc: 3.1568e-05 - val_loss: 0.0951 - val_acc: 2.7058e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09765 to 0.09505, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0899 - acc: 1.5032e-05 - val_loss: 0.0902 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09505 to 0.09016, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0878 - acc: 1.8791e-05 - val_loss: 0.0891 - val_acc: 3.0065e-05\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09016 to 0.08911, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0870 - acc: 2.2549e-05 - val_loss: 0.0885 - val_acc: 3.3071e-05\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08911 to 0.08855, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0866 - acc: 2.5555e-05 - val_loss: 0.0882 - val_acc: 3.6078e-05\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08855 to 0.08820, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0863 - acc: 3.0065e-05 - val_loss: 0.0880 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.08820 to 0.08797, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 9/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0861 - acc: 3.3071e-05 - val_loss: 0.0878 - val_acc: 3.9084e-05\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08797 to 0.08780, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 10/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0859 - acc: 3.5326e-05 - val_loss: 0.0877 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08780 to 0.08767, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0858 - acc: 3.6078e-05 - val_loss: 0.0876 - val_acc: 4.2091e-05\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08767 to 0.08757, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 3.7581e-05 - val_loss: 0.0875 - val_acc: 4.5097e-05\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08757 to 0.08749, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0857 - acc: 3.7581e-05 - val_loss: 0.0874 - val_acc: 4.5097e-05\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08749 to 0.08742, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0856 - acc: 1.1951e-04 - val_loss: 0.0874 - val_acc: 3.9385e-04\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08742 to 0.08737, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 15/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0015 - val_loss: 0.0873 - val_acc: 0.0037\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08737 to 0.08732, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 16/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0111 - val_loss: 0.0873 - val_acc: 0.0306\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08732 to 0.08728, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0855 - acc: 0.0198 - val_loss: 0.0872 - val_acc: 0.0345\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.08728 to 0.08724, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 18/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0234 - val_loss: 0.0872 - val_acc: 0.0383\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08724 to 0.08721, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 19/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0268 - val_loss: 0.0872 - val_acc: 0.0412\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08721 to 0.08719, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0301 - val_loss: 0.0872 - val_acc: 0.0437\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08719 to 0.08716, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0854 - acc: 0.0333 - val_loss: 0.0871 - val_acc: 0.0465\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08716 to 0.08714, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 22/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0363 - val_loss: 0.0871 - val_acc: 0.0496\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08714 to 0.08712, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 23/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0853 - acc: 0.0391 - val_loss: 0.0871 - val_acc: 0.0509\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08712 to 0.08710, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 24/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0416 - val_loss: 0.0871 - val_acc: 0.0532\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.08710 to 0.08709, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 25/200\n",
      "1330452/1330452 [==============================] - 15s 11us/step - loss: 0.0853 - acc: 0.0439 - val_loss: 0.0871 - val_acc: 0.0542\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.08709 to 0.08707, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 26/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0853 - acc: 0.0450 - val_loss: 0.0871 - val_acc: 0.0621\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.08707 to 0.08706, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 27/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0853 - acc: 0.0481 - val_loss: 0.0870 - val_acc: 0.0570\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08706 to 0.08704, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 28/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0852 - acc: 0.0498 - val_loss: 0.0870 - val_acc: 0.0585\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.08704 to 0.08703, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 29/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0852 - acc: 0.0498 - val_loss: 0.0870 - val_acc: 0.0691\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08703 to 0.08703, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 30/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0852 - acc: 0.0519 - val_loss: 0.0870 - val_acc: 0.0628\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.08703 to 0.08701, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 31/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0852 - acc: 0.0536 - val_loss: 0.0870 - val_acc: 0.0614\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.08701 to 0.08700, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 32/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0852 - acc: 0.0548 - val_loss: 0.0870 - val_acc: 0.0665\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.08700 to 0.08699, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 33/200\n",
      "1330452/1330452 [==============================] - 14s 11us/step - loss: 0.0852 - acc: 0.0556 - val_loss: 0.0870 - val_acc: 0.0652\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.08699 to 0.08698, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 34/200\n",
      "1330452/1330452 [==============================] - 15s 12us/step - loss: 0.0852 - acc: 0.0568 - val_loss: 0.0870 - val_acc: 0.0698\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.08698 to 0.08698, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 35/200\n",
      "1330452/1330452 [==============================] - 18s 13us/step - loss: 0.0852 - acc: 0.0574 - val_loss: 0.0870 - val_acc: 0.0657\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.08698 to 0.08697, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 36/200\n",
      "1330452/1330452 [==============================] - 18s 13us/step - loss: 0.0852 - acc: 0.0587 - val_loss: 0.0870 - val_acc: 0.0703\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.08697 to 0.08696, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 37/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0852 - acc: 0.0581 - val_loss: 0.0870 - val_acc: 0.0693\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.08696 to 0.08696, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 38/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0852 - acc: 0.0593 - val_loss: 0.0870 - val_acc: 0.0596\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.08696 to 0.08695, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 39/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0852 - acc: 0.0594 - val_loss: 0.0869 - val_acc: 0.0628\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08695 to 0.08694, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 40/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0852 - acc: 0.0592 - val_loss: 0.0869 - val_acc: 0.0659\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.08694 to 0.08694, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 41/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0610 - val_loss: 0.0869 - val_acc: 0.0690\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.08694 to 0.08693, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 42/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0616 - val_loss: 0.0869 - val_acc: 0.0737\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.08693 to 0.08693, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 43/200\n",
      "1330452/1330452 [==============================] - 17s 12us/step - loss: 0.0851 - acc: 0.0622 - val_loss: 0.0869 - val_acc: 0.0719\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.08693 to 0.08692, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 44/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0617 - val_loss: 0.0869 - val_acc: 0.0692\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.08692 to 0.08692, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 45/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0613 - val_loss: 0.0869 - val_acc: 0.0683\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.08692 to 0.08692, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 46/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0614 - val_loss: 0.0869 - val_acc: 0.0682\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.08692 to 0.08692, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 47/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0629 - val_loss: 0.0869 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.08692 to 0.08691, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 48/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0612 - val_loss: 0.0869 - val_acc: 0.0671\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.08691 to 0.08690, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 49/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0626 - val_loss: 0.0869 - val_acc: 0.0762\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.08690 to 0.08690, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 50/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0621 - val_loss: 0.0869 - val_acc: 0.0806\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.08690\n",
      "Epoch 51/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0623 - val_loss: 0.0869 - val_acc: 0.0730\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.08690 to 0.08689, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 52/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0631 - val_loss: 0.0869 - val_acc: 0.0581\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.08689\n",
      "Epoch 53/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0615 - val_loss: 0.0869 - val_acc: 0.0779\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.08689 to 0.08689, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 54/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0620 - val_loss: 0.0869 - val_acc: 0.0750\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.08689 to 0.08689, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 55/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0609 - val_loss: 0.0869 - val_acc: 0.0756\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.08689\n",
      "Epoch 56/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0619 - val_loss: 0.0869 - val_acc: 0.0737\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.08689 to 0.08688, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 57/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0607 - val_loss: 0.0869 - val_acc: 0.0793\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.08688\n",
      "Epoch 58/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0614 - val_loss: 0.0869 - val_acc: 0.0700\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.08688 to 0.08688, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 59/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0619 - val_loss: 0.0869 - val_acc: 0.0704\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.08688 to 0.08687, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 60/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0606 - val_loss: 0.0869 - val_acc: 0.0656\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.08687 to 0.08687, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 61/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0621 - val_loss: 0.0869 - val_acc: 0.0615\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.08687 to 0.08687, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 62/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0614 - val_loss: 0.0869 - val_acc: 0.0617\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.08687 to 0.08687, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 63/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0611 - val_loss: 0.0869 - val_acc: 0.0827\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.08687\n",
      "Epoch 64/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0621 - val_loss: 0.0869 - val_acc: 0.0731\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.08687 to 0.08686, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 65/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0617 - val_loss: 0.0869 - val_acc: 0.0824\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.08686\n",
      "Epoch 66/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0591 - val_loss: 0.0869 - val_acc: 0.0767\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.08686\n",
      "Epoch 67/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0606 - val_loss: 0.0869 - val_acc: 0.0739\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.08686 to 0.08686, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 68/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0595 - val_loss: 0.0869 - val_acc: 0.0724\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.08686\n",
      "Epoch 69/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0602 - val_loss: 0.0869 - val_acc: 0.0744\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.08686 to 0.08685, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 70/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0600 - val_loss: 0.0869 - val_acc: 0.0819\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.08685\n",
      "Epoch 71/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0612 - val_loss: 0.0869 - val_acc: 0.0799\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.08685\n",
      "Epoch 72/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0602 - val_loss: 0.0869 - val_acc: 0.0806\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.08685 to 0.08685, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 73/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0570 - val_loss: 0.0869 - val_acc: 0.0795\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.08685 to 0.08685, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 74/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0595 - val_loss: 0.0868 - val_acc: 0.0753\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.08685 to 0.08685, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 75/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0576 - val_loss: 0.0868 - val_acc: 0.0753\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.08685 to 0.08684, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 76/200\n",
      "1330452/1330452 [==============================] - 17s 12us/step - loss: 0.0851 - acc: 0.0585 - val_loss: 0.0868 - val_acc: 0.0780\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.08684 to 0.08684, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 77/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0592 - val_loss: 0.0868 - val_acc: 0.0827\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.08684\n",
      "Epoch 78/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0580 - val_loss: 0.0868 - val_acc: 0.0777\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.08684 to 0.08684, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 79/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0566 - val_loss: 0.0868 - val_acc: 0.0772\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.08684 to 0.08684, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 80/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0553 - val_loss: 0.0868 - val_acc: 0.0692\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.08684\n",
      "Epoch 81/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0544 - val_loss: 0.0868 - val_acc: 0.0736\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.08684 to 0.08684, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 82/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0515 - val_loss: 0.0868 - val_acc: 0.0738\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.08684\n",
      "Epoch 83/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0477 - val_loss: 0.0868 - val_acc: 0.0553\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.08684 to 0.08683, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 84/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0490 - val_loss: 0.0868 - val_acc: 0.0262\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.08683\n",
      "Epoch 85/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0508 - val_loss: 0.0868 - val_acc: 0.0814\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.08683\n",
      "Epoch 86/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0515 - val_loss: 0.0868 - val_acc: 0.0708\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.08683 to 0.08683, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 87/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0527 - val_loss: 0.0868 - val_acc: 0.0466\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.08683 to 0.08683, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 88/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0468 - val_loss: 0.0868 - val_acc: 0.0633\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.08683\n",
      "Epoch 89/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0488 - val_loss: 0.0868 - val_acc: 0.0704\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.08683\n",
      "Epoch 90/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0506 - val_loss: 0.0868 - val_acc: 0.0794\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.08683\n",
      "Epoch 91/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0850 - acc: 0.0479 - val_loss: 0.0868 - val_acc: 0.0503\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.08683 to 0.08682, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 92/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0851 - acc: 0.0461 - val_loss: 0.0868 - val_acc: 0.0767\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.08682\n",
      "Epoch 93/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0850 - acc: 0.0506 - val_loss: 0.0868 - val_acc: 0.0657\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.08682 to 0.08682, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 94/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0850 - acc: 0.0460 - val_loss: 0.0868 - val_acc: 0.0810\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.08682\n",
      "Epoch 95/200\n",
      "1330452/1330452 [==============================] - 17s 12us/step - loss: 0.0851 - acc: 0.0484 - val_loss: 0.0868 - val_acc: 0.0785\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.08682\n",
      "Epoch 96/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0405 - val_loss: 0.0868 - val_acc: 0.0530\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.08682 to 0.08682, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 97/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0851 - acc: 0.0405 - val_loss: 0.0868 - val_acc: 0.0550\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.08682\n",
      "Epoch 98/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0446 - val_loss: 0.0868 - val_acc: 0.0239\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.08682 to 0.08682, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 99/200\n",
      "1330452/1330452 [==============================] - 17s 12us/step - loss: 0.0850 - acc: 0.0448 - val_loss: 0.0868 - val_acc: 0.0574\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.08682\n",
      "Epoch 100/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0399 - val_loss: 0.0868 - val_acc: 0.0403\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.08682 to 0.08681, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 101/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0410 - val_loss: 0.0868 - val_acc: 0.0770\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.08681\n",
      "Epoch 102/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0426 - val_loss: 0.0868 - val_acc: 0.0393\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.08681 to 0.08681, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 103/200\n",
      "1330452/1330452 [==============================] - 17s 12us/step - loss: 0.0850 - acc: 0.0410 - val_loss: 0.0868 - val_acc: 0.0595\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.08681\n",
      "Epoch 104/200\n",
      "1330452/1330452 [==============================] - 17s 12us/step - loss: 0.0850 - acc: 0.0368 - val_loss: 0.0868 - val_acc: 0.0386\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.08681\n",
      "Epoch 105/200\n",
      "1330452/1330452 [==============================] - 17s 12us/step - loss: 0.0850 - acc: 0.0333 - val_loss: 0.0868 - val_acc: 0.0371\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.08681 to 0.08681, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 106/200\n",
      "1330452/1330452 [==============================] - 17s 13us/step - loss: 0.0850 - acc: 0.0350 - val_loss: 0.0868 - val_acc: 0.0721\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.08681\n",
      "Epoch 107/200\n",
      "1330452/1330452 [==============================] - 17s 12us/step - loss: 0.0850 - acc: 0.0317 - val_loss: 0.0868 - val_acc: 0.0450\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.08681\n",
      "Epoch 108/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0301 - val_loss: 0.0868 - val_acc: 0.0585\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.08681\n",
      "Epoch 109/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0390 - val_loss: 0.0868 - val_acc: 0.0558\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.08681\n",
      "Epoch 110/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0318 - val_loss: 0.0868 - val_acc: 0.0465\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.08681 to 0.08681, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 111/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0319 - val_loss: 0.0868 - val_acc: 0.0168\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.08681 to 0.08680, saving model to ./H5files/spae_sigmoid_adam_mse_redds100bal.h5\n",
      "Epoch 112/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0375 - val_loss: 0.0868 - val_acc: 0.0591\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.08680\n",
      "Epoch 113/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0327 - val_loss: 0.0868 - val_acc: 0.0446\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.08680\n",
      "Epoch 114/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0275 - val_loss: 0.0869 - val_acc: 0.0482\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.08680\n",
      "Epoch 115/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0296 - val_loss: 0.0868 - val_acc: 0.0383\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.08680\n",
      "Epoch 116/200\n",
      "1330452/1330452 [==============================] - 16s 12us/step - loss: 0.0850 - acc: 0.0254 - val_loss: 0.0868 - val_acc: 0.0322\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.08680\n",
      "Time elapsed (hh:mm:ss.ms) 0:31:01.626682\n"
     ]
    }
   ],
   "source": [
    "hist_spae_sigmoid_adam_mse = fit_ae(checkpoint_file = \"./H5files/spae_sigmoid_adam_mse_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                  autoencoder = spae_sigmoid_adam_mse, \n",
    "                                  epochs = 200, \n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss value: 0.08502755085169872\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_value_spae_sigmoid_adam_mse  = plot_hist_auto(hist_spae_sigmoid_adam_mse, './Figures/hist_spae_sigmoid_adam_mse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valueDict = {\n",
    "    'loss_value_ae_sigmoid_adam_mse': best_loss_value_ae_sigmoid_adam_mse,\n",
    "    'loss_value_spae_sigmoid_adam_mse': best_loss_value_spae_sigmoid_adam_mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_value_ae_sigmoid_adam_mse': 0.04916937082501633,\n",
       " 'loss_value_spae_sigmoid_adam_mse': 0.08502755085169872}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330452, 140)\n",
      "(415767, 140)\n",
      "(1330452, 140)\n",
      "(415767, 140)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train_x_asam.shape)\n",
    "print(enc_test_x_asam.shape)\n",
    "\n",
    "print(enc_train_x_spsam.shape)\n",
    "print(enc_test_x_spsam.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_asam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 13:27:42 2019\n",
      "Train on 1064361 samples, validate on 266091 samples\n",
      "Epoch 1/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.2198 - acc: 0.8884 - val_loss: 0.1282 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12816, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 2/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.1261 - acc: 0.9340 - val_loss: 0.1262 - val_acc: 0.9371\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12816 to 0.12625, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 3/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.1051 - acc: 0.9460 - val_loss: 0.0964 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12625 to 0.09642, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 4/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0902 - acc: 0.9546 - val_loss: 0.0805 - val_acc: 0.9597\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09642 to 0.08048, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 5/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0786 - acc: 0.9609 - val_loss: 0.0732 - val_acc: 0.9634\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08048 to 0.07317, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 6/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0706 - acc: 0.9651 - val_loss: 0.0599 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07317 to 0.05994, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 7/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0639 - acc: 0.9688 - val_loss: 0.0551 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.05994 to 0.05510, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 8/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0589 - acc: 0.9714 - val_loss: 0.0577 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05510\n",
      "Epoch 9/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0544 - acc: 0.9737 - val_loss: 0.0594 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05510\n",
      "Epoch 10/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0512 - acc: 0.9753 - val_loss: 0.0478 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05510 to 0.04777, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 11/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0488 - acc: 0.9765 - val_loss: 0.0456 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04777 to 0.04562, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 12/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0462 - acc: 0.9777 - val_loss: 0.0450 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04562 to 0.04501, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 13/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0439 - acc: 0.9787 - val_loss: 0.0388 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.04501 to 0.03877, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 14/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0423 - acc: 0.9797 - val_loss: 0.0420 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03877\n",
      "Epoch 15/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0409 - acc: 0.9803 - val_loss: 0.0388 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03877\n",
      "Epoch 16/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0393 - acc: 0.9811 - val_loss: 0.0370 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03877 to 0.03701, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 17/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0378 - acc: 0.9816 - val_loss: 0.0434 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.03701\n",
      "Epoch 18/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0372 - acc: 0.9820 - val_loss: 0.0339 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03701 to 0.03392, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 19/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0361 - acc: 0.9825 - val_loss: 0.0318 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.03392 to 0.03182, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 20/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0348 - acc: 0.9831 - val_loss: 0.0317 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.03182 to 0.03167, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 21/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0342 - acc: 0.9835 - val_loss: 0.0333 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03167\n",
      "Epoch 22/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0334 - acc: 0.9839 - val_loss: 0.0295 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.03167 to 0.02953, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 23/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0326 - acc: 0.9843 - val_loss: 0.0335 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02953\n",
      "Epoch 24/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0317 - acc: 0.9847 - val_loss: 0.0296 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02953\n",
      "Epoch 25/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0315 - acc: 0.9848 - val_loss: 0.0339 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02953\n",
      "Epoch 26/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0307 - acc: 0.9852 - val_loss: 0.0287 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02953 to 0.02874, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 27/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0308 - acc: 0.9852 - val_loss: 0.0264 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02874 to 0.02640, saving model to ./H5files/ae_ann_2h_unisoftsigbinlosadam_redds100bal.h5\n",
      "Epoch 28/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0304 - acc: 0.9854 - val_loss: 0.0288 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02640\n",
      "Epoch 29/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0298 - acc: 0.9857 - val_loss: 0.0297 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02640\n",
      "Epoch 30/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0292 - acc: 0.9859 - val_loss: 0.0313 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02640\n",
      "Epoch 31/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0292 - acc: 0.9858 - val_loss: 0.0326 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02640\n",
      "Epoch 32/200\n",
      "1064361/1064361 [==============================] - 20s 19us/step - loss: 0.0286 - acc: 0.9862 - val_loss: 0.0270 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02640\n",
      "Time elapsed (hh:mm:ss.ms) 0:10:43.554813\n"
     ]
    }
   ],
   "source": [
    "hist_ae_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ae_ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = ae_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_asam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_ae_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_ae_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_ae_ann_2h_unisoftsigbinlosadam, './Figures/ae_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_ae_ann_2h_prob_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict(ae_ann_2h_unisoftsigbinlosadam,enc_test_x_asam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_asam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 13:38:26 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 6s 19us/step - loss: 0.3656 - acc: 0.8155\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1753 - acc: 0.9121\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1479 - acc: 0.9234\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1360 - acc: 0.9289\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1267 - acc: 0.9344\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1204 - acc: 0.9377\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1139 - acc: 0.9416\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1077 - acc: 0.9456\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1019 - acc: 0.9487\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0964 - acc: 0.9519\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0912 - acc: 0.9543\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0882 - acc: 0.9562\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0838 - acc: 0.9587\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0797 - acc: 0.9606\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0773 - acc: 0.9617\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0738 - acc: 0.9639\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0707 - acc: 0.9652\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0682 - acc: 0.9668\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0672 - acc: 0.9672\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0632 - acc: 0.9690\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0618 - acc: 0.9699\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0607 - acc: 0.9706\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0578 - acc: 0.9720\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0571 - acc: 0.9722\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0564 - acc: 0.9729\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0542 - acc: 0.9735\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0521 - acc: 0.9746\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0515 - acc: 0.9750\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0499 - acc: 0.9756\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0494 - acc: 0.9760\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0486 - acc: 0.9766\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0478 - acc: 0.9769\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0477 - acc: 0.9772\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0454 - acc: 0.9781\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0462 - acc: 0.9778\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0438 - acc: 0.9787\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0443 - acc: 0.9788\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0434 - acc: 0.9790\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0424 - acc: 0.9793\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0407 - acc: 0.9801\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0420 - acc: 0.9800\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0400 - acc: 0.9807\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0400 - acc: 0.9805\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0394 - acc: 0.9813\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0394 - acc: 0.9812\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0393 - acc: 0.9812\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0383 - acc: 0.9816\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0374 - acc: 0.9822\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0375 - acc: 0.9822\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0370 - acc: 0.9822\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0366 - acc: 0.9826\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0363 - acc: 0.9827\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0357 - acc: 0.9828\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0355 - acc: 0.9830\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0349 - acc: 0.9833\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0356 - acc: 0.9830\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0338 - acc: 0.9836\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0347 - acc: 0.9833\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0335 - acc: 0.9841\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0335 - acc: 0.9841\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0330 - acc: 0.9845\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0328 - acc: 0.9843\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0326 - acc: 0.9845\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.0321 - acc: 0.9844\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0324 - acc: 0.9848\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0318 - acc: 0.9849\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0314 - acc: 0.9850\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0317 - acc: 0.9847\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0311 - acc: 0.9852\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0315 - acc: 0.9850\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0309 - acc: 0.9855\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0309 - acc: 0.9854\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0309 - acc: 0.9853\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0300 - acc: 0.9859\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9856\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0302 - acc: 0.9857\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0304 - acc: 0.9858\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0290 - acc: 0.9863\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0307 - acc: 0.9855\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0288 - acc: 0.9862\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0292 - acc: 0.9862\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0295 - acc: 0.9864\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0299 - acc: 0.9861\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0285 - acc: 0.9868\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0296 - acc: 0.9862\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0282 - acc: 0.9868\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0289 - acc: 0.9863\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0294 - acc: 0.9862\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0273 - acc: 0.9872\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0284 - acc: 0.9866\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0281 - acc: 0.9868\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0285 - acc: 0.9868\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0273 - acc: 0.9870\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0284 - acc: 0.9866\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0282 - acc: 0.9868\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0270 - acc: 0.9875\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0272 - acc: 0.9874\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0274 - acc: 0.9872\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0281 - acc: 0.9871\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0273 - acc: 0.9870\n",
      "83154/83154 [==============================] - 0s 5us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332613/332613 [==============================] - 7s 20us/step - loss: 0.3646 - acc: 0.8172\n",
      "Epoch 2/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1717 - acc: 0.9137\n",
      "Epoch 3/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1472 - acc: 0.9227\n",
      "Epoch 4/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1372 - acc: 0.9267\n",
      "Epoch 5/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1301 - acc: 0.9305\n",
      "Epoch 6/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1227 - acc: 0.9346\n",
      "Epoch 7/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1193 - acc: 0.9369\n",
      "Epoch 8/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1131 - acc: 0.9397\n",
      "Epoch 9/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1076 - acc: 0.9435\n",
      "Epoch 10/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.1030 - acc: 0.9465\n",
      "Epoch 11/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0975 - acc: 0.9497\n",
      "Epoch 12/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0947 - acc: 0.9518\n",
      "Epoch 13/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0897 - acc: 0.9542\n",
      "Epoch 14/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0862 - acc: 0.9562\n",
      "Epoch 15/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0829 - acc: 0.9580\n",
      "Epoch 16/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0787 - acc: 0.9607\n",
      "Epoch 17/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0773 - acc: 0.9615\n",
      "Epoch 18/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0740 - acc: 0.9633\n",
      "Epoch 19/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0717 - acc: 0.9645\n",
      "Epoch 20/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0690 - acc: 0.9660\n",
      "Epoch 21/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0681 - acc: 0.9665\n",
      "Epoch 22/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0645 - acc: 0.9683\n",
      "Epoch 23/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0629 - acc: 0.9695\n",
      "Epoch 24/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0617 - acc: 0.9699\n",
      "Epoch 25/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0604 - acc: 0.9707\n",
      "Epoch 26/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0591 - acc: 0.9712\n",
      "Epoch 27/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0576 - acc: 0.9721\n",
      "Epoch 28/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0565 - acc: 0.9729\n",
      "Epoch 29/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0549 - acc: 0.9736\n",
      "Epoch 30/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0535 - acc: 0.9742\n",
      "Epoch 31/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0529 - acc: 0.9745\n",
      "Epoch 32/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0517 - acc: 0.9749\n",
      "Epoch 33/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0510 - acc: 0.9755\n",
      "Epoch 34/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0503 - acc: 0.9758\n",
      "Epoch 35/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0487 - acc: 0.9764\n",
      "Epoch 36/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0488 - acc: 0.9767\n",
      "Epoch 37/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0472 - acc: 0.9770\n",
      "Epoch 38/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0480 - acc: 0.9769\n",
      "Epoch 39/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0467 - acc: 0.9775\n",
      "Epoch 40/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0454 - acc: 0.9782\n",
      "Epoch 41/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0450 - acc: 0.9784\n",
      "Epoch 42/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0445 - acc: 0.9785\n",
      "Epoch 43/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0428 - acc: 0.9794\n",
      "Epoch 44/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0435 - acc: 0.9790\n",
      "Epoch 45/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0423 - acc: 0.9794\n",
      "Epoch 46/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0418 - acc: 0.9799\n",
      "Epoch 47/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0422 - acc: 0.9798\n",
      "Epoch 48/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0403 - acc: 0.9807\n",
      "Epoch 49/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0411 - acc: 0.9801\n",
      "Epoch 50/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0400 - acc: 0.9808\n",
      "Epoch 51/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0398 - acc: 0.9809\n",
      "Epoch 52/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0395 - acc: 0.9811\n",
      "Epoch 53/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0386 - acc: 0.9812\n",
      "Epoch 54/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0379 - acc: 0.9820\n",
      "Epoch 55/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0375 - acc: 0.9820\n",
      "Epoch 56/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0374 - acc: 0.9818\n",
      "Epoch 57/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0372 - acc: 0.9822\n",
      "Epoch 58/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0373 - acc: 0.9821\n",
      "Epoch 59/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0369 - acc: 0.9824\n",
      "Epoch 60/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0370 - acc: 0.9824\n",
      "Epoch 61/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0355 - acc: 0.9831\n",
      "Epoch 62/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0362 - acc: 0.9827\n",
      "Epoch 63/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0355 - acc: 0.9829\n",
      "Epoch 64/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0343 - acc: 0.9835\n",
      "Epoch 65/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0359 - acc: 0.9828\n",
      "Epoch 66/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0343 - acc: 0.9833\n",
      "Epoch 67/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0351 - acc: 0.9831\n",
      "Epoch 68/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0340 - acc: 0.9835\n",
      "Epoch 69/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0338 - acc: 0.9840\n",
      "Epoch 70/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0336 - acc: 0.9841\n",
      "Epoch 71/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0332 - acc: 0.9841\n",
      "Epoch 72/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0331 - acc: 0.9841\n",
      "Epoch 73/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0327 - acc: 0.9842\n",
      "Epoch 74/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0331 - acc: 0.9842\n",
      "Epoch 75/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0324 - acc: 0.9844\n",
      "Epoch 76/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0320 - acc: 0.9844\n",
      "Epoch 77/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0322 - acc: 0.9846\n",
      "Epoch 78/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0326 - acc: 0.9845\n",
      "Epoch 79/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0320 - acc: 0.9849\n",
      "Epoch 80/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0321 - acc: 0.9849\n",
      "Epoch 81/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0316 - acc: 0.9850\n",
      "Epoch 82/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0311 - acc: 0.9852\n",
      "Epoch 83/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0317 - acc: 0.9848\n",
      "Epoch 84/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0314 - acc: 0.9850\n",
      "Epoch 85/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0312 - acc: 0.9852\n",
      "Epoch 86/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0310 - acc: 0.9851\n",
      "Epoch 87/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0309 - acc: 0.9854\n",
      "Epoch 88/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9855\n",
      "Epoch 89/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0307 - acc: 0.9853\n",
      "Epoch 90/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9856\n",
      "Epoch 91/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0303 - acc: 0.9852\n",
      "Epoch 92/100\n",
      "332613/332613 [==============================] - 6s 17us/step - loss: 0.0310 - acc: 0.9851\n",
      "Epoch 93/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0297 - acc: 0.9860\n",
      "Epoch 94/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0299 - acc: 0.9858\n",
      "Epoch 95/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0294 - acc: 0.9860\n",
      "Epoch 96/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9855\n",
      "Epoch 97/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9858\n",
      "Epoch 98/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0289 - acc: 0.9863\n",
      "Epoch 99/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0291 - acc: 0.9859\n",
      "Epoch 100/100\n",
      "332613/332613 [==============================] - 6s 18us/step - loss: 0.0295 - acc: 0.9857\n",
      "83154/83154 [==============================] - 0s 5us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 7s 20us/step - loss: 0.3636 - acc: 0.8167\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1729 - acc: 0.9120\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1480 - acc: 0.9223\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1384 - acc: 0.9263\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1282 - acc: 0.9321\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1217 - acc: 0.9360\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1153 - acc: 0.9397\n",
      "Epoch 8/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1104 - acc: 0.9424\n",
      "Epoch 9/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1056 - acc: 0.9453\n",
      "Epoch 10/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.1013 - acc: 0.9474\n",
      "Epoch 11/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0967 - acc: 0.9504\n",
      "Epoch 12/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0934 - acc: 0.9523\n",
      "Epoch 13/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0894 - acc: 0.9549\n",
      "Epoch 14/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0852 - acc: 0.9566\n",
      "Epoch 15/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0826 - acc: 0.9588\n",
      "Epoch 16/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0786 - acc: 0.9607\n",
      "Epoch 17/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0776 - acc: 0.9614\n",
      "Epoch 18/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0736 - acc: 0.9637\n",
      "Epoch 19/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0711 - acc: 0.9647\n",
      "Epoch 20/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0703 - acc: 0.9657\n",
      "Epoch 21/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0669 - acc: 0.9672\n",
      "Epoch 22/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0653 - acc: 0.9685\n",
      "Epoch 23/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0639 - acc: 0.9690\n",
      "Epoch 24/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0612 - acc: 0.9702\n",
      "Epoch 25/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0600 - acc: 0.9712\n",
      "Epoch 26/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0579 - acc: 0.9719\n",
      "Epoch 27/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0569 - acc: 0.9723\n",
      "Epoch 28/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0562 - acc: 0.9726\n",
      "Epoch 29/100\n",
      "332614/332614 [==============================] - 7s 20us/step - loss: 0.0543 - acc: 0.9738\n",
      "Epoch 30/100\n",
      "332614/332614 [==============================] - 7s 20us/step - loss: 0.0532 - acc: 0.9745\n",
      "Epoch 31/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0517 - acc: 0.9751\n",
      "Epoch 32/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0509 - acc: 0.9754\n",
      "Epoch 33/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0501 - acc: 0.9760\n",
      "Epoch 34/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0483 - acc: 0.9768\n",
      "Epoch 35/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0486 - acc: 0.9768\n",
      "Epoch 36/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0472 - acc: 0.9773\n",
      "Epoch 37/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0479 - acc: 0.9772\n",
      "Epoch 38/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0453 - acc: 0.9782\n",
      "Epoch 39/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0446 - acc: 0.9785\n",
      "Epoch 40/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0444 - acc: 0.9786\n",
      "Epoch 41/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0441 - acc: 0.9788\n",
      "Epoch 42/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0434 - acc: 0.9793\n",
      "Epoch 43/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0427 - acc: 0.9794\n",
      "Epoch 44/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0421 - acc: 0.9798\n",
      "Epoch 45/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0416 - acc: 0.9802\n",
      "Epoch 46/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0405 - acc: 0.9804\n",
      "Epoch 47/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0410 - acc: 0.9803\n",
      "Epoch 48/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0397 - acc: 0.9809\n",
      "Epoch 49/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0405 - acc: 0.9804\n",
      "Epoch 50/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0396 - acc: 0.9811\n",
      "Epoch 51/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0392 - acc: 0.9812\n",
      "Epoch 52/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0379 - acc: 0.9818\n",
      "Epoch 53/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0379 - acc: 0.9816\n",
      "Epoch 54/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0377 - acc: 0.9819\n",
      "Epoch 55/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0376 - acc: 0.9820\n",
      "Epoch 56/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0367 - acc: 0.9825\n",
      "Epoch 57/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0369 - acc: 0.9824\n",
      "Epoch 58/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0368 - acc: 0.9821\n",
      "Epoch 59/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0360 - acc: 0.9826\n",
      "Epoch 60/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0358 - acc: 0.9828\n",
      "Epoch 61/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0354 - acc: 0.9832\n",
      "Epoch 62/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0345 - acc: 0.9833\n",
      "Epoch 63/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0352 - acc: 0.9832\n",
      "Epoch 64/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0351 - acc: 0.9831\n",
      "Epoch 65/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0337 - acc: 0.9842\n",
      "Epoch 66/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0342 - acc: 0.9835\n",
      "Epoch 67/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0340 - acc: 0.9839\n",
      "Epoch 68/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0333 - acc: 0.9840\n",
      "Epoch 69/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0333 - acc: 0.9842\n",
      "Epoch 70/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0337 - acc: 0.9841\n",
      "Epoch 71/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0336 - acc: 0.9839\n",
      "Epoch 72/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0322 - acc: 0.9848\n",
      "Epoch 73/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0314 - acc: 0.9849\n",
      "Epoch 74/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0330 - acc: 0.9844\n",
      "Epoch 75/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0320 - acc: 0.9848\n",
      "Epoch 76/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0327 - acc: 0.9844\n",
      "Epoch 77/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0315 - acc: 0.9849\n",
      "Epoch 78/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0320 - acc: 0.9847\n",
      "Epoch 79/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0319 - acc: 0.9848\n",
      "Epoch 80/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0307 - acc: 0.9853\n",
      "Epoch 81/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0307 - acc: 0.9853\n",
      "Epoch 82/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0313 - acc: 0.9852\n",
      "Epoch 83/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0316 - acc: 0.9849\n",
      "Epoch 84/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0305 - acc: 0.9857\n",
      "Epoch 85/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0313 - acc: 0.9851\n",
      "Epoch 86/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0300 - acc: 0.9857\n",
      "Epoch 87/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0308 - acc: 0.9856\n",
      "Epoch 88/100\n",
      "332614/332614 [==============================] - 6s 19us/step - loss: 0.0299 - acc: 0.9854\n",
      "Epoch 89/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0303 - acc: 0.9855\n",
      "Epoch 90/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0299 - acc: 0.9858\n",
      "Epoch 91/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0298 - acc: 0.9858\n",
      "Epoch 92/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9854\n",
      "Epoch 93/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0292 - acc: 0.9861\n",
      "Epoch 94/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0301 - acc: 0.9857\n",
      "Epoch 95/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0290 - acc: 0.9861\n",
      "Epoch 96/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0295 - acc: 0.9863\n",
      "Epoch 97/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0291 - acc: 0.9865\n",
      "Epoch 98/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0292 - acc: 0.9864\n",
      "Epoch 99/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0282 - acc: 0.9865\n",
      "Epoch 100/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.0295 - acc: 0.9860\n",
      "83153/83153 [==============================] - 0s 6us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 210)               29610     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 158)               33338     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 105)               16695     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 106       \n",
      "=================================================================\n",
      "Total params: 80,169\n",
      "Trainable params: 79,959\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "332614/332614 [==============================] - 7s 21us/step - loss: 0.3803 - acc: 0.8074\n",
      "Epoch 2/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1764 - acc: 0.9130\n",
      "Epoch 3/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1489 - acc: 0.9238\n",
      "Epoch 4/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1365 - acc: 0.9287\n",
      "Epoch 5/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1293 - acc: 0.9317\n",
      "Epoch 6/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1234 - acc: 0.9351\n",
      "Epoch 7/100\n",
      "332614/332614 [==============================] - 6s 18us/step - loss: 0.1185 - acc: 0.9376\n",
      "Epoch 8/100\n",
      "249900/332614 [=====================>........] - ETA: 1s - loss: 0.1146 - acc: 0.9398"
     ]
    }
   ],
   "source": [
    "pred_ae_ann_2h_prob_unisoftsigbinlosadam,pred_ae_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_ae_ann_2h_01_unisoftsigbinlosadam, pred_ae_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=enc_train_x_spsam,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_sp_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = sp_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = enc_train_x_spsam,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_sp_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_sp_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_sp_ann_2h_unisoftsigbinlosadam, './Figures/sp_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict(sp_ann_2h_unisoftsigbinlosadam,enc_test_x_spsam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=enc_train_x_asam\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=enc_test_x_spsam\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sp_ann_2h_prob_unisoftsigbinlosadam,pred_sp_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_sp_ann_2h_prob_unisoftsigbinlosadam, pred_sp_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_sp_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- ANN with no encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodr_ann_2h_unisoftsigbinlosadam1 = ann_2h(neurons=neurons,\n",
    "                                      encoded_train_x=train_x,\n",
    "                                      init_mode='uniform',\n",
    "                                      activation_input='relu',\n",
    "                                      weight_constraint=5,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      activation_output='sigmoid',\n",
    "                                      loss='binary_crossentropy',\n",
    "                                      optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_nodr_ann_2h_unisoftsigbinlosadam = ann_fit(checkpoint_file = \"./H5files/ann_2h_unisoftsigbinlosadam_redds\"+str(dsnum)+\"bal.h5\",\n",
    "                                        ann = nodr_ann_2h_unisoftsigbinlosadam1,\n",
    "                                        enc_train_x = train_x,\n",
    "                                        train_y = train_y,\n",
    "                                        epochs = 200,\n",
    "                                        shuffle = True,\n",
    "                                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_nodr_ann_2h_unisoftsigbinlosadam.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_value_nodr_ann_2h_unisoftsigbinlosadam = plot_hist_auto(hist_nodr_ann_2h_unisoftsigbinlosadam, './Figures/nodr_ann_2h_unisoftsigbinlosadam'+str(dsnum)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict(nodr_ann_2h_unisoftsigbinlosadam,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_x=train_x\n",
    "input_dim=enc_train_x.shape[1]\n",
    "enc_test_x=test_x\n",
    "test_y=test_y\n",
    "train_y=train_y\n",
    "init_mode='uniform'\n",
    "activation_input='relu'\n",
    "weight_constraint=5\n",
    "dropout_rate=0.0\n",
    "activation_output='sigmoid'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='Adam'\n",
    "\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nodr_ann_2h_prob_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam = ann_predict_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_nodr_ann_2h_01_unisoftsigbinlosadam, pred_nodr_ann_2h_01_unisoftsigbinlosadam, './Figures/ROC_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png', './Figures/CM_nodr_ann_2h_unisoftsigbinlosadam_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with ae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the PCA algorithm with our Data\n",
    "# pca = PCA().fit(data_rescaled)\n",
    "pca_ = PCA(n_components = 0.95, svd_solver = 'full').fit(train_x)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "n_coml = [pca_.n_components_]\n",
    "\n",
    "plt.plot(np.cumsum(pca_.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Variance (%)', fontsize=14) #for each component\n",
    "plt.title('Pulsar Dataset Explained Variance '+str(dsnum)+' node DS', fontsize=14)\n",
    "\n",
    "n_coml = [*n_coml]\n",
    "\n",
    "for i, v in enumerate(n_coml):\n",
    "    plt.text(v-0.8, i+0.94, '{:.0f}'.format(v), color='navy', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/PCA_components_ds'+str(dsnum)+'bal.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300, \n",
    "                             criterion='gini', \n",
    "                             max_depth=16, \n",
    "#                              min_samples_split=2, \n",
    "                             #min_samples_leaf=1, \n",
    "                             max_features=0.3, \n",
    "                             #bootstrap=True,\n",
    "                             oob_score=True,\n",
    "                             random_state=23)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(enc_train_x_asam, train_y)\n",
    "\n",
    "pred_y_ae_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(enc_test_x_asam),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=2)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_ae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_y_ae_RF, pred_y_ae_RF, './Figures/ROC_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_ae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with spae encoded DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(enc_train_x_spsam, train_y)\n",
    "\n",
    "pred_y_spae_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(enc_test_x_spsam),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=2)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_spae_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_y_spae_RF, pred_y_spae_RF, './Figures/ROC_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_spae_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **---------- RF with pca DS ----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(datetime.ctime(start_time))\n",
    "\n",
    "clf.fit(train_x_pca, train_y)\n",
    "\n",
    "pred_y_pca_RF = cross_val_predict(estimator=clf,\n",
    "                              X=np.array(test_x_pca),\n",
    "                              y=test_y,\n",
    "                              cv=KFold(n_splits=5, random_state=23),\n",
    "                              n_jobs=2)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print(\"Time elapsed (hh:mm:ss.ms) {}\".format(time_elapsed))\n",
    "\n",
    "print(sm.classification_report(test_y, pred_y_pca_RF,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_cm(pred_y_pca_RF, pred_y_pca_RF, './Figures/ROC_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png', './Figures/CM_pca_rf_E100MaxfautoMaxdnoneBootT_redds'+str(dsnum)+'bal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_ae_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_ae_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_sp_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_01_unisoftsigbinlosadam.shape)\n",
    "print(pred_nodr_ann_2h_prob_unisoftsigbinlosadam.shape)\n",
    "print(pred_y_ae_RF.shape)\n",
    "print(pred_y_spae_RF.shape)\n",
    "print(pred_y_pca_RF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate_ae_ann, recall_ae_ann, thresholds_ae_ann = roc_curve(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_ae_ann = auc(false_positive_rate_ae_ann, recall_ae_ann)\n",
    "false_positive_rate_sp_ann, recall_sp_ann, thresholds_sp_ann = roc_curve(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_sp_ann = auc(false_positive_rate_sp_ann, recall_sp_ann)\n",
    "false_positive_rate_nodr_ann, recall_nodr_ann, thresholds_nodr_ann = roc_curve(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)\n",
    "roc_auc_nodr_ann = auc(false_positive_rate_nodr_ann, recall_nodr_ann)\n",
    "\n",
    "false_positive_rate_ae_RF, recall_ae_RF, thresholds_ae_RF = roc_curve(test_y, pred_y_ae_RF)\n",
    "roc_auc_ae_RF = auc(false_positive_rate_ae_RF, recall_ae_RF)\n",
    "false_positive_rate_spae_RF, recall_spae_RF, thresholds_spae_RF = roc_curve(test_y, pred_y_spae_RF)\n",
    "roc_auc_spae_RF = auc(false_positive_rate_spae_RF, recall_spae_RF)\n",
    "false_positive_rate_pca_RF, recall_pca_RF, thresholds_pca_RF = roc_curve(test_y, pred_y_pca_RF)\n",
    "roc_auc_pca_RF = auc(false_positive_rate_pca_RF, recall_pca_RF)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC)', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "# plt.ylim([0.97,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Receiver Operating Characteristic (ROC) Zoom in', fontsize=16)\n",
    "\n",
    "plt.plot(false_positive_rate_ae_ann, recall_ae_ann, 'b', label = 'AUC AE + DNN = %0.3f' %roc_auc_ae_ann)\n",
    "plt.plot(false_positive_rate_sp_ann, recall_sp_ann, 'g', label = 'AUC SAE + DNN = %0.3f' %roc_auc_sp_ann)\n",
    "plt.plot(false_positive_rate_nodr_ann, recall_nodr_ann, 'r', label = 'AUC DNN = %0.3f' %roc_auc_nodr_ann)\n",
    "plt.plot(false_positive_rate_ae_RF, recall_ae_RF, 'c', label = 'AUC AE + RF = %0.3f' %roc_auc_ae_RF)\n",
    "plt.plot(false_positive_rate_spae_RF, recall_spae_RF, 'm', label = 'AUC SAE + RF = %0.3f' %roc_auc_spae_RF)\n",
    "plt.plot(false_positive_rate_pca_RF, recall_pca_RF, 'black', label = 'AUC PCA + RF = %0.3f' %roc_auc_pca_RF)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "# plt.ylim([0.0,1.0])\n",
    "plt.ylim([0.955,1.0])\n",
    "\n",
    "plt.ylabel('Recall - TPR', fontsize=14)\n",
    "plt.xlabel('Fall-out (1-Specificity) - FPR', fontsize=14)\n",
    "plt.savefig('./Figures/ROC_allmodels'+str(dsnum)+'bal_zoom.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)\n",
    "# labels = ['Normal', 'Malicious']\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=sns.light_palette(\"purple\"), vmin = 0.2);\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('True Class')\n",
    "# plt.xlabel('Predicted Class')\n",
    "# plt.savefig('./Figures/CM_ae_ann_thirdds'+str(dsnum)+'bal_TEST.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ae_ann = \"AE+DNN\"\n",
    "acc_ae_ann = (sm.accuracy_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_ae_ann = (sm.precision_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_ae_ann = (sm.recall_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_ae_ann = (sm.f1_score(test_y, pred_ae_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_sp_ann = \"SAE+DNN\"\n",
    "acc_sp_ann = (sm.accuracy_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_sp_ann = (sm.precision_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_sp_ann = (sm.recall_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_sp_ann = (sm.f1_score(test_y, pred_sp_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_nodr_ann = \"DNN\"\n",
    "acc_nodr_ann = (sm.accuracy_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "pre_nodr_ann = (sm.precision_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "recall_nodr_ann = (sm.recall_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100) \n",
    "f1score_nodr_ann = (sm.f1_score(test_y, pred_nodr_ann_2h_01_unisoftsigbinlosadam)*100)\n",
    "\n",
    "classi_ae_RF = \"AE+RF\"\n",
    "acc_ae_RF = (sm.accuracy_score(test_y, pred_y_ae_RF)*100) \n",
    "pre_ae_RF = (sm.precision_score(test_y, pred_y_ae_RF)*100) \n",
    "recall_ae_RF = (sm.recall_score(test_y, pred_y_ae_RF)*100) \n",
    "f1score_ae_RF = (sm.f1_score(test_y, pred_y_ae_RF)*100)\n",
    "\n",
    "classi_spae_RF = \"SAE+RF\"\n",
    "acc_spae_RF = (sm.accuracy_score(test_y, pred_y_spae_RF)*100) \n",
    "pre_spae_RF = (sm.precision_score(test_y, pred_y_spae_RF)*100) \n",
    "recall_spae_RF = (sm.recall_score(test_y, pred_y_spae_RF)*100) \n",
    "f1score_spae_RF = (sm.f1_score(test_y, pred_y_spae_RF)*100)\n",
    "\n",
    "classi_pca_RF = \"PCA+RF\"\n",
    "acc_pca_RF = (sm.accuracy_score(test_y, pred_y_pca_RF)*100) \n",
    "pre_pca_RF = (sm.precision_score(test_y, pred_y_pca_RF)*100) \n",
    "recall_pca_RF = (sm.recall_score(test_y, pred_y_pca_RF)*100) \n",
    "f1score_pca_RF = (sm.f1_score(test_y, pred_y_pca_RF)*100)\n",
    "\n",
    "\n",
    "print('Classifier\\tAcc\\tPreci\\tRecall\\tF1Score')\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_ann, acc_ae_ann, pre_ae_ann, recall_ae_ann, f1score_ae_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_sp_ann, acc_sp_ann, pre_sp_ann, recall_sp_ann, f1score_sp_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_nodr_ann, acc_nodr_ann, pre_nodr_ann, recall_nodr_ann, f1score_nodr_ann))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_ae_RF, acc_ae_RF, pre_ae_RF, recall_ae_RF, f1score_ae_RF))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_spae_RF, acc_spae_RF, pre_spae_RF, recall_spae_RF, f1score_spae_RF))\n",
    "print('{0:}\\t\\t{1:.2f}\\t{2:.2f}\\t{3:.2f}\\t{4:.2f}'.format(classi_pca_RF, acc_pca_RF, pre_pca_RF, recall_pca_RF, f1score_pca_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1list = [[\"AE+DNN\",f1score_ae_ann],[\"SAE+DNN\",f1score_sp_ann],[\"DNN\",f1score_nodr_ann],\n",
    "          [\"AE+RF\",f1score_ae_RF],[\"SAE+RF\",f1score_spae_RF],[\"PCA+RF\",f1score_pca_RF]]\n",
    "\n",
    "xs, ys = [*zip(*f1list)]\n",
    "\n",
    "'{:.2f}'.format(f1score_ae_ann)\n",
    "\n",
    "plt.figure(figsize=(8,6), )\n",
    "plt.barh(xs, ys, color = \"purple\")\n",
    "plt.title(\"F1 score vs Classifier\", fontsize=16)\n",
    "plt.xlabel(\"Classifier\", fontsize=14)\n",
    "plt.ylabel(\"F1 score\", fontsize=14)\n",
    "plt.xticks(np.arange(0, 101, 10), fontsize=12)\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "for i, v in enumerate(ys):\n",
    "    plt.text(v+1, i+0.1, '{:.2f}'.format(v), color='purple', fontsize=14)\n",
    "\n",
    "plt.savefig('./Figures/F1scoreplot_allmodels'+str(dsnum)+'bal.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
